I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright (c) 2011 X.commerce, a business unit of eBay Inc.
2 # Copyright 2010 United States Government as represented by the
3 # Administrator of the National Aeronautics and Space Administration.
4 # All Rights Reserved.
5 #
6 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
7 #    not use this file except in compliance with the License. You may obtain
8 #    a copy of the License at
9 #
10 #         http://www.apache.org/licenses/LICENSE-2.0
11 #
12 #    Unless required by applicable law or agreed to in writing, software
13 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
14 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
15 #    License for the specific language governing permissions and limitations
16 #    under the License.
17 
18 """Implementation of SQLAlchemy backend."""
19 
20 import collections
21 import copy
22 import datetime
23 import functools
24 import inspect
25 import sys
26 
27 from oslo_db import api as oslo_db_api
28 from oslo_db import exception as db_exc
29 from oslo_db.sqlalchemy import enginefacade
30 from oslo_db.sqlalchemy import update_match
31 from oslo_db.sqlalchemy import utils as sqlalchemyutils
32 from oslo_log import log as logging
33 from oslo_utils import importutils
34 from oslo_utils import timeutils
35 from oslo_utils import uuidutils
36 import six
37 from six.moves import range
38 import sqlalchemy as sa
39 from sqlalchemy import and_
40 from sqlalchemy import Boolean
41 from sqlalchemy.exc import NoSuchTableError
42 from sqlalchemy.ext.compiler import compiles
43 from sqlalchemy import Integer
44 from sqlalchemy import MetaData
45 from sqlalchemy import or_
46 from sqlalchemy.orm import aliased
47 from sqlalchemy.orm import contains_eager
48 from sqlalchemy.orm import joinedload
49 from sqlalchemy.orm import joinedload_all
50 from sqlalchemy.orm import noload
51 from sqlalchemy.orm import undefer
52 from sqlalchemy.schema import Table
53 from sqlalchemy import sql
54 from sqlalchemy.sql.expression import asc
55 from sqlalchemy.sql.expression import cast
56 from sqlalchemy.sql.expression import desc
57 from sqlalchemy.sql.expression import UpdateBase
58 from sqlalchemy.sql import false
59 from sqlalchemy.sql import func
60 from sqlalchemy.sql import null
61 from sqlalchemy.sql import true
62 
63 from nova import block_device
64 from nova.compute import task_states
65 from nova.compute import vm_states
66 import nova.conf
67 import nova.context
68 from nova.db.sqlalchemy import models
69 from nova import exception
70 from nova.i18n import _
71 from nova import safe_utils
72 
73 profiler_sqlalchemy = importutils.try_import('osprofiler.sqlalchemy')
74 
75 CONF = nova.conf.CONF
76 
77 
78 LOG = logging.getLogger(__name__)
79 
80 main_context_manager = enginefacade.transaction_context()
81 api_context_manager = enginefacade.transaction_context()
82 
83 
84 def _get_db_conf(conf_group, connection=None):
85     kw = dict(conf_group.items())
86     if connection is not None:
87         kw['connection'] = connection
88     return kw
89 
90 
91 def _context_manager_from_context(context):
92     if context:
93         try:
94             return context.db_connection
95         except AttributeError:
96             pass
97 
98 
99 def configure(conf):
100     main_context_manager.configure(**_get_db_conf(conf.database))
101     api_context_manager.configure(**_get_db_conf(conf.api_database))
102 
103     if profiler_sqlalchemy and CONF.profiler.enabled \
104             and CONF.profiler.trace_sqlalchemy:
105 
106         main_context_manager.append_on_engine_create(
107             lambda eng: profiler_sqlalchemy.add_tracing(sa, eng, "db"))
108         api_context_manager.append_on_engine_create(
109             lambda eng: profiler_sqlalchemy.add_tracing(sa, eng, "db"))
110 
111 
112 def create_context_manager(connection=None):
113     """Create a database context manager object.
114 
115     : param connection: The database connection string
116     """
117     ctxt_mgr = enginefacade.transaction_context()
118     ctxt_mgr.configure(**_get_db_conf(CONF.database, connection=connection))
119     return ctxt_mgr
120 
121 
122 def get_context_manager(context):
123     """Get a database context manager object.
124 
125     :param context: The request context that can contain a context manager
126     """
127     return _context_manager_from_context(context) or main_context_manager
128 
129 
130 def get_engine(use_slave=False, context=None):
131     """Get a database engine object.
132 
133     :param use_slave: Whether to use the slave connection
134     :param context: The request context that can contain a context manager
135     """
136     ctxt_mgr = get_context_manager(context)
137     if use_slave:
138         return ctxt_mgr.reader.get_engine()
139     return ctxt_mgr.writer.get_engine()
140 
141 
142 def get_api_engine():
143     return api_context_manager.writer.get_engine()
144 
145 
146 _SHADOW_TABLE_PREFIX = 'shadow_'
147 _DEFAULT_QUOTA_NAME = 'default'
148 PER_PROJECT_QUOTAS = ['fixed_ips', 'floating_ips', 'networks']
149 
150 
151 def get_backend():
152     """The backend is this module itself."""
153     return sys.modules[__name__]
154 
155 
156 def require_context(f):
157     """Decorator to require *any* user or admin context.
158 
159     This does no authorization for user or project access matching, see
160     :py:func:`nova.context.authorize_project_context` and
161     :py:func:`nova.context.authorize_user_context`.
162 
163     The first argument to the wrapped function must be the context.
164 
165     """
166 
167     @functools.wraps(f)
168     def wrapper(*args, **kwargs):
169         nova.context.require_context(args[0])
170         return f(*args, **kwargs)
171     return wrapper
172 
173 
174 def select_db_reader_mode(f):
175     """Decorator to select synchronous or asynchronous reader mode.
176 
177     The kwarg argument 'use_slave' defines reader mode. Asynchronous reader
178     will be used if 'use_slave' is True and synchronous reader otherwise.
179     If 'use_slave' is not specified default value 'False' will be used.
180 
181     Wrapped function must have a context in the arguments.
182     """
183 
184     @functools.wraps(f)
185     def wrapper(*args, **kwargs):
186         wrapped_func = safe_utils.get_wrapped_function(f)
187         keyed_args = inspect.getcallargs(wrapped_func, *args, **kwargs)
188 
189         context = keyed_args['context']
190         use_slave = keyed_args.get('use_slave', False)
191 
192         if use_slave:
193             reader_mode = get_context_manager(context).async_
194         else:
195             reader_mode = get_context_manager(context).reader
196 
197         with reader_mode.using(context):
198             return f(*args, **kwargs)
199     return wrapper
200 
201 
202 def pick_context_manager_writer(f):
203     """Decorator to use a writer db context manager.
204 
205     The db context manager will be picked from the RequestContext.
206 
207     Wrapped function must have a RequestContext in the arguments.
208     """
209     @functools.wraps(f)
210     def wrapped(context, *args, **kwargs):
211         ctxt_mgr = get_context_manager(context)
212         with ctxt_mgr.writer.using(context):
213             return f(context, *args, **kwargs)
214     return wrapped
215 
216 
217 def pick_context_manager_reader(f):
218     """Decorator to use a reader db context manager.
219 
220     The db context manager will be picked from the RequestContext.
221 
222     Wrapped function must have a RequestContext in the arguments.
223     """
224     @functools.wraps(f)
225     def wrapped(context, *args, **kwargs):
226         ctxt_mgr = get_context_manager(context)
227         with ctxt_mgr.reader.using(context):
228             return f(context, *args, **kwargs)
229     return wrapped
230 
231 
232 def pick_context_manager_reader_allow_async(f):
233     """Decorator to use a reader.allow_async db context manager.
234 
235     The db context manager will be picked from the RequestContext.
236 
237     Wrapped function must have a RequestContext in the arguments.
238     """
239     @functools.wraps(f)
240     def wrapped(context, *args, **kwargs):
241         ctxt_mgr = get_context_manager(context)
242         with ctxt_mgr.reader.allow_async.using(context):
243             return f(context, *args, **kwargs)
244     return wrapped
245 
246 
247 def model_query(context, model,
248                 args=None,
249                 read_deleted=None,
250                 project_only=False):
251     """Query helper that accounts for context's `read_deleted` field.
252 
253     :param context:     NovaContext of the query.
254     :param model:       Model to query. Must be a subclass of ModelBase.
255     :param args:        Arguments to query. If None - model is used.
256     :param read_deleted: If not None, overrides context's read_deleted field.
257                         Permitted values are 'no', which does not return
258                         deleted values; 'only', which only returns deleted
259                         values; and 'yes', which does not filter deleted
260                         values.
261     :param project_only: If set and context is user-type, then restrict
262                         query to match the context's project_id. If set to
263                         'allow_none', restriction includes project_id = None.
264     """
265 
266     if read_deleted is None:
267         read_deleted = context.read_deleted
268 
269     query_kwargs = {}
270     if 'no' == read_deleted:
271         query_kwargs['deleted'] = False
272     elif 'only' == read_deleted:
273         query_kwargs['deleted'] = True
274     elif 'yes' == read_deleted:
275         pass
276     else:
277         raise ValueError(_("Unrecognized read_deleted value '%s'")
278                            % read_deleted)
279 
280     query = sqlalchemyutils.model_query(
281         model, context.session, args, **query_kwargs)
282 
283     # We can't use oslo.db model_query's project_id here, as it doesn't allow
284     # us to return both our projects and unowned projects.
285     if nova.context.is_user_context(context) and project_only:
286         if project_only == 'allow_none':
287             query = query.\
288                 filter(or_(model.project_id == context.project_id,
289                            model.project_id == null()))
290         else:
291             query = query.filter_by(project_id=context.project_id)
292 
293     return query
294 
295 
296 def convert_objects_related_datetimes(values, *datetime_keys):
297     if not datetime_keys:
298         datetime_keys = ('created_at', 'deleted_at', 'updated_at')
299 
300     for key in datetime_keys:
301         if key in values and values[key]:
302             if isinstance(values[key], six.string_types):
303                 try:
304                     values[key] = timeutils.parse_strtime(values[key])
305                 except ValueError:
306                     # Try alternate parsing since parse_strtime will fail
307                     # with say converting '2015-05-28T19:59:38+00:00'
308                     values[key] = timeutils.parse_isotime(values[key])
309             # NOTE(danms): Strip UTC timezones from datetimes, since they're
310             # stored that way in the database
311             values[key] = values[key].replace(tzinfo=None)
312     return values
313 
314 
315 ###################
316 
317 
318 def constraint(**conditions):
319     return Constraint(conditions)
320 
321 
322 def equal_any(*values):
323     return EqualityCondition(values)
324 
325 
326 def not_equal(*values):
327     return InequalityCondition(values)
328 
329 
330 class Constraint(object):
331 
332     def __init__(self, conditions):
333         self.conditions = conditions
334 
335     def apply(self, model, query):
336         for key, condition in self.conditions.items():
337             for clause in condition.clauses(getattr(model, key)):
338                 query = query.filter(clause)
339         return query
340 
341 
342 class EqualityCondition(object):
343 
344     def __init__(self, values):
345         self.values = values
346 
347     def clauses(self, field):
348         # method signature requires us to return an iterable even if for OR
349         # operator this will actually be a single clause
350         return [or_(*[field == value for value in self.values])]
351 
352 
353 class InequalityCondition(object):
354 
355     def __init__(self, values):
356         self.values = values
357 
358     def clauses(self, field):
359         return [field != value for value in self.values]
360 
361 
362 class DeleteFromSelect(UpdateBase):
363     def __init__(self, table, select, column):
364         self.table = table
365         self.select = select
366         self.column = column
367 
368 
369 # NOTE(guochbo): some versions of MySQL doesn't yet support subquery with
370 # 'LIMIT & IN/ALL/ANY/SOME' We need work around this with nesting select .
371 @compiles(DeleteFromSelect)
372 def visit_delete_from_select(element, compiler, **kw):
373     return "DELETE FROM %s WHERE %s in (SELECT T1.%s FROM (%s) as T1)" % (
374         compiler.process(element.table, asfrom=True),
375         compiler.process(element.column),
376         element.column.name,
377         compiler.process(element.select))
378 
379 ###################
380 
381 
382 @pick_context_manager_writer
383 def service_destroy(context, service_id):
384     service = service_get(context, service_id)
385 
386     model_query(context, models.Service).\
387                 filter_by(id=service_id).\
388                 soft_delete(synchronize_session=False)
389 
390     # TODO(sbauza): Remove the service_id filter in a later release
391     # once we are sure that all compute nodes report the host field
392     model_query(context, models.ComputeNode).\
393                 filter(or_(models.ComputeNode.service_id == service_id,
394                            models.ComputeNode.host == service['host'])).\
395                 soft_delete(synchronize_session=False)
396 
397 
398 @pick_context_manager_reader
399 def service_get(context, service_id):
400     query = model_query(context, models.Service).filter_by(id=service_id)
401 
402     result = query.first()
403     if not result:
404         raise exception.ServiceNotFound(service_id=service_id)
405 
406     return result
407 
408 
409 @pick_context_manager_reader
410 def service_get_by_uuid(context, service_uuid):
411     query = model_query(context, models.Service).filter_by(uuid=service_uuid)
412 
413     result = query.first()
414     if not result:
415         raise exception.ServiceNotFound(service_id=service_uuid)
416 
417     return result
418 
419 
420 @pick_context_manager_reader_allow_async
421 def service_get_minimum_version(context, binaries):
422     min_versions = context.session.query(
423         models.Service.binary,
424         func.min(models.Service.version)).\
425                          filter(models.Service.binary.in_(binaries)).\
426                          filter(models.Service.deleted == 0).\
427                          filter(models.Service.forced_down == false()).\
428                          group_by(models.Service.binary)
429     return dict(min_versions)
430 
431 
432 @pick_context_manager_reader
433 def service_get_all(context, disabled=None):
434     query = model_query(context, models.Service)
435 
436     if disabled is not None:
437         query = query.filter_by(disabled=disabled)
438 
439     return query.all()
440 
441 
442 @pick_context_manager_reader
443 def service_get_all_by_topic(context, topic):
444     return model_query(context, models.Service, read_deleted="no").\
445                 filter_by(disabled=False).\
446                 filter_by(topic=topic).\
447                 all()
448 
449 
450 @pick_context_manager_reader
451 def service_get_by_host_and_topic(context, host, topic):
452     return model_query(context, models.Service, read_deleted="no").\
453                 filter_by(disabled=False).\
454                 filter_by(host=host).\
455                 filter_by(topic=topic).\
456                 first()
457 
458 
459 @pick_context_manager_reader
460 def service_get_all_by_binary(context, binary, include_disabled=False):
461     query = model_query(context, models.Service, read_deleted="no").\
462                     filter_by(binary=binary)
463     if not include_disabled:
464         query = query.filter_by(disabled=False)
465     return query.all()
466 
467 
468 @pick_context_manager_reader
469 def service_get_all_computes_by_hv_type(context, hv_type,
470                                         include_disabled=False):
471     query = model_query(context, models.Service, read_deleted="no").\
472                     filter_by(binary='nova-compute')
473     if not include_disabled:
474         query = query.filter_by(disabled=False)
475     query = query.join(models.ComputeNode,
476                        models.Service.host == models.ComputeNode.host).\
477                   filter(models.ComputeNode.hypervisor_type == hv_type).\
478                   distinct('host')
479     return query.all()
480 
481 
482 @pick_context_manager_reader
483 def service_get_by_host_and_binary(context, host, binary):
484     result = model_query(context, models.Service, read_deleted="no").\
485                     filter_by(host=host).\
486                     filter_by(binary=binary).\
487                     first()
488 
489     if not result:
490         raise exception.HostBinaryNotFound(host=host, binary=binary)
491 
492     return result
493 
494 
495 @pick_context_manager_reader
496 def service_get_all_by_host(context, host):
497     return model_query(context, models.Service, read_deleted="no").\
498                 filter_by(host=host).\
499                 all()
500 
501 
502 @pick_context_manager_reader_allow_async
503 def service_get_by_compute_host(context, host):
504     result = model_query(context, models.Service, read_deleted="no").\
505                 filter_by(host=host).\
506                 filter_by(binary='nova-compute').\
507                 first()
508 
509     if not result:
510         raise exception.ComputeHostNotFound(host=host)
511 
512     return result
513 
514 
515 @pick_context_manager_writer
516 def service_create(context, values):
517     service_ref = models.Service()
518     service_ref.update(values)
519     # We only auto-disable nova-compute services since those are the only
520     # ones that can be enabled using the os-services REST API and they are
521     # the only ones where being disabled means anything. It does
522     # not make sense to be able to disable non-compute services like
523     # nova-scheduler or nova-osapi_compute since that does nothing.
524     if not CONF.enable_new_services and values.get('binary') == 'nova-compute':
525         msg = _("New compute service disabled due to config option.")
526         service_ref.disabled = True
527         service_ref.disabled_reason = msg
528     try:
529         service_ref.save(context.session)
530     except db_exc.DBDuplicateEntry as e:
531         if 'binary' in e.columns:
532             raise exception.ServiceBinaryExists(host=values.get('host'),
533                         binary=values.get('binary'))
534         raise exception.ServiceTopicExists(host=values.get('host'),
535                         topic=values.get('topic'))
536     return service_ref
537 
538 
539 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
540 @pick_context_manager_writer
541 def service_update(context, service_id, values):
542     service_ref = service_get(context, service_id)
543     # Only servicegroup.drivers.db.DbDriver._report_state() updates
544     # 'report_count', so if that value changes then store the timestamp
545     # as the last time we got a state report.
546     if 'report_count' in values:
547         if values['report_count'] > service_ref.report_count:
548             service_ref.last_seen_up = timeutils.utcnow()
549     service_ref.update(values)
550 
551     return service_ref
552 
553 
554 ###################
555 
556 
557 def _compute_node_select(context, filters=None, limit=None, marker=None):
558     if filters is None:
559         filters = {}
560 
561     cn_tbl = sa.alias(models.ComputeNode.__table__, name='cn')
562     select = sa.select([cn_tbl])
563 
564     if context.read_deleted == "no":
565         select = select.where(cn_tbl.c.deleted == 0)
566     if "compute_id" in filters:
567         select = select.where(cn_tbl.c.id == filters["compute_id"])
568     if "service_id" in filters:
569         select = select.where(cn_tbl.c.service_id == filters["service_id"])
570     if "host" in filters:
571         select = select.where(cn_tbl.c.host == filters["host"])
572     if "hypervisor_hostname" in filters:
573         hyp_hostname = filters["hypervisor_hostname"]
574         select = select.where(cn_tbl.c.hypervisor_hostname == hyp_hostname)
575     if "mapped" in filters:
576         select = select.where(cn_tbl.c.mapped < filters['mapped'])
577     if marker is not None:
578         try:
579             compute_node_get(context, marker)
580         except exception.ComputeHostNotFound:
581             raise exception.MarkerNotFound(marker=marker)
582         select = select.where(cn_tbl.c.id > marker)
583     if limit is not None:
584         select = select.limit(limit)
585     # Explicitly order by id, so we're not dependent on the native sort
586     # order of the underlying DB.
587     select = select.order_by(asc("id"))
588     return select
589 
590 
591 def _compute_node_fetchall(context, filters=None, limit=None, marker=None):
592     select = _compute_node_select(context, filters, limit=limit, marker=marker)
593     engine = get_engine(context=context)
594     conn = engine.connect()
595 
596     results = conn.execute(select).fetchall()
597 
598     # Callers expect dict-like objects, not SQLAlchemy RowProxy objects...
599     results = [dict(r) for r in results]
600     conn.close()
601     return results
602 
603 
604 @pick_context_manager_reader
605 def compute_node_get(context, compute_id):
606     results = _compute_node_fetchall(context, {"compute_id": compute_id})
607     if not results:
608         raise exception.ComputeHostNotFound(host=compute_id)
609     return results[0]
610 
611 
612 @pick_context_manager_reader
613 def compute_node_get_model(context, compute_id):
614     # TODO(edleafe): remove once the compute node resource provider migration
615     # is complete, and this distinction is no longer necessary.
616     result = model_query(context, models.ComputeNode).\
617             filter_by(id=compute_id).\
618             first()
619     if not result:
620         raise exception.ComputeHostNotFound(host=compute_id)
621     return result
622 
623 
624 @pick_context_manager_reader
625 def compute_nodes_get_by_service_id(context, service_id):
626     results = _compute_node_fetchall(context, {"service_id": service_id})
627     if not results:
628         raise exception.ServiceNotFound(service_id=service_id)
629     return results
630 
631 
632 @pick_context_manager_reader
633 def compute_node_get_by_host_and_nodename(context, host, nodename):
634     results = _compute_node_fetchall(context,
635             {"host": host, "hypervisor_hostname": nodename})
636     if not results:
637         raise exception.ComputeHostNotFound(host=host)
638     return results[0]
639 
640 
641 @pick_context_manager_reader_allow_async
642 def compute_node_get_all_by_host(context, host):
643     results = _compute_node_fetchall(context, {"host": host})
644     if not results:
645         raise exception.ComputeHostNotFound(host=host)
646     return results
647 
648 
649 @pick_context_manager_reader
650 def compute_node_get_all(context):
651     return _compute_node_fetchall(context)
652 
653 
654 @pick_context_manager_reader
655 def compute_node_get_all_mapped_less_than(context, mapped_less_than):
656     return _compute_node_fetchall(context,
657                                   {'mapped': mapped_less_than})
658 
659 
660 @pick_context_manager_reader
661 def compute_node_get_all_by_pagination(context, limit=None, marker=None):
662     return _compute_node_fetchall(context, limit=limit, marker=marker)
663 
664 
665 @pick_context_manager_reader
666 def compute_node_search_by_hypervisor(context, hypervisor_match):
667     field = models.ComputeNode.hypervisor_hostname
668     return model_query(context, models.ComputeNode).\
669             filter(field.like('%%%s%%' % hypervisor_match)).\
670             all()
671 
672 
673 @pick_context_manager_writer
674 def compute_node_create(context, values):
675     """Creates a new ComputeNode and populates the capacity fields
676     with the most recent data.
677     """
678     convert_objects_related_datetimes(values)
679 
680     compute_node_ref = models.ComputeNode()
681     compute_node_ref.update(values)
682     compute_node_ref.save(context.session)
683 
684     return compute_node_ref
685 
686 
687 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
688 @pick_context_manager_writer
689 def compute_node_update(context, compute_id, values):
690     """Updates the ComputeNode record with the most recent data."""
691 
692     compute_ref = compute_node_get_model(context, compute_id)
693     # Always update this, even if there's going to be no other
694     # changes in data.  This ensures that we invalidate the
695     # scheduler cache of compute node data in case of races.
696     values['updated_at'] = timeutils.utcnow()
697     convert_objects_related_datetimes(values)
698     compute_ref.update(values)
699 
700     return compute_ref
701 
702 
703 @pick_context_manager_writer
704 def compute_node_delete(context, compute_id):
705     """Delete a ComputeNode record."""
706     result = model_query(context, models.ComputeNode).\
707              filter_by(id=compute_id).\
708              soft_delete(synchronize_session=False)
709 
710     if not result:
711         raise exception.ComputeHostNotFound(host=compute_id)
712 
713 
714 @pick_context_manager_reader
715 def compute_node_statistics(context):
716     """Compute statistics over all compute nodes."""
717     engine = get_engine(context=context)
718     services_tbl = models.Service.__table__
719 
720     inner_sel = sa.alias(_compute_node_select(context), name='inner_sel')
721 
722     # TODO(sbauza): Remove the service_id filter in a later release
723     # once we are sure that all compute nodes report the host field
724     j = sa.join(
725         inner_sel, services_tbl,
726         sql.and_(
727             sql.or_(
728                 inner_sel.c.host == services_tbl.c.host,
729                 inner_sel.c.service_id == services_tbl.c.id
730             ),
731             services_tbl.c.disabled == false(),
732             services_tbl.c.binary == 'nova-compute',
733             services_tbl.c.deleted == 0
734         )
735     )
736 
737     # NOTE(jaypipes): This COALESCE() stuff is temporary while the data
738     # migration to the new resource providers inventories and allocations
739     # tables is completed.
740     agg_cols = [
741         func.count().label('count'),
742         sql.func.sum(
743             inner_sel.c.vcpus
744         ).label('vcpus'),
745         sql.func.sum(
746             inner_sel.c.memory_mb
747         ).label('memory_mb'),
748         sql.func.sum(
749             inner_sel.c.local_gb
750         ).label('local_gb'),
751         sql.func.sum(
752             inner_sel.c.vcpus_used
753         ).label('vcpus_used'),
754         sql.func.sum(
755             inner_sel.c.memory_mb_used
756         ).label('memory_mb_used'),
757         sql.func.sum(
758             inner_sel.c.local_gb_used
759         ).label('local_gb_used'),
760         sql.func.sum(
761             inner_sel.c.free_ram_mb
762         ).label('free_ram_mb'),
763         sql.func.sum(
764             inner_sel.c.free_disk_gb
765         ).label('free_disk_gb'),
766         sql.func.sum(
767             inner_sel.c.current_workload
768         ).label('current_workload'),
769         sql.func.sum(
770             inner_sel.c.running_vms
771         ).label('running_vms'),
772         sql.func.sum(
773             inner_sel.c.disk_available_least
774         ).label('disk_available_least'),
775     ]
776     select = sql.select(agg_cols).select_from(j)
777     conn = engine.connect()
778 
779     results = conn.execute(select).fetchone()
780 
781     # Build a dict of the info--making no assumptions about result
782     fields = ('count', 'vcpus', 'memory_mb', 'local_gb', 'vcpus_used',
783               'memory_mb_used', 'local_gb_used', 'free_ram_mb', 'free_disk_gb',
784               'current_workload', 'running_vms', 'disk_available_least')
785     results = {field: int(results[idx] or 0)
786                for idx, field in enumerate(fields)}
787     conn.close()
788     return results
789 
790 
791 ###################
792 
793 
794 @pick_context_manager_writer
795 def certificate_create(context, values):
796     certificate_ref = models.Certificate()
797     for (key, value) in values.items():
798         certificate_ref[key] = value
799     certificate_ref.save(context.session)
800     return certificate_ref
801 
802 
803 @pick_context_manager_reader
804 def certificate_get_all_by_project(context, project_id):
805     return model_query(context, models.Certificate, read_deleted="no").\
806                    filter_by(project_id=project_id).\
807                    all()
808 
809 
810 @pick_context_manager_reader
811 def certificate_get_all_by_user(context, user_id):
812     return model_query(context, models.Certificate, read_deleted="no").\
813                    filter_by(user_id=user_id).\
814                    all()
815 
816 
817 @pick_context_manager_reader
818 def certificate_get_all_by_user_and_project(context, user_id, project_id):
819     return model_query(context, models.Certificate, read_deleted="no").\
820                    filter_by(user_id=user_id).\
821                    filter_by(project_id=project_id).\
822                    all()
823 
824 
825 ###################
826 
827 
828 @require_context
829 @pick_context_manager_reader
830 def floating_ip_get(context, id):
831     try:
832         result = model_query(context, models.FloatingIp, project_only=True).\
833                      filter_by(id=id).\
834                      options(joinedload_all('fixed_ip.instance')).\
835                      first()
836 
837         if not result:
838             raise exception.FloatingIpNotFound(id=id)
839     except db_exc.DBError:
840         LOG.warning("Invalid floating IP ID %s in request", id)
841         raise exception.InvalidID(id=id)
842     return result
843 
844 
845 @require_context
846 @pick_context_manager_reader
847 def floating_ip_get_pools(context):
848     pools = []
849     for result in model_query(context, models.FloatingIp,
850                               (models.FloatingIp.pool,)).distinct():
851         pools.append({'name': result[0]})
852     return pools
853 
854 
855 @require_context
856 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
857 @pick_context_manager_writer
858 def floating_ip_allocate_address(context, project_id, pool,
859                                  auto_assigned=False):
860     nova.context.authorize_project_context(context, project_id)
861     floating_ip_ref = model_query(context, models.FloatingIp,
862                                   read_deleted="no").\
863         filter_by(fixed_ip_id=None).\
864         filter_by(project_id=None).\
865         filter_by(pool=pool).\
866         first()
867 
868     if not floating_ip_ref:
869         raise exception.NoMoreFloatingIps()
870 
871     params = {'project_id': project_id, 'auto_assigned': auto_assigned}
872 
873     rows_update = model_query(context, models.FloatingIp, read_deleted="no").\
874         filter_by(id=floating_ip_ref['id']).\
875         filter_by(fixed_ip_id=None).\
876         filter_by(project_id=None).\
877         filter_by(pool=pool).\
878         update(params, synchronize_session='evaluate')
879 
880     if not rows_update:
881         LOG.debug('The row was updated in a concurrent transaction, '
882                   'we will fetch another one')
883         raise db_exc.RetryRequest(exception.FloatingIpAllocateFailed())
884 
885     return floating_ip_ref['address']
886 
887 
888 @require_context
889 @pick_context_manager_writer
890 def floating_ip_bulk_create(context, ips, want_result=True):
891     try:
892         tab = models.FloatingIp().__table__
893         context.session.execute(tab.insert(), ips)
894     except db_exc.DBDuplicateEntry as e:
895         raise exception.FloatingIpExists(address=e.value)
896 
897     if want_result:
898         return model_query(context, models.FloatingIp).filter(
899             models.FloatingIp.address.in_(
900                 [ip['address'] for ip in ips])).all()
901 
902 
903 def _ip_range_splitter(ips, block_size=256):
904     """Yields blocks of IPs no more than block_size elements long."""
905     out = []
906     count = 0
907     for ip in ips:
908         out.append(ip['address'])
909         count += 1
910 
911         if count > block_size - 1:
912             yield out
913             out = []
914             count = 0
915 
916     if out:
917         yield out
918 
919 
920 @require_context
921 @pick_context_manager_writer
922 def floating_ip_bulk_destroy(context, ips):
923     project_id_to_quota_count = collections.defaultdict(int)
924     for ip_block in _ip_range_splitter(ips):
925         # Find any floating IPs that were not auto_assigned and
926         # thus need quota released.
927         query = model_query(context, models.FloatingIp).\
928             filter(models.FloatingIp.address.in_(ip_block)).\
929             filter_by(auto_assigned=False)
930         for row in query.all():
931             # The count is negative since we release quota by
932             # reserving negative quota.
933             project_id_to_quota_count[row['project_id']] -= 1
934         # Delete the floating IPs.
935         model_query(context, models.FloatingIp).\
936             filter(models.FloatingIp.address.in_(ip_block)).\
937             soft_delete(synchronize_session='fetch')
938 
939 
940 @require_context
941 @pick_context_manager_writer
942 def floating_ip_create(context, values):
943     floating_ip_ref = models.FloatingIp()
944     floating_ip_ref.update(values)
945     try:
946         floating_ip_ref.save(context.session)
947     except db_exc.DBDuplicateEntry:
948         raise exception.FloatingIpExists(address=values['address'])
949     return floating_ip_ref
950 
951 
952 def _floating_ip_count_by_project(context, project_id):
953     nova.context.authorize_project_context(context, project_id)
954     # TODO(tr3buchet): why leave auto_assigned floating IPs out?
955     return model_query(context, models.FloatingIp, read_deleted="no").\
956                    filter_by(project_id=project_id).\
957                    filter_by(auto_assigned=False).\
958                    count()
959 
960 
961 @require_context
962 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
963 @pick_context_manager_writer
964 def floating_ip_fixed_ip_associate(context, floating_address,
965                                    fixed_address, host):
966     fixed_ip_ref = model_query(context, models.FixedIp).\
967                      filter_by(address=fixed_address).\
968                      options(joinedload('network')).\
969                      first()
970     if not fixed_ip_ref:
971         raise exception.FixedIpNotFoundForAddress(address=fixed_address)
972     rows = model_query(context, models.FloatingIp).\
973                 filter_by(address=floating_address).\
974                 filter(models.FloatingIp.project_id ==
975                        context.project_id).\
976                 filter(or_(models.FloatingIp.fixed_ip_id ==
977                            fixed_ip_ref['id'],
978                            models.FloatingIp.fixed_ip_id.is_(None))).\
979                 update({'fixed_ip_id': fixed_ip_ref['id'], 'host': host})
980 
981     if not rows:
982         raise exception.FloatingIpAssociateFailed(address=floating_address)
983 
984     return fixed_ip_ref
985 
986 
987 @require_context
988 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
989 @pick_context_manager_writer
990 def floating_ip_deallocate(context, address):
991     return model_query(context, models.FloatingIp).\
992         filter_by(address=address).\
993         filter(and_(models.FloatingIp.project_id != null()),
994                     models.FloatingIp.fixed_ip_id == null()).\
995         update({'project_id': None,
996                 'host': None,
997                 'auto_assigned': False},
998                synchronize_session=False)
999 
1000 
1001 @require_context
1002 @pick_context_manager_writer
1003 def floating_ip_destroy(context, address):
1004     model_query(context, models.FloatingIp).\
1005             filter_by(address=address).\
1006             delete()
1007 
1008 
1009 @require_context
1010 @pick_context_manager_writer
1011 def floating_ip_disassociate(context, address):
1012     floating_ip_ref = model_query(context,
1013                                   models.FloatingIp).\
1014                         filter_by(address=address).\
1015                         first()
1016     if not floating_ip_ref:
1017         raise exception.FloatingIpNotFoundForAddress(address=address)
1018 
1019     fixed_ip_ref = model_query(context, models.FixedIp).\
1020         filter_by(id=floating_ip_ref['fixed_ip_id']).\
1021         options(joinedload('network')).\
1022         first()
1023     floating_ip_ref.fixed_ip_id = None
1024     floating_ip_ref.host = None
1025 
1026     return fixed_ip_ref
1027 
1028 
1029 def _floating_ip_get_all(context):
1030     return model_query(context, models.FloatingIp, read_deleted="no")
1031 
1032 
1033 @pick_context_manager_reader
1034 def floating_ip_get_all(context):
1035     floating_ip_refs = _floating_ip_get_all(context).\
1036                        options(joinedload('fixed_ip')).\
1037                        all()
1038     if not floating_ip_refs:
1039         raise exception.NoFloatingIpsDefined()
1040     return floating_ip_refs
1041 
1042 
1043 @pick_context_manager_reader
1044 def floating_ip_get_all_by_host(context, host):
1045     floating_ip_refs = _floating_ip_get_all(context).\
1046                        filter_by(host=host).\
1047                        options(joinedload('fixed_ip')).\
1048                        all()
1049     if not floating_ip_refs:
1050         raise exception.FloatingIpNotFoundForHost(host=host)
1051     return floating_ip_refs
1052 
1053 
1054 @require_context
1055 @pick_context_manager_reader
1056 def floating_ip_get_all_by_project(context, project_id):
1057     nova.context.authorize_project_context(context, project_id)
1058     # TODO(tr3buchet): why do we not want auto_assigned floating IPs here?
1059     return _floating_ip_get_all(context).\
1060                          filter_by(project_id=project_id).\
1061                          filter_by(auto_assigned=False).\
1062                          options(joinedload_all('fixed_ip.instance')).\
1063                          all()
1064 
1065 
1066 @require_context
1067 @pick_context_manager_reader
1068 def floating_ip_get_by_address(context, address):
1069     return _floating_ip_get_by_address(context, address)
1070 
1071 
1072 def _floating_ip_get_by_address(context, address):
1073 
1074     # if address string is empty explicitly set it to None
1075     if not address:
1076         address = None
1077     try:
1078         result = model_query(context, models.FloatingIp).\
1079                     filter_by(address=address).\
1080                     options(joinedload_all('fixed_ip.instance')).\
1081                     first()
1082 
1083         if not result:
1084             raise exception.FloatingIpNotFoundForAddress(address=address)
1085     except db_exc.DBError:
1086         msg = _("Invalid floating IP %s in request") % address
1087         LOG.warning(msg)
1088         raise exception.InvalidIpAddressError(msg)
1089 
1090     # If the floating IP has a project ID set, check to make sure
1091     # the non-admin user has access.
1092     if result.project_id and nova.context.is_user_context(context):
1093         nova.context.authorize_project_context(context, result.project_id)
1094 
1095     return result
1096 
1097 
1098 @require_context
1099 @pick_context_manager_reader
1100 def floating_ip_get_by_fixed_address(context, fixed_address):
1101     return model_query(context, models.FloatingIp).\
1102                        outerjoin(models.FixedIp,
1103                                  models.FixedIp.id ==
1104                                  models.FloatingIp.fixed_ip_id).\
1105                        filter(models.FixedIp.address == fixed_address).\
1106                        all()
1107 
1108 
1109 @require_context
1110 @pick_context_manager_reader
1111 def floating_ip_get_by_fixed_ip_id(context, fixed_ip_id):
1112     return model_query(context, models.FloatingIp).\
1113                 filter_by(fixed_ip_id=fixed_ip_id).\
1114                 all()
1115 
1116 
1117 @require_context
1118 @pick_context_manager_writer
1119 def floating_ip_update(context, address, values):
1120     float_ip_ref = _floating_ip_get_by_address(context, address)
1121     float_ip_ref.update(values)
1122     try:
1123         float_ip_ref.save(context.session)
1124     except db_exc.DBDuplicateEntry:
1125         raise exception.FloatingIpExists(address=values['address'])
1126     return float_ip_ref
1127 
1128 
1129 ###################
1130 
1131 
1132 @require_context
1133 @pick_context_manager_reader
1134 def dnsdomain_get(context, fqdomain):
1135     return model_query(context, models.DNSDomain, read_deleted="no").\
1136                filter_by(domain=fqdomain).\
1137                with_lockmode('update').\
1138                first()
1139 
1140 
1141 def _dnsdomain_get_or_create(context, fqdomain):
1142     domain_ref = dnsdomain_get(context, fqdomain)
1143     if not domain_ref:
1144         dns_ref = models.DNSDomain()
1145         dns_ref.update({'domain': fqdomain,
1146                         'availability_zone': None,
1147                         'project_id': None})
1148         return dns_ref
1149 
1150     return domain_ref
1151 
1152 
1153 @pick_context_manager_writer
1154 def dnsdomain_register_for_zone(context, fqdomain, zone):
1155     domain_ref = _dnsdomain_get_or_create(context, fqdomain)
1156     domain_ref.scope = 'private'
1157     domain_ref.availability_zone = zone
1158     context.session.add(domain_ref)
1159 
1160 
1161 @pick_context_manager_writer
1162 def dnsdomain_register_for_project(context, fqdomain, project):
1163     domain_ref = _dnsdomain_get_or_create(context, fqdomain)
1164     domain_ref.scope = 'public'
1165     domain_ref.project_id = project
1166     context.session.add(domain_ref)
1167 
1168 
1169 @pick_context_manager_writer
1170 def dnsdomain_unregister(context, fqdomain):
1171     model_query(context, models.DNSDomain).\
1172                  filter_by(domain=fqdomain).\
1173                  delete()
1174 
1175 
1176 @pick_context_manager_reader
1177 def dnsdomain_get_all(context):
1178     return model_query(context, models.DNSDomain, read_deleted="no").all()
1179 
1180 
1181 ###################
1182 
1183 
1184 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
1185 @pick_context_manager_writer
1186 def fixed_ip_associate(context, address, instance_uuid, network_id=None,
1187                        reserved=False, virtual_interface_id=None):
1188     """Keyword arguments:
1189     reserved -- should be a boolean value(True or False), exact value will be
1190     used to filter on the fixed IP address
1191     """
1192     if not uuidutils.is_uuid_like(instance_uuid):
1193         raise exception.InvalidUUID(uuid=instance_uuid)
1194 
1195     network_or_none = or_(models.FixedIp.network_id == network_id,
1196                           models.FixedIp.network_id == null())
1197     fixed_ip_ref = model_query(context, models.FixedIp, read_deleted="no").\
1198                            filter(network_or_none).\
1199                            filter_by(reserved=reserved).\
1200                            filter_by(address=address).\
1201                            first()
1202 
1203     if fixed_ip_ref is None:
1204         raise exception.FixedIpNotFoundForNetwork(address=address,
1205                                         network_uuid=network_id)
1206     if fixed_ip_ref.instance_uuid:
1207         raise exception.FixedIpAlreadyInUse(address=address,
1208                                             instance_uuid=instance_uuid)
1209 
1210     params = {'instance_uuid': instance_uuid,
1211               'allocated': virtual_interface_id is not None}
1212     if not fixed_ip_ref.network_id:
1213         params['network_id'] = network_id
1214     if virtual_interface_id:
1215         params['virtual_interface_id'] = virtual_interface_id
1216 
1217     rows_updated = model_query(context, models.FixedIp, read_deleted="no").\
1218                             filter_by(id=fixed_ip_ref.id).\
1219                             filter(network_or_none).\
1220                             filter_by(reserved=reserved).\
1221                             filter_by(address=address).\
1222                             update(params, synchronize_session='evaluate')
1223 
1224     if not rows_updated:
1225         LOG.debug('The row was updated in a concurrent transaction, '
1226                   'we will fetch another row')
1227         raise db_exc.RetryRequest(
1228             exception.FixedIpAssociateFailed(net=network_id))
1229 
1230     return fixed_ip_ref
1231 
1232 
1233 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
1234 @pick_context_manager_writer
1235 def fixed_ip_associate_pool(context, network_id, instance_uuid=None,
1236                             host=None, virtual_interface_id=None):
1237     """allocate a fixed ip out of a fixed ip network pool.
1238 
1239     This allocates an unallocated fixed ip out of a specified
1240     network. We sort by updated_at to hand out the oldest address in
1241     the list.
1242 
1243     """
1244     if instance_uuid and not uuidutils.is_uuid_like(instance_uuid):
1245         raise exception.InvalidUUID(uuid=instance_uuid)
1246 
1247     network_or_none = or_(models.FixedIp.network_id == network_id,
1248                           models.FixedIp.network_id == null())
1249     fixed_ip_ref = model_query(context, models.FixedIp, read_deleted="no").\
1250                            filter(network_or_none).\
1251                            filter_by(reserved=False).\
1252                            filter_by(instance_uuid=None).\
1253                            filter_by(host=None).\
1254                            filter_by(leased=False).\
1255                            order_by(asc(models.FixedIp.updated_at)).\
1256                            first()
1257 
1258     if not fixed_ip_ref:
1259         raise exception.NoMoreFixedIps(net=network_id)
1260 
1261     params = {'allocated': virtual_interface_id is not None}
1262     if fixed_ip_ref['network_id'] is None:
1263         params['network_id'] = network_id
1264     if instance_uuid:
1265         params['instance_uuid'] = instance_uuid
1266     if host:
1267         params['host'] = host
1268     if virtual_interface_id:
1269         params['virtual_interface_id'] = virtual_interface_id
1270 
1271     rows_updated = model_query(context, models.FixedIp, read_deleted="no").\
1272         filter_by(id=fixed_ip_ref['id']).\
1273         filter_by(network_id=fixed_ip_ref['network_id']).\
1274         filter_by(reserved=False).\
1275         filter_by(instance_uuid=None).\
1276         filter_by(host=None).\
1277         filter_by(leased=False).\
1278         filter_by(address=fixed_ip_ref['address']).\
1279         update(params, synchronize_session='evaluate')
1280 
1281     if not rows_updated:
1282         LOG.debug('The row was updated in a concurrent transaction, '
1283                   'we will fetch another row')
1284         raise db_exc.RetryRequest(
1285             exception.FixedIpAssociateFailed(net=network_id))
1286 
1287     return fixed_ip_ref
1288 
1289 
1290 @require_context
1291 @pick_context_manager_writer
1292 def fixed_ip_create(context, values):
1293     fixed_ip_ref = models.FixedIp()
1294     fixed_ip_ref.update(values)
1295     try:
1296         fixed_ip_ref.save(context.session)
1297     except db_exc.DBDuplicateEntry:
1298         raise exception.FixedIpExists(address=values['address'])
1299     return fixed_ip_ref
1300 
1301 
1302 @require_context
1303 @pick_context_manager_writer
1304 def fixed_ip_bulk_create(context, ips):
1305     try:
1306         tab = models.FixedIp.__table__
1307         context.session.execute(tab.insert(), ips)
1308     except db_exc.DBDuplicateEntry as e:
1309         raise exception.FixedIpExists(address=e.value)
1310 
1311 
1312 @require_context
1313 @pick_context_manager_writer
1314 def fixed_ip_disassociate(context, address):
1315     _fixed_ip_get_by_address(context, address).update(
1316         {'instance_uuid': None,
1317          'virtual_interface_id': None})
1318 
1319 
1320 @pick_context_manager_writer
1321 def fixed_ip_disassociate_all_by_timeout(context, host, time):
1322     # NOTE(vish): only update fixed ips that "belong" to this
1323     #             host; i.e. the network host or the instance
1324     #             host matches. Two queries necessary because
1325     #             join with update doesn't work.
1326     host_filter = or_(and_(models.Instance.host == host,
1327                            models.Network.multi_host == true()),
1328                       models.Network.host == host)
1329     result = model_query(context, models.FixedIp, (models.FixedIp.id,),
1330                          read_deleted="no").\
1331             filter(models.FixedIp.allocated == false()).\
1332             filter(models.FixedIp.updated_at < time).\
1333             join((models.Network,
1334                   models.Network.id == models.FixedIp.network_id)).\
1335             join((models.Instance,
1336                   models.Instance.uuid == models.FixedIp.instance_uuid)).\
1337             filter(host_filter).\
1338             all()
1339     fixed_ip_ids = [fip[0] for fip in result]
1340     if not fixed_ip_ids:
1341         return 0
1342     result = model_query(context, models.FixedIp).\
1343                          filter(models.FixedIp.id.in_(fixed_ip_ids)).\
1344                          update({'instance_uuid': None,
1345                                  'leased': False,
1346                                  'updated_at': timeutils.utcnow()},
1347                                 synchronize_session='fetch')
1348     return result
1349 
1350 
1351 @require_context
1352 @pick_context_manager_reader
1353 def fixed_ip_get(context, id, get_network=False):
1354     query = model_query(context, models.FixedIp).filter_by(id=id)
1355     if get_network:
1356         query = query.options(joinedload('network'))
1357     result = query.first()
1358     if not result:
1359         raise exception.FixedIpNotFound(id=id)
1360 
1361     # FIXME(sirp): shouldn't we just use project_only here to restrict the
1362     # results?
1363     if (nova.context.is_user_context(context) and
1364             result['instance_uuid'] is not None):
1365         instance = instance_get_by_uuid(context.elevated(read_deleted='yes'),
1366                                         result['instance_uuid'])
1367         nova.context.authorize_project_context(context, instance.project_id)
1368 
1369     return result
1370 
1371 
1372 @pick_context_manager_reader
1373 def fixed_ip_get_all(context):
1374     result = model_query(context, models.FixedIp, read_deleted="yes").all()
1375     if not result:
1376         raise exception.NoFixedIpsDefined()
1377 
1378     return result
1379 
1380 
1381 @require_context
1382 @pick_context_manager_reader
1383 def fixed_ip_get_by_address(context, address, columns_to_join=None):
1384     return _fixed_ip_get_by_address(context, address,
1385                                     columns_to_join=columns_to_join)
1386 
1387 
1388 def _fixed_ip_get_by_address(context, address, columns_to_join=None):
1389     if columns_to_join is None:
1390         columns_to_join = []
1391 
1392     try:
1393         result = model_query(context, models.FixedIp)
1394         for column in columns_to_join:
1395             result = result.options(joinedload_all(column))
1396         result = result.filter_by(address=address).first()
1397         if not result:
1398             raise exception.FixedIpNotFoundForAddress(address=address)
1399     except db_exc.DBError:
1400         msg = _("Invalid fixed IP Address %s in request") % address
1401         LOG.warning(msg)
1402         raise exception.FixedIpInvalid(msg)
1403 
1404     # NOTE(sirp): shouldn't we just use project_only here to restrict the
1405     # results?
1406     if (nova.context.is_user_context(context) and
1407             result['instance_uuid'] is not None):
1408         instance = _instance_get_by_uuid(
1409             context.elevated(read_deleted='yes'),
1410             result['instance_uuid'])
1411         nova.context.authorize_project_context(context,
1412                                                instance.project_id)
1413     return result
1414 
1415 
1416 @require_context
1417 @pick_context_manager_reader
1418 def fixed_ip_get_by_floating_address(context, floating_address):
1419     return model_query(context, models.FixedIp).\
1420                        join(models.FloatingIp,
1421                             models.FloatingIp.fixed_ip_id ==
1422                             models.FixedIp.id).\
1423                        filter(models.FloatingIp.address == floating_address).\
1424                        first()
1425     # NOTE(tr3buchet) please don't invent an exception here, None is fine
1426 
1427 
1428 @require_context
1429 @pick_context_manager_reader
1430 def fixed_ip_get_by_instance(context, instance_uuid):
1431     if not uuidutils.is_uuid_like(instance_uuid):
1432         raise exception.InvalidUUID(uuid=instance_uuid)
1433 
1434     vif_and = and_(models.VirtualInterface.id ==
1435                    models.FixedIp.virtual_interface_id,
1436                    models.VirtualInterface.deleted == 0)
1437     result = model_query(context, models.FixedIp, read_deleted="no").\
1438                  filter_by(instance_uuid=instance_uuid).\
1439                  outerjoin(models.VirtualInterface, vif_and).\
1440                  options(contains_eager("virtual_interface")).\
1441                  options(joinedload('network')).\
1442                  options(joinedload('floating_ips')).\
1443                  order_by(asc(models.VirtualInterface.created_at),
1444                           asc(models.VirtualInterface.id)).\
1445                  all()
1446 
1447     if not result:
1448         raise exception.FixedIpNotFoundForInstance(instance_uuid=instance_uuid)
1449 
1450     return result
1451 
1452 
1453 @pick_context_manager_reader
1454 def fixed_ip_get_by_host(context, host):
1455     instance_uuids = _instance_get_all_uuids_by_host(context, host)
1456     if not instance_uuids:
1457         return []
1458 
1459     return model_query(context, models.FixedIp).\
1460              filter(models.FixedIp.instance_uuid.in_(instance_uuids)).\
1461              all()
1462 
1463 
1464 @require_context
1465 @pick_context_manager_reader
1466 def fixed_ip_get_by_network_host(context, network_id, host):
1467     result = model_query(context, models.FixedIp, read_deleted="no").\
1468                  filter_by(network_id=network_id).\
1469                  filter_by(host=host).\
1470                  first()
1471 
1472     if not result:
1473         raise exception.FixedIpNotFoundForNetworkHost(network_id=network_id,
1474                                                       host=host)
1475     return result
1476 
1477 
1478 @require_context
1479 @pick_context_manager_reader
1480 def fixed_ips_by_virtual_interface(context, vif_id):
1481     result = model_query(context, models.FixedIp, read_deleted="no").\
1482                  filter_by(virtual_interface_id=vif_id).\
1483                  options(joinedload('network')).\
1484                  options(joinedload('floating_ips')).\
1485                  all()
1486 
1487     return result
1488 
1489 
1490 @require_context
1491 @pick_context_manager_writer
1492 def fixed_ip_update(context, address, values):
1493     _fixed_ip_get_by_address(context, address).update(values)
1494 
1495 
1496 def _fixed_ip_count_by_project(context, project_id):
1497     nova.context.authorize_project_context(context, project_id)
1498     return model_query(context, models.FixedIp, (models.FixedIp.id,),
1499                        read_deleted="no").\
1500                 join((models.Instance,
1501                       models.Instance.uuid == models.FixedIp.instance_uuid)).\
1502                 filter(models.Instance.project_id == project_id).\
1503                 count()
1504 
1505 
1506 ###################
1507 
1508 
1509 @require_context
1510 @pick_context_manager_writer
1511 def virtual_interface_create(context, values):
1512     """Create a new virtual interface record in the database.
1513 
1514     :param values: = dict containing column values
1515     """
1516     try:
1517         vif_ref = models.VirtualInterface()
1518         vif_ref.update(values)
1519         vif_ref.save(context.session)
1520     except db_exc.DBError:
1521         LOG.exception("VIF creation failed with a database error.")
1522         raise exception.VirtualInterfaceCreateException()
1523 
1524     return vif_ref
1525 
1526 
1527 def _virtual_interface_query(context):
1528     return model_query(context, models.VirtualInterface, read_deleted="no")
1529 
1530 
1531 @require_context
1532 @pick_context_manager_writer
1533 def virtual_interface_update(context, address, values):
1534     vif_ref = virtual_interface_get_by_address(context, address)
1535     vif_ref.update(values)
1536     vif_ref.save(context.session)
1537     return vif_ref
1538 
1539 
1540 @require_context
1541 @pick_context_manager_reader
1542 def virtual_interface_get(context, vif_id):
1543     """Gets a virtual interface from the table.
1544 
1545     :param vif_id: = id of the virtual interface
1546     """
1547     vif_ref = _virtual_interface_query(context).\
1548                       filter_by(id=vif_id).\
1549                       first()
1550     return vif_ref
1551 
1552 
1553 @require_context
1554 @pick_context_manager_reader
1555 def virtual_interface_get_by_address(context, address):
1556     """Gets a virtual interface from the table.
1557 
1558     :param address: = the address of the interface you're looking to get
1559     """
1560     try:
1561         vif_ref = _virtual_interface_query(context).\
1562                           filter_by(address=address).\
1563                           first()
1564     except db_exc.DBError:
1565         msg = _("Invalid virtual interface address %s in request") % address
1566         LOG.warning(msg)
1567         raise exception.InvalidIpAddressError(msg)
1568     return vif_ref
1569 
1570 
1571 @require_context
1572 @pick_context_manager_reader
1573 def virtual_interface_get_by_uuid(context, vif_uuid):
1574     """Gets a virtual interface from the table.
1575 
1576     :param vif_uuid: the uuid of the interface you're looking to get
1577     """
1578     vif_ref = _virtual_interface_query(context).\
1579                       filter_by(uuid=vif_uuid).\
1580                       first()
1581     return vif_ref
1582 
1583 
1584 @require_context
1585 @pick_context_manager_reader_allow_async
1586 def virtual_interface_get_by_instance(context, instance_uuid):
1587     """Gets all virtual interfaces for instance.
1588 
1589     :param instance_uuid: = uuid of the instance to retrieve vifs for
1590     """
1591     vif_refs = _virtual_interface_query(context).\
1592                        filter_by(instance_uuid=instance_uuid).\
1593                        order_by(asc("created_at"), asc("id")).\
1594                        all()
1595     return vif_refs
1596 
1597 
1598 @require_context
1599 @pick_context_manager_reader
1600 def virtual_interface_get_by_instance_and_network(context, instance_uuid,
1601                                                   network_id):
1602     """Gets virtual interface for instance that's associated with network."""
1603     vif_ref = _virtual_interface_query(context).\
1604                       filter_by(instance_uuid=instance_uuid).\
1605                       filter_by(network_id=network_id).\
1606                       first()
1607     return vif_ref
1608 
1609 
1610 @require_context
1611 @pick_context_manager_writer
1612 def virtual_interface_delete_by_instance(context, instance_uuid):
1613     """Delete virtual interface records that are associated
1614     with the instance given by instance_id.
1615 
1616     :param instance_uuid: = uuid of instance
1617     """
1618     _virtual_interface_query(context).\
1619            filter_by(instance_uuid=instance_uuid).\
1620            soft_delete()
1621 
1622 
1623 @require_context
1624 @pick_context_manager_writer
1625 def virtual_interface_delete(context, id):
1626     """Delete virtual interface records.
1627 
1628     :param id: id of the interface
1629     """
1630     _virtual_interface_query(context).\
1631         filter_by(id=id).\
1632         soft_delete()
1633 
1634 
1635 @require_context
1636 @pick_context_manager_reader
1637 def virtual_interface_get_all(context):
1638     """Get all vifs."""
1639     vif_refs = _virtual_interface_query(context).all()
1640     return vif_refs
1641 
1642 
1643 ###################
1644 
1645 
1646 def _metadata_refs(metadata_dict, meta_class):
1647     metadata_refs = []
1648     if metadata_dict:
1649         for k, v in metadata_dict.items():
1650             metadata_ref = meta_class()
1651             metadata_ref['key'] = k
1652             metadata_ref['value'] = v
1653             metadata_refs.append(metadata_ref)
1654     return metadata_refs
1655 
1656 
1657 def _validate_unique_server_name(context, name):
1658     if not CONF.osapi_compute_unique_server_name_scope:
1659         return
1660 
1661     lowername = name.lower()
1662     base_query = model_query(context, models.Instance, read_deleted='no').\
1663             filter(func.lower(models.Instance.hostname) == lowername)
1664 
1665     if CONF.osapi_compute_unique_server_name_scope == 'project':
1666         instance_with_same_name = base_query.\
1667                         filter_by(project_id=context.project_id).\
1668                         count()
1669 
1670     elif CONF.osapi_compute_unique_server_name_scope == 'global':
1671         instance_with_same_name = base_query.count()
1672 
1673     else:
1674         return
1675 
1676     if instance_with_same_name > 0:
1677         raise exception.InstanceExists(name=lowername)
1678 
1679 
1680 def _handle_objects_related_type_conversions(values):
1681     """Make sure that certain things in values (which may have come from
1682     an objects.instance.Instance object) are in suitable form for the
1683     database.
1684     """
1685     # NOTE(danms): Make sure IP addresses are passed as strings to
1686     # the database engine
1687     for key in ('access_ip_v4', 'access_ip_v6'):
1688         if key in values and values[key] is not None:
1689             values[key] = str(values[key])
1690 
1691     datetime_keys = ('created_at', 'deleted_at', 'updated_at',
1692                      'launched_at', 'terminated_at')
1693     convert_objects_related_datetimes(values, *datetime_keys)
1694 
1695 
1696 def _check_instance_exists_in_project(context, instance_uuid):
1697     if not model_query(context, models.Instance, read_deleted="no",
1698                        project_only=True).filter_by(
1699                        uuid=instance_uuid).first():
1700         raise exception.InstanceNotFound(instance_id=instance_uuid)
1701 
1702 
1703 @require_context
1704 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
1705 @pick_context_manager_writer
1706 def instance_create(context, values):
1707     """Create a new Instance record in the database.
1708 
1709     context - request context object
1710     values - dict containing column values.
1711     """
1712 
1713     security_group_ensure_default(context)
1714 
1715     values = values.copy()
1716     values['metadata'] = _metadata_refs(
1717             values.get('metadata'), models.InstanceMetadata)
1718 
1719     values['system_metadata'] = _metadata_refs(
1720             values.get('system_metadata'), models.InstanceSystemMetadata)
1721     _handle_objects_related_type_conversions(values)
1722 
1723     instance_ref = models.Instance()
1724     if not values.get('uuid'):
1725         values['uuid'] = uuidutils.generate_uuid()
1726     instance_ref['info_cache'] = models.InstanceInfoCache()
1727     info_cache = values.pop('info_cache', None)
1728     if info_cache is not None:
1729         instance_ref['info_cache'].update(info_cache)
1730     security_groups = values.pop('security_groups', [])
1731     instance_ref['extra'] = models.InstanceExtra()
1732     instance_ref['extra'].update(
1733         {'numa_topology': None,
1734          'pci_requests': None,
1735          'vcpu_model': None,
1736          'trusted_certs': None,
1737          })
1738     instance_ref['extra'].update(values.pop('extra', {}))
1739     instance_ref.update(values)
1740 
1741     def _get_sec_group_models(security_groups):
1742         models = []
1743         default_group = _security_group_ensure_default(context)
1744         if 'default' in security_groups:
1745             models.append(default_group)
1746             # Generate a new list, so we don't modify the original
1747             security_groups = [x for x in security_groups if x != 'default']
1748         if security_groups:
1749             models.extend(_security_group_get_by_names(
1750                 context, security_groups))
1751         return models
1752 
1753     if 'hostname' in values:
1754         _validate_unique_server_name(context, values['hostname'])
1755     instance_ref.security_groups = _get_sec_group_models(security_groups)
1756     context.session.add(instance_ref)
1757 
1758     # create the instance uuid to ec2_id mapping entry for instance
1759     ec2_instance_create(context, instance_ref['uuid'])
1760 
1761     # Parity with the return value of instance_get_all_by_filters_sort()
1762     # Obviously a newly-created instance record can't already have a fault
1763     # record because of the FK constraint, so this is fine.
1764     instance_ref.fault = None
1765 
1766     return instance_ref
1767 
1768 
1769 @require_context
1770 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
1771 @pick_context_manager_writer
1772 def instance_destroy(context, instance_uuid, constraint=None,
1773                      hard_delete=False):
1774     if uuidutils.is_uuid_like(instance_uuid):
1775         instance_ref = _instance_get_by_uuid(context, instance_uuid)
1776     else:
1777         raise exception.InvalidUUID(uuid=instance_uuid)
1778 
1779     query = model_query(context, models.Instance).\
1780                     filter_by(uuid=instance_uuid)
1781     if constraint is not None:
1782         query = constraint.apply(models.Instance, query)
1783     # Either in hard or soft delete, we soft delete the instance first
1784     # to make sure that that the constraints were met.
1785     count = query.soft_delete()
1786     if count == 0:
1787         raise exception.ConstraintNotMet()
1788 
1789     models_to_delete = [
1790         models.SecurityGroupInstanceAssociation, models.InstanceInfoCache,
1791         models.InstanceMetadata, models.InstanceFault, models.InstanceExtra,
1792         models.InstanceSystemMetadata, models.BlockDeviceMapping,
1793         models.Migration, models.VirtualInterface
1794     ]
1795 
1796     # For most referenced models we filter by the instance_uuid column, but for
1797     # these models we filter by the uuid column.
1798     filtered_by_uuid = [models.InstanceIdMapping]
1799 
1800     for model in models_to_delete + filtered_by_uuid:
1801         key = 'instance_uuid' if model not in filtered_by_uuid else 'uuid'
1802         filter_ = {key: instance_uuid}
1803         if hard_delete:
1804             model_query(context, model).filter_by(**filter_).delete()
1805         else:
1806             model_query(context, model).filter_by(**filter_).soft_delete()
1807 
1808     # NOTE(snikitin): We can't use model_query here, because there is no
1809     # column 'deleted' in 'tags' or 'console_auth_tokens' tables.
1810     context.session.query(models.Tag).filter_by(
1811         resource_id=instance_uuid).delete()
1812     context.session.query(models.ConsoleAuthToken).filter_by(
1813         instance_uuid=instance_uuid).delete()
1814     # NOTE(cfriesen): We intentionally do not soft-delete entries in the
1815     # instance_actions or instance_actions_events tables because they
1816     # can be used by operators to find out what actions were performed on a
1817     # deleted instance.  Both of these tables are special-cased in
1818     # _archive_deleted_rows_for_table().
1819     if hard_delete:
1820         # NOTE(ttsiousts): In case of hard delete, we need to remove the
1821         # instance actions too since instance_uuid is a foreign key and
1822         # for this we need to delete the corresponding InstanceActionEvents
1823         actions = context.session.query(models.InstanceAction).filter_by(
1824             instance_uuid=instance_uuid).all()
1825         for action in actions:
1826             context.session.query(models.InstanceActionEvent).filter_by(
1827                 action_id=action.id).delete()
1828         context.session.query(models.InstanceAction).filter_by(
1829             instance_uuid=instance_uuid).delete()
1830         # NOTE(ttsiouts): The instance is the last thing to be deleted in
1831         # order to respect all constraints
1832         context.session.query(models.Instance).filter_by(
1833             uuid=instance_uuid).delete()
1834 
1835     return instance_ref
1836 
1837 
1838 @require_context
1839 @pick_context_manager_reader_allow_async
1840 def instance_get_by_uuid(context, uuid, columns_to_join=None):
1841     return _instance_get_by_uuid(context, uuid,
1842                                  columns_to_join=columns_to_join)
1843 
1844 
1845 def _instance_get_by_uuid(context, uuid, columns_to_join=None):
1846     result = _build_instance_get(context, columns_to_join=columns_to_join).\
1847                 filter_by(uuid=uuid).\
1848                 first()
1849 
1850     if not result:
1851         raise exception.InstanceNotFound(instance_id=uuid)
1852 
1853     return result
1854 
1855 
1856 @require_context
1857 @pick_context_manager_reader
1858 def instance_get(context, instance_id, columns_to_join=None):
1859     try:
1860         result = _build_instance_get(context, columns_to_join=columns_to_join
1861                                      ).filter_by(id=instance_id).first()
1862 
1863         if not result:
1864             raise exception.InstanceNotFound(instance_id=instance_id)
1865 
1866         return result
1867     except db_exc.DBError:
1868         # NOTE(sdague): catch all in case the db engine chokes on the
1869         # id because it's too long of an int to store.
1870         LOG.warning("Invalid instance id %s in request", instance_id)
1871         raise exception.InvalidID(id=instance_id)
1872 
1873 
1874 def _build_instance_get(context, columns_to_join=None):
1875     query = model_query(context, models.Instance, project_only=True).\
1876             options(joinedload_all('security_groups.rules')).\
1877             options(joinedload('info_cache'))
1878     if columns_to_join is None:
1879         columns_to_join = ['metadata', 'system_metadata']
1880     for column in columns_to_join:
1881         if column in ['info_cache', 'security_groups']:
1882             # Already always joined above
1883             continue
1884         if 'extra.' in column:
1885             query = query.options(undefer(column))
1886         else:
1887             query = query.options(joinedload(column))
1888     # NOTE(alaski) Stop lazy loading of columns not needed.
1889     for col in ['metadata', 'system_metadata']:
1890         if col not in columns_to_join:
1891             query = query.options(noload(col))
1892     return query
1893 
1894 
1895 def _instances_fill_metadata(context, instances, manual_joins=None):
1896     """Selectively fill instances with manually-joined metadata. Note that
1897     instance will be converted to a dict.
1898 
1899     :param context: security context
1900     :param instances: list of instances to fill
1901     :param manual_joins: list of tables to manually join (can be any
1902                          combination of 'metadata' and 'system_metadata' or
1903                          None to take the default of both)
1904     """
1905     uuids = [inst['uuid'] for inst in instances]
1906 
1907     if manual_joins is None:
1908         manual_joins = ['metadata', 'system_metadata']
1909 
1910     meta = collections.defaultdict(list)
1911     if 'metadata' in manual_joins:
1912         for row in _instance_metadata_get_multi(context, uuids):
1913             meta[row['instance_uuid']].append(row)
1914 
1915     sys_meta = collections.defaultdict(list)
1916     if 'system_metadata' in manual_joins:
1917         for row in _instance_system_metadata_get_multi(context, uuids):
1918             sys_meta[row['instance_uuid']].append(row)
1919 
1920     pcidevs = collections.defaultdict(list)
1921     if 'pci_devices' in manual_joins:
1922         for row in _instance_pcidevs_get_multi(context, uuids):
1923             pcidevs[row['instance_uuid']].append(row)
1924 
1925     if 'fault' in manual_joins:
1926         faults = instance_fault_get_by_instance_uuids(context, uuids,
1927                                                       latest=True)
1928     else:
1929         faults = {}
1930 
1931     filled_instances = []
1932     for inst in instances:
1933         inst = dict(inst)
1934         inst['system_metadata'] = sys_meta[inst['uuid']]
1935         inst['metadata'] = meta[inst['uuid']]
1936         if 'pci_devices' in manual_joins:
1937             inst['pci_devices'] = pcidevs[inst['uuid']]
1938         inst_faults = faults.get(inst['uuid'])
1939         inst['fault'] = inst_faults and inst_faults[0] or None
1940         filled_instances.append(inst)
1941 
1942     return filled_instances
1943 
1944 
1945 def _manual_join_columns(columns_to_join):
1946     """Separate manually joined columns from columns_to_join
1947 
1948     If columns_to_join contains 'metadata', 'system_metadata', 'fault', or
1949     'pci_devices' those columns are removed from columns_to_join and added
1950     to a manual_joins list to be used with the _instances_fill_metadata method.
1951 
1952     The columns_to_join formal parameter is copied and not modified, the return
1953     tuple has the modified columns_to_join list to be used with joinedload in
1954     a model query.
1955 
1956     :param:columns_to_join: List of columns to join in a model query.
1957     :return: tuple of (manual_joins, columns_to_join)
1958     """
1959     manual_joins = []
1960     columns_to_join_new = copy.copy(columns_to_join)
1961     for column in ('metadata', 'system_metadata', 'pci_devices', 'fault'):
1962         if column in columns_to_join_new:
1963             columns_to_join_new.remove(column)
1964             manual_joins.append(column)
1965     return manual_joins, columns_to_join_new
1966 
1967 
1968 @require_context
1969 @pick_context_manager_reader
1970 def instance_get_all(context, columns_to_join=None):
1971     if columns_to_join is None:
1972         columns_to_join_new = ['info_cache', 'security_groups']
1973         manual_joins = ['metadata', 'system_metadata']
1974     else:
1975         manual_joins, columns_to_join_new = (
1976             _manual_join_columns(columns_to_join))
1977     query = model_query(context, models.Instance)
1978     for column in columns_to_join_new:
1979         query = query.options(joinedload(column))
1980     if not context.is_admin:
1981         # If we're not admin context, add appropriate filter..
1982         if context.project_id:
1983             query = query.filter_by(project_id=context.project_id)
1984         else:
1985             query = query.filter_by(user_id=context.user_id)
1986     instances = query.all()
1987     return _instances_fill_metadata(context, instances, manual_joins)
1988 
1989 
1990 @require_context
1991 @pick_context_manager_reader_allow_async
1992 def instance_get_all_by_filters(context, filters, sort_key, sort_dir,
1993                                 limit=None, marker=None, columns_to_join=None):
1994     """Return instances matching all filters sorted by the primary key.
1995 
1996     See instance_get_all_by_filters_sort for more information.
1997     """
1998     # Invoke the API with the multiple sort keys and directions using the
1999     # single sort key/direction
2000     return instance_get_all_by_filters_sort(context, filters, limit=limit,
2001                                             marker=marker,
2002                                             columns_to_join=columns_to_join,
2003                                             sort_keys=[sort_key],
2004                                             sort_dirs=[sort_dir])
2005 
2006 
2007 def _get_query_nova_resource_by_changes_time(query, filters, model_object):
2008     """Filter resources by changes-since or changes-before.
2009 
2010     Special keys are used to tweek the query further::
2011 
2012     |   'changes-since' - only return resources updated after
2013     |   'changes-before' - only return resources updated before
2014 
2015     Return query results.
2016 
2017     :param query: query to apply filters to.
2018     :param filters: dictionary of filters with regex values.
2019     :param model_object: object of the operation target.
2020     """
2021     for change_filter in ['changes-since', 'changes-before']:
2022         if filters and filters.get(change_filter):
2023             changes_filter_time = timeutils.normalize_time(
2024                 filters.get(change_filter))
2025             updated_at = getattr(model_object, 'updated_at')
2026             if change_filter == 'changes-since':
2027                 query = query.filter(updated_at >= changes_filter_time)
2028             else:
2029                 query = query.filter(updated_at <= changes_filter_time)
2030     return query
2031 
2032 
2033 @require_context
2034 @pick_context_manager_reader_allow_async
2035 def instance_get_all_by_filters_sort(context, filters, limit=None, marker=None,
2036                                      columns_to_join=None, sort_keys=None,
2037                                      sort_dirs=None):
2038     """Return instances that match all filters sorted by the given keys.
2039     Deleted instances will be returned by default, unless there's a filter that
2040     says otherwise.
2041 
2042     Depending on the name of a filter, matching for that filter is
2043     performed using either exact matching or as regular expression
2044     matching. Exact matching is applied for the following filters::
2045 
2046     |   ['project_id', 'user_id', 'image_ref',
2047     |    'vm_state', 'instance_type_id', 'uuid',
2048     |    'metadata', 'host', 'system_metadata']
2049 
2050 
2051     A third type of filter (also using exact matching), filters
2052     based on instance metadata tags when supplied under a special
2053     key named 'filter'::
2054 
2055     |   filters = {
2056     |       'filter': [
2057     |           {'name': 'tag-key', 'value': '<metakey>'},
2058     |           {'name': 'tag-value', 'value': '<metaval>'},
2059     |           {'name': 'tag:<metakey>', 'value': '<metaval>'}
2060     |       ]
2061     |   }
2062 
2063     Special keys are used to tweek the query further::
2064 
2065     |   'changes-since' - only return instances updated after
2066     |   'changes-before' - only return instances updated before
2067     |   'deleted' - only return (or exclude) deleted instances
2068     |   'soft_deleted' - modify behavior of 'deleted' to either
2069     |                    include or exclude instances whose
2070     |                    vm_state is SOFT_DELETED.
2071 
2072     A fourth type of filter (also using exact matching), filters
2073     based on instance tags (not metadata tags). There are two types
2074     of these tags:
2075 
2076     `tags` -- One or more strings that will be used to filter results
2077             in an AND expression: T1 AND T2
2078 
2079     `tags-any` -- One or more strings that will be used to filter results in
2080             an OR expression: T1 OR T2
2081 
2082     `not-tags` -- One or more strings that will be used to filter results in
2083             an NOT AND expression: NOT (T1 AND T2)
2084 
2085     `not-tags-any` -- One or more strings that will be used to filter results
2086             in an NOT OR expression: NOT (T1 OR T2)
2087 
2088     Tags should be represented as list::
2089 
2090     |    filters = {
2091     |        'tags': [some-tag, some-another-tag],
2092     |        'tags-any: [some-any-tag, some-another-any-tag],
2093     |        'not-tags: [some-not-tag, some-another-not-tag],
2094     |        'not-tags-any: [some-not-any-tag, some-another-not-any-tag]
2095     |    }
2096 
2097     """
2098     # NOTE(mriedem): If the limit is 0 there is no point in even going
2099     # to the database since nothing is going to be returned anyway.
2100     if limit == 0:
2101         return []
2102 
2103     sort_keys, sort_dirs = process_sort_params(sort_keys,
2104                                                sort_dirs,
2105                                                default_dir='desc')
2106 
2107     if columns_to_join is None:
2108         columns_to_join_new = ['info_cache', 'security_groups']
2109         manual_joins = ['metadata', 'system_metadata']
2110     else:
2111         manual_joins, columns_to_join_new = (
2112             _manual_join_columns(columns_to_join))
2113 
2114     query_prefix = context.session.query(models.Instance)
2115     for column in columns_to_join_new:
2116         if 'extra.' in column:
2117             query_prefix = query_prefix.options(undefer(column))
2118         else:
2119             query_prefix = query_prefix.options(joinedload(column))
2120 
2121     # Note: order_by is done in the sqlalchemy.utils.py paginate_query(),
2122     # no need to do it here as well
2123 
2124     # Make a copy of the filters dictionary to use going forward, as we'll
2125     # be modifying it and we shouldn't affect the caller's use of it.
2126     filters = copy.deepcopy(filters)
2127 
2128     model_object = models.Instance
2129     query_prefix = _get_query_nova_resource_by_changes_time(query_prefix,
2130                                                             filters,
2131                                                             model_object)
2132 
2133     if 'deleted' in filters:
2134         # Instances can be soft or hard deleted and the query needs to
2135         # include or exclude both
2136         deleted = filters.pop('deleted')
2137         if deleted:
2138             if filters.pop('soft_deleted', True):
2139                 delete = or_(
2140                     models.Instance.deleted == models.Instance.id,
2141                     models.Instance.vm_state == vm_states.SOFT_DELETED
2142                     )
2143                 query_prefix = query_prefix.\
2144                     filter(delete)
2145             else:
2146                 query_prefix = query_prefix.\
2147                     filter(models.Instance.deleted == models.Instance.id)
2148         else:
2149             query_prefix = query_prefix.\
2150                     filter_by(deleted=0)
2151             if not filters.pop('soft_deleted', False):
2152                 # It would be better to have vm_state not be nullable
2153                 # but until then we test it explicitly as a workaround.
2154                 not_soft_deleted = or_(
2155                     models.Instance.vm_state != vm_states.SOFT_DELETED,
2156                     models.Instance.vm_state == null()
2157                     )
2158                 query_prefix = query_prefix.filter(not_soft_deleted)
2159 
2160     if 'cleaned' in filters:
2161         cleaned = 1 if filters.pop('cleaned') else 0
2162         query_prefix = query_prefix.filter(models.Instance.cleaned == cleaned)
2163 
2164     if 'tags' in filters:
2165         tags = filters.pop('tags')
2166         # We build a JOIN ladder expression for each tag, JOIN'ing
2167         # the first tag to the instances table, and each subsequent
2168         # tag to the last JOIN'd tags table
2169         first_tag = tags.pop(0)
2170         query_prefix = query_prefix.join(models.Instance.tags)
2171         query_prefix = query_prefix.filter(models.Tag.tag == first_tag)
2172 
2173         for tag in tags:
2174             tag_alias = aliased(models.Tag)
2175             query_prefix = query_prefix.join(tag_alias,
2176                                              models.Instance.tags)
2177             query_prefix = query_prefix.filter(tag_alias.tag == tag)
2178 
2179     if 'tags-any' in filters:
2180         tags = filters.pop('tags-any')
2181         tag_alias = aliased(models.Tag)
2182         query_prefix = query_prefix.join(tag_alias, models.Instance.tags)
2183         query_prefix = query_prefix.filter(tag_alias.tag.in_(tags))
2184 
2185     if 'not-tags' in filters:
2186         tags = filters.pop('not-tags')
2187         first_tag = tags.pop(0)
2188         subq = query_prefix.session.query(models.Tag.resource_id)
2189         subq = subq.join(models.Instance.tags)
2190         subq = subq.filter(models.Tag.tag == first_tag)
2191 
2192         for tag in tags:
2193             tag_alias = aliased(models.Tag)
2194             subq = subq.join(tag_alias, models.Instance.tags)
2195             subq = subq.filter(tag_alias.tag == tag)
2196 
2197         query_prefix = query_prefix.filter(~models.Instance.uuid.in_(subq))
2198 
2199     if 'not-tags-any' in filters:
2200         tags = filters.pop('not-tags-any')
2201         query_prefix = query_prefix.filter(~models.Instance.tags.any(
2202             models.Tag.tag.in_(tags)))
2203 
2204     if not context.is_admin:
2205         # If we're not admin context, add appropriate filter..
2206         if context.project_id:
2207             filters['project_id'] = context.project_id
2208         else:
2209             filters['user_id'] = context.user_id
2210 
2211     # Filters for exact matches that we can do along with the SQL query...
2212     # For other filters that don't match this, we will do regexp matching
2213     exact_match_filter_names = ['project_id', 'user_id', 'image_ref',
2214                                 'vm_state', 'instance_type_id', 'uuid',
2215                                 'metadata', 'host', 'task_state',
2216                                 'system_metadata']
2217 
2218     # Filter the query
2219     query_prefix = _exact_instance_filter(query_prefix,
2220                                 filters, exact_match_filter_names)
2221     if query_prefix is None:
2222         return []
2223     query_prefix = _regex_instance_filter(query_prefix, filters)
2224 
2225     # paginate query
2226     if marker is not None:
2227         try:
2228             marker = _instance_get_by_uuid(
2229                     context.elevated(read_deleted='yes'), marker)
2230         except exception.InstanceNotFound:
2231             raise exception.MarkerNotFound(marker=marker)
2232     try:
2233         query_prefix = sqlalchemyutils.paginate_query(query_prefix,
2234                                models.Instance, limit,
2235                                sort_keys,
2236                                marker=marker,
2237                                sort_dirs=sort_dirs)
2238     except db_exc.InvalidSortKey:
2239         raise exception.InvalidSortKey()
2240 
2241     return _instances_fill_metadata(context, query_prefix.all(), manual_joins)
2242 
2243 
2244 @require_context
2245 @pick_context_manager_reader_allow_async
2246 def instance_get_by_sort_filters(context, sort_keys, sort_dirs, values):
2247     """Attempt to get a single instance based on a combination of sort
2248     keys, directions and filter values. This is used to try to find a
2249     marker instance when we don't have a marker uuid.
2250 
2251     This returns just a uuid of the instance that matched.
2252     """
2253 
2254     model = models.Instance
2255     return _model_get_uuid_by_sort_filters(context, model, sort_keys,
2256                                            sort_dirs, values)
2257 
2258 
2259 def _model_get_uuid_by_sort_filters(context, model, sort_keys, sort_dirs,
2260                                     values):
2261     query = context.session.query(model.uuid)
2262 
2263     # NOTE(danms): Below is a re-implementation of our
2264     # oslo_db.sqlalchemy.utils.paginate_query() utility. We can't use that
2265     # directly because it does not return the marker and we need it to.
2266     # The below is basically the same algorithm, stripped down to just what
2267     # we need, and augmented with the filter criteria required for us to
2268     # get back the instance that would correspond to our query.
2269 
2270     # This is our position in sort_keys,sort_dirs,values for the loop below
2271     key_index = 0
2272 
2273     # We build a list of criteria to apply to the query, which looks
2274     # approximately like this (assuming all ascending):
2275     #
2276     #  OR(row.key1 > val1,
2277     #     AND(row.key1 == val1, row.key2 > val2),
2278     #     AND(row.key1 == val1, row.key2 == val2, row.key3 >= val3),
2279     #  )
2280     #
2281     # The final key is compared with the "or equal" variant so that
2282     # a complete match instance is still returned.
2283     criteria = []
2284 
2285     for skey, sdir, val in zip(sort_keys, sort_dirs, values):
2286         # Apply ordering to our query for the key, direction we're processing
2287         if sdir == 'desc':
2288             query = query.order_by(desc(getattr(model, skey)))
2289         else:
2290             query = query.order_by(asc(getattr(model, skey)))
2291 
2292         # Build a list of equivalence requirements on keys we've already
2293         # processed through the loop. In other words, if we're adding
2294         # key2 > val2, make sure that key1 == val1
2295         crit_attrs = []
2296         for equal_attr in range(0, key_index):
2297             crit_attrs.append(
2298                 (getattr(model, sort_keys[equal_attr]) == values[equal_attr]))
2299 
2300         model_attr = getattr(model, skey)
2301         if isinstance(model_attr.type, Boolean):
2302             model_attr = cast(model_attr, Integer)
2303             val = int(val)
2304 
2305         if skey == sort_keys[-1]:
2306             # If we are the last key, then we should use or-equal to
2307             # allow a complete match to be returned
2308             if sdir == 'asc':
2309                 crit = (model_attr >= val)
2310             else:
2311                 crit = (model_attr <= val)
2312         else:
2313             # If we're not the last key, then strict greater or less than
2314             # so we order strictly.
2315             if sdir == 'asc':
2316                 crit = (model_attr > val)
2317             else:
2318                 crit = (model_attr < val)
2319 
2320         # AND together all the above
2321         crit_attrs.append(crit)
2322         criteria.append(and_(*crit_attrs))
2323         key_index += 1
2324 
2325     # OR together all the ANDs
2326     query = query.filter(or_(*criteria))
2327 
2328     # We can't raise InstanceNotFound because we don't have a uuid to
2329     # be looking for, so just return nothing if no match.
2330     result = query.limit(1).first()
2331     if result:
2332         # We're querying for a single column, which means we get back a
2333         # tuple of one thing. Strip that out and just return the uuid
2334         # for our caller.
2335         return result[0]
2336     else:
2337         return result
2338 
2339 
2340 def _db_connection_type(db_connection):
2341     """Returns a lowercase symbol for the db type.
2342 
2343     This is useful when we need to change what we are doing per DB
2344     (like handling regexes). In a CellsV2 world it probably needs to
2345     do something better than use the database configuration string.
2346     """
2347 
2348     db_string = db_connection.split(':')[0].split('+')[0]
2349     return db_string.lower()
2350 
2351 
2352 def _safe_regex_mysql(raw_string):
2353     """Make regex safe to mysql.
2354 
2355     Certain items like '|' are interpreted raw by mysql REGEX. If you
2356     search for a single | then you trigger an error because it's
2357     expecting content on either side.
2358 
2359     For consistency sake we escape all '|'. This does mean we wouldn't
2360     support something like foo|bar to match completely different
2361     things, however, one can argue putting such complicated regex into
2362     name search probably means you are doing this wrong.
2363     """
2364     return raw_string.replace('|', '\\|')
2365 
2366 
2367 def _get_regexp_ops(connection):
2368     """Return safety filter and db opts for regex."""
2369     regexp_op_map = {
2370         'postgresql': '~',
2371         'mysql': 'REGEXP',
2372         'sqlite': 'REGEXP'
2373     }
2374     regex_safe_filters = {
2375         'mysql': _safe_regex_mysql
2376     }
2377     db_type = _db_connection_type(connection)
2378 
2379     return (regex_safe_filters.get(db_type, lambda x: x),
2380             regexp_op_map.get(db_type, 'LIKE'))
2381 
2382 
2383 def _regex_instance_filter(query, filters):
2384 
2385     """Applies regular expression filtering to an Instance query.
2386 
2387     Returns the updated query.
2388 
2389     :param query: query to apply filters to
2390     :param filters: dictionary of filters with regex values
2391     """
2392 
2393     model = models.Instance
2394     safe_regex_filter, db_regexp_op = _get_regexp_ops(CONF.database.connection)
2395     for filter_name in filters:
2396         try:
2397             column_attr = getattr(model, filter_name)
2398         except AttributeError:
2399             continue
2400         if 'property' == type(column_attr).__name__:
2401             continue
2402         filter_val = filters[filter_name]
2403         # Sometimes the REGEX filter value is not a string
2404         if not isinstance(filter_val, six.string_types):
2405             filter_val = str(filter_val)
2406         if db_regexp_op == 'LIKE':
2407             query = query.filter(column_attr.op(db_regexp_op)(
2408                                  u'%' + filter_val + u'%'))
2409         else:
2410             filter_val = safe_regex_filter(filter_val)
2411             query = query.filter(column_attr.op(db_regexp_op)(
2412                                  filter_val))
2413     return query
2414 
2415 
2416 def _exact_instance_filter(query, filters, legal_keys):
2417     """Applies exact match filtering to an Instance query.
2418 
2419     Returns the updated query.  Modifies filters argument to remove
2420     filters consumed.
2421 
2422     :param query: query to apply filters to
2423     :param filters: dictionary of filters; values that are lists,
2424                     tuples, sets, or frozensets cause an 'IN' test to
2425                     be performed, while exact matching ('==' operator)
2426                     is used for other values
2427     :param legal_keys: list of keys to apply exact filtering to
2428     """
2429 
2430     filter_dict = {}
2431     model = models.Instance
2432 
2433     # Walk through all the keys
2434     for key in legal_keys:
2435         # Skip ones we're not filtering on
2436         if key not in filters:
2437             continue
2438 
2439         # OK, filtering on this key; what value do we search for?
2440         value = filters.pop(key)
2441 
2442         if key in ('metadata', 'system_metadata'):
2443             column_attr = getattr(model, key)
2444             if isinstance(value, list):
2445                 for item in value:
2446                     for k, v in item.items():
2447                         query = query.filter(column_attr.any(key=k))
2448                         query = query.filter(column_attr.any(value=v))
2449 
2450             else:
2451                 for k, v in value.items():
2452                     query = query.filter(column_attr.any(key=k))
2453                     query = query.filter(column_attr.any(value=v))
2454         elif isinstance(value, (list, tuple, set, frozenset)):
2455             if not value:
2456                 return None  # empty IN-predicate; short circuit
2457             # Looking for values in a list; apply to query directly
2458             column_attr = getattr(model, key)
2459             query = query.filter(column_attr.in_(value))
2460         else:
2461             # OK, simple exact match; save for later
2462             filter_dict[key] = value
2463 
2464     # Apply simple exact matches
2465     if filter_dict:
2466         query = query.filter(*[getattr(models.Instance, k) == v
2467                                for k, v in filter_dict.items()])
2468     return query
2469 
2470 
2471 def process_sort_params(sort_keys, sort_dirs,
2472                         default_keys=['created_at', 'id'],
2473                         default_dir='asc'):
2474     """Process the sort parameters to include default keys.
2475 
2476     Creates a list of sort keys and a list of sort directions. Adds the default
2477     keys to the end of the list if they are not already included.
2478 
2479     When adding the default keys to the sort keys list, the associated
2480     direction is:
2481     1) The first element in the 'sort_dirs' list (if specified), else
2482     2) 'default_dir' value (Note that 'asc' is the default value since this is
2483     the default in sqlalchemy.utils.paginate_query)
2484 
2485     :param sort_keys: List of sort keys to include in the processed list
2486     :param sort_dirs: List of sort directions to include in the processed list
2487     :param default_keys: List of sort keys that need to be included in the
2488                          processed list, they are added at the end of the list
2489                          if not already specified.
2490     :param default_dir: Sort direction associated with each of the default
2491                         keys that are not supplied, used when they are added
2492                         to the processed list
2493     :returns: list of sort keys, list of sort directions
2494     :raise exception.InvalidInput: If more sort directions than sort keys
2495                                    are specified or if an invalid sort
2496                                    direction is specified
2497     """
2498     # Determine direction to use for when adding default keys
2499     if sort_dirs and len(sort_dirs) != 0:
2500         default_dir_value = sort_dirs[0]
2501     else:
2502         default_dir_value = default_dir
2503 
2504     # Create list of keys (do not modify the input list)
2505     if sort_keys:
2506         result_keys = list(sort_keys)
2507     else:
2508         result_keys = []
2509 
2510     # If a list of directions is not provided, use the default sort direction
2511     # for all provided keys
2512     if sort_dirs:
2513         result_dirs = []
2514         # Verify sort direction
2515         for sort_dir in sort_dirs:
2516             if sort_dir not in ('asc', 'desc'):
2517                 msg = _("Unknown sort direction, must be 'desc' or 'asc'")
2518                 raise exception.InvalidInput(reason=msg)
2519             result_dirs.append(sort_dir)
2520     else:
2521         result_dirs = [default_dir_value for _sort_key in result_keys]
2522 
2523     # Ensure that the key and direction length match
2524     while len(result_dirs) < len(result_keys):
2525         result_dirs.append(default_dir_value)
2526     # Unless more direction are specified, which is an error
2527     if len(result_dirs) > len(result_keys):
2528         msg = _("Sort direction size exceeds sort key size")
2529         raise exception.InvalidInput(reason=msg)
2530 
2531     # Ensure defaults are included
2532     for key in default_keys:
2533         if key not in result_keys:
2534             result_keys.append(key)
2535             result_dirs.append(default_dir_value)
2536 
2537     return result_keys, result_dirs
2538 
2539 
2540 @require_context
2541 @pick_context_manager_reader_allow_async
2542 def instance_get_active_by_window_joined(context, begin, end=None,
2543                                          project_id=None, host=None,
2544                                          columns_to_join=None, limit=None,
2545                                          marker=None):
2546     """Return instances and joins that were active during window."""
2547     query = context.session.query(models.Instance)
2548 
2549     if columns_to_join is None:
2550         columns_to_join_new = ['info_cache', 'security_groups']
2551         manual_joins = ['metadata', 'system_metadata']
2552     else:
2553         manual_joins, columns_to_join_new = (
2554             _manual_join_columns(columns_to_join))
2555 
2556     for column in columns_to_join_new:
2557         if 'extra.' in column:
2558             query = query.options(undefer(column))
2559         else:
2560             query = query.options(joinedload(column))
2561 
2562     query = query.filter(or_(models.Instance.terminated_at == null(),
2563                              models.Instance.terminated_at > begin))
2564     if end:
2565         query = query.filter(models.Instance.launched_at < end)
2566     if project_id:
2567         query = query.filter_by(project_id=project_id)
2568     if host:
2569         query = query.filter_by(host=host)
2570 
2571     if marker is not None:
2572         try:
2573             marker = _instance_get_by_uuid(
2574                 context.elevated(read_deleted='yes'), marker)
2575         except exception.InstanceNotFound:
2576             raise exception.MarkerNotFound(marker=marker)
2577 
2578     query = sqlalchemyutils.paginate_query(
2579         query, models.Instance, limit, ['project_id', 'uuid'], marker=marker)
2580 
2581     return _instances_fill_metadata(context, query.all(), manual_joins)
2582 
2583 
2584 def _instance_get_all_query(context, project_only=False, joins=None):
2585     if joins is None:
2586         joins = ['info_cache', 'security_groups']
2587 
2588     query = model_query(context,
2589                         models.Instance,
2590                         project_only=project_only)
2591     for column in joins:
2592         if 'extra.' in column:
2593             query = query.options(undefer(column))
2594         else:
2595             query = query.options(joinedload(column))
2596     return query
2597 
2598 
2599 @pick_context_manager_reader_allow_async
2600 def instance_get_all_by_host(context, host, columns_to_join=None):
2601     query = _instance_get_all_query(context, joins=columns_to_join)
2602     return _instances_fill_metadata(context,
2603                                     query.filter_by(host=host).all(),
2604                                     manual_joins=columns_to_join)
2605 
2606 
2607 def _instance_get_all_uuids_by_host(context, host):
2608     """Return a list of the instance uuids on a given host.
2609 
2610     Returns a list of UUIDs, not Instance model objects.
2611     """
2612     uuids = []
2613     for tuple in model_query(context, models.Instance, (models.Instance.uuid,),
2614                              read_deleted="no").\
2615                 filter_by(host=host).\
2616                 all():
2617         uuids.append(tuple[0])
2618     return uuids
2619 
2620 
2621 @pick_context_manager_reader
2622 def instance_get_all_uuids_by_host(context, host):
2623     return _instance_get_all_uuids_by_host(context, host)
2624 
2625 
2626 @pick_context_manager_reader
2627 def instance_get_all_by_host_and_node(context, host, node,
2628                                       columns_to_join=None):
2629     if columns_to_join is None:
2630         manual_joins = []
2631     else:
2632         candidates = ['system_metadata', 'metadata']
2633         manual_joins = [x for x in columns_to_join if x in candidates]
2634         columns_to_join = list(set(columns_to_join) - set(candidates))
2635     return _instances_fill_metadata(context,
2636             _instance_get_all_query(
2637                 context,
2638                 joins=columns_to_join).filter_by(host=host).
2639                 filter_by(node=node).all(), manual_joins=manual_joins)
2640 
2641 
2642 @pick_context_manager_reader
2643 def instance_get_all_by_host_and_not_type(context, host, type_id=None):
2644     return _instances_fill_metadata(context,
2645         _instance_get_all_query(context).filter_by(host=host).
2646                    filter(models.Instance.instance_type_id != type_id).all())
2647 
2648 
2649 @pick_context_manager_reader
2650 def instance_get_all_by_grantee_security_groups(context, group_ids):
2651     if not group_ids:
2652         return []
2653     return _instances_fill_metadata(context,
2654         _instance_get_all_query(context).
2655             join(models.Instance.security_groups).
2656             filter(models.SecurityGroup.rules.any(
2657                 models.SecurityGroupIngressRule.group_id.in_(group_ids))).
2658             all())
2659 
2660 
2661 @require_context
2662 @pick_context_manager_reader
2663 def instance_floating_address_get_all(context, instance_uuid):
2664     if not uuidutils.is_uuid_like(instance_uuid):
2665         raise exception.InvalidUUID(uuid=instance_uuid)
2666 
2667     floating_ips = model_query(context,
2668                                models.FloatingIp,
2669                                (models.FloatingIp.address,)).\
2670         join(models.FloatingIp.fixed_ip).\
2671         filter_by(instance_uuid=instance_uuid)
2672 
2673     return [floating_ip.address for floating_ip in floating_ips]
2674 
2675 
2676 # NOTE(hanlind): This method can be removed as conductor RPC API moves to v2.0.
2677 @pick_context_manager_reader
2678 def instance_get_all_hung_in_rebooting(context, reboot_window):
2679     reboot_window = (timeutils.utcnow() -
2680                      datetime.timedelta(seconds=reboot_window))
2681 
2682     # NOTE(danms): this is only used in the _poll_rebooting_instances()
2683     # call in compute/manager, so we can avoid the metadata lookups
2684     # explicitly
2685     return _instances_fill_metadata(context,
2686         model_query(context, models.Instance).
2687             filter(models.Instance.updated_at <= reboot_window).
2688             filter_by(task_state=task_states.REBOOTING).all(),
2689         manual_joins=[])
2690 
2691 
2692 def _retry_instance_update():
2693     """Wrap with oslo_db_api.wrap_db_retry, and also retry on
2694     UnknownInstanceUpdateConflict.
2695     """
2696     exception_checker = \
2697         lambda exc: isinstance(exc, (exception.UnknownInstanceUpdateConflict,))
2698     return oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True,
2699                                      exception_checker=exception_checker)
2700 
2701 
2702 @require_context
2703 @_retry_instance_update()
2704 @pick_context_manager_writer
2705 def instance_update(context, instance_uuid, values, expected=None):
2706     return _instance_update(context, instance_uuid, values, expected)
2707 
2708 
2709 @require_context
2710 @_retry_instance_update()
2711 @pick_context_manager_writer
2712 def instance_update_and_get_original(context, instance_uuid, values,
2713                                      columns_to_join=None, expected=None):
2714     """Set the given properties on an instance and update it. Return
2715     a shallow copy of the original instance reference, as well as the
2716     updated one.
2717 
2718     :param context: = request context object
2719     :param instance_uuid: = instance uuid
2720     :param values: = dict containing column values
2721 
2722     If "expected_task_state" exists in values, the update can only happen
2723     when the task state before update matches expected_task_state. Otherwise
2724     a UnexpectedTaskStateError is thrown.
2725 
2726     :returns: a tuple of the form (old_instance_ref, new_instance_ref)
2727 
2728     Raises NotFound if instance does not exist.
2729     """
2730     instance_ref = _instance_get_by_uuid(context, instance_uuid,
2731                                          columns_to_join=columns_to_join)
2732     return (copy.copy(instance_ref), _instance_update(
2733         context, instance_uuid, values, expected, original=instance_ref))
2734 
2735 
2736 # NOTE(danms): This updates the instance's metadata list in-place and in
2737 # the database to avoid stale data and refresh issues. It assumes the
2738 # delete=True behavior of instance_metadata_update(...)
2739 def _instance_metadata_update_in_place(context, instance, metadata_type, model,
2740                                        metadata):
2741     metadata = dict(metadata)
2742     to_delete = []
2743     for keyvalue in instance[metadata_type]:
2744         key = keyvalue['key']
2745         if key in metadata:
2746             keyvalue['value'] = metadata.pop(key)
2747         elif key not in metadata:
2748             to_delete.append(keyvalue)
2749 
2750     # NOTE: we have to hard_delete here otherwise we will get more than one
2751     # system_metadata record when we read deleted for an instance;
2752     # regular metadata doesn't have the same problem because we don't
2753     # allow reading deleted regular metadata anywhere.
2754     if metadata_type == 'system_metadata':
2755         for condemned in to_delete:
2756             context.session.delete(condemned)
2757             instance[metadata_type].remove(condemned)
2758     else:
2759         for condemned in to_delete:
2760             condemned.soft_delete(context.session)
2761 
2762     for key, value in metadata.items():
2763         newitem = model()
2764         newitem.update({'key': key, 'value': value,
2765                         'instance_uuid': instance['uuid']})
2766         context.session.add(newitem)
2767         instance[metadata_type].append(newitem)
2768 
2769 
2770 def _instance_update(context, instance_uuid, values, expected, original=None):
2771     if not uuidutils.is_uuid_like(instance_uuid):
2772         raise exception.InvalidUUID(uuid=instance_uuid)
2773 
2774     if expected is None:
2775         expected = {}
2776     else:
2777         # Coerce all single values to singleton lists
2778         expected = {k: [None] if v is None else sqlalchemyutils.to_list(v)
2779                        for (k, v) in expected.items()}
2780 
2781     # Extract 'expected_' values from values dict, as these aren't actually
2782     # updates
2783     for field in ('task_state', 'vm_state'):
2784         expected_field = 'expected_%s' % field
2785         if expected_field in values:
2786             value = values.pop(expected_field, None)
2787             # Coerce all single values to singleton lists
2788             if value is None:
2789                 expected[field] = [None]
2790             else:
2791                 expected[field] = sqlalchemyutils.to_list(value)
2792 
2793     # Values which need to be updated separately
2794     metadata = values.pop('metadata', None)
2795     system_metadata = values.pop('system_metadata', None)
2796 
2797     _handle_objects_related_type_conversions(values)
2798 
2799     # Hostname is potentially unique, but this is enforced in code rather
2800     # than the DB. The query below races, but the number of users of
2801     # osapi_compute_unique_server_name_scope is small, and a robust fix
2802     # will be complex. This is intentionally left as is for the moment.
2803     if 'hostname' in values:
2804         _validate_unique_server_name(context, values['hostname'])
2805 
2806     compare = models.Instance(uuid=instance_uuid, **expected)
2807     try:
2808         instance_ref = model_query(context, models.Instance,
2809                                    project_only=True).\
2810                        update_on_match(compare, 'uuid', values)
2811     except update_match.NoRowsMatched:
2812         # Update failed. Try to find why and raise a specific error.
2813 
2814         # We should get here only because our expected values were not current
2815         # when update_on_match executed. Having failed, we now have a hint that
2816         # the values are out of date and should check them.
2817 
2818         # This code is made more complex because we are using repeatable reads.
2819         # If we have previously read the original instance in the current
2820         # transaction, reading it again will return the same data, even though
2821         # the above update failed because it has changed: it is not possible to
2822         # determine what has changed in this transaction. In this case we raise
2823         # UnknownInstanceUpdateConflict, which will cause the operation to be
2824         # retried in a new transaction.
2825 
2826         # Because of the above, if we have previously read the instance in the
2827         # current transaction it will have been passed as 'original', and there
2828         # is no point refreshing it. If we have not previously read the
2829         # instance, we can fetch it here and we will get fresh data.
2830         if original is None:
2831             original = _instance_get_by_uuid(context, instance_uuid)
2832 
2833         conflicts_expected = {}
2834         conflicts_actual = {}
2835         for (field, expected_values) in expected.items():
2836             actual = original[field]
2837             if actual not in expected_values:
2838                 conflicts_expected[field] = expected_values
2839                 conflicts_actual[field] = actual
2840 
2841         # Exception properties
2842         exc_props = {
2843             'instance_uuid': instance_uuid,
2844             'expected': conflicts_expected,
2845             'actual': conflicts_actual
2846         }
2847 
2848         # There was a conflict, but something (probably the MySQL read view,
2849         # but possibly an exceptionally unlikely second race) is preventing us
2850         # from seeing what it is. When we go round again we'll get a fresh
2851         # transaction and a fresh read view.
2852         if len(conflicts_actual) == 0:
2853             raise exception.UnknownInstanceUpdateConflict(**exc_props)
2854 
2855         # Task state gets special handling for convenience. We raise the
2856         # specific error UnexpectedDeletingTaskStateError or
2857         # UnexpectedTaskStateError as appropriate
2858         if 'task_state' in conflicts_actual:
2859             conflict_task_state = conflicts_actual['task_state']
2860             if conflict_task_state == task_states.DELETING:
2861                 exc = exception.UnexpectedDeletingTaskStateError
2862             else:
2863                 exc = exception.UnexpectedTaskStateError
2864 
2865         # Everything else is an InstanceUpdateConflict
2866         else:
2867             exc = exception.InstanceUpdateConflict
2868 
2869         raise exc(**exc_props)
2870 
2871     if metadata is not None:
2872         _instance_metadata_update_in_place(context, instance_ref,
2873                                            'metadata',
2874                                            models.InstanceMetadata,
2875                                            metadata)
2876 
2877     if system_metadata is not None:
2878         _instance_metadata_update_in_place(context, instance_ref,
2879                                            'system_metadata',
2880                                            models.InstanceSystemMetadata,
2881                                            system_metadata)
2882 
2883     return instance_ref
2884 
2885 
2886 @pick_context_manager_writer
2887 def instance_add_security_group(context, instance_uuid, security_group_id):
2888     """Associate the given security group with the given instance."""
2889     sec_group_ref = models.SecurityGroupInstanceAssociation()
2890     sec_group_ref.update({'instance_uuid': instance_uuid,
2891                           'security_group_id': security_group_id})
2892     sec_group_ref.save(context.session)
2893 
2894 
2895 @require_context
2896 @pick_context_manager_writer
2897 def instance_remove_security_group(context, instance_uuid, security_group_id):
2898     """Disassociate the given security group from the given instance."""
2899     model_query(context, models.SecurityGroupInstanceAssociation).\
2900                 filter_by(instance_uuid=instance_uuid).\
2901                 filter_by(security_group_id=security_group_id).\
2902                 soft_delete()
2903 
2904 
2905 ###################
2906 
2907 
2908 @require_context
2909 @pick_context_manager_reader
2910 def instance_info_cache_get(context, instance_uuid):
2911     """Gets an instance info cache from the table.
2912 
2913     :param instance_uuid: = uuid of the info cache's instance
2914     """
2915     return model_query(context, models.InstanceInfoCache).\
2916                          filter_by(instance_uuid=instance_uuid).\
2917                          first()
2918 
2919 
2920 @require_context
2921 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
2922 @pick_context_manager_writer
2923 def instance_info_cache_update(context, instance_uuid, values):
2924     """Update an instance info cache record in the table.
2925 
2926     :param instance_uuid: = uuid of info cache's instance
2927     :param values: = dict containing column values to update
2928     """
2929     convert_objects_related_datetimes(values)
2930 
2931     info_cache = model_query(context, models.InstanceInfoCache).\
2932                      filter_by(instance_uuid=instance_uuid).\
2933                      first()
2934     needs_create = False
2935     if info_cache and info_cache['deleted']:
2936         raise exception.InstanceInfoCacheNotFound(
2937                 instance_uuid=instance_uuid)
2938     elif not info_cache:
2939         # NOTE(tr3buchet): just in case someone blows away an instance's
2940         #                  cache entry, re-create it.
2941         values['instance_uuid'] = instance_uuid
2942         info_cache = models.InstanceInfoCache(**values)
2943         needs_create = True
2944 
2945     try:
2946         with get_context_manager(context).writer.savepoint.using(context):
2947             if needs_create:
2948                 info_cache.save(context.session)
2949             else:
2950                 info_cache.update(values)
2951     except db_exc.DBDuplicateEntry:
2952         # NOTE(sirp): Possible race if two greenthreads attempt to
2953         # recreate the instance cache entry at the same time. First one
2954         # wins.
2955         pass
2956 
2957     return info_cache
2958 
2959 
2960 @require_context
2961 @pick_context_manager_writer
2962 def instance_info_cache_delete(context, instance_uuid):
2963     """Deletes an existing instance_info_cache record
2964 
2965     :param instance_uuid: = uuid of the instance tied to the cache record
2966     """
2967     model_query(context, models.InstanceInfoCache).\
2968                          filter_by(instance_uuid=instance_uuid).\
2969                          soft_delete()
2970 
2971 
2972 ###################
2973 
2974 
2975 def _instance_extra_create(context, values):
2976     inst_extra_ref = models.InstanceExtra()
2977     inst_extra_ref.update(values)
2978     inst_extra_ref.save(context.session)
2979     return inst_extra_ref
2980 
2981 
2982 @pick_context_manager_writer
2983 def instance_extra_update_by_uuid(context, instance_uuid, values):
2984     rows_updated = model_query(context, models.InstanceExtra).\
2985         filter_by(instance_uuid=instance_uuid).\
2986         update(values)
2987     if not rows_updated:
2988         LOG.debug("Created instance_extra for %s", instance_uuid)
2989         create_values = copy.copy(values)
2990         create_values["instance_uuid"] = instance_uuid
2991         _instance_extra_create(context, create_values)
2992         rows_updated = 1
2993     return rows_updated
2994 
2995 
2996 @pick_context_manager_reader
2997 def instance_extra_get_by_instance_uuid(context, instance_uuid,
2998                                         columns=None):
2999     query = model_query(context, models.InstanceExtra).\
3000         filter_by(instance_uuid=instance_uuid)
3001     if columns is None:
3002         columns = ['numa_topology', 'pci_requests', 'flavor', 'vcpu_model',
3003                    'trusted_certs', 'migration_context']
3004     for column in columns:
3005         query = query.options(undefer(column))
3006     instance_extra = query.first()
3007     return instance_extra
3008 
3009 
3010 ###################
3011 
3012 
3013 @require_context
3014 @pick_context_manager_writer
3015 def key_pair_create(context, values):
3016     try:
3017         key_pair_ref = models.KeyPair()
3018         key_pair_ref.update(values)
3019         key_pair_ref.save(context.session)
3020         return key_pair_ref
3021     except db_exc.DBDuplicateEntry:
3022         raise exception.KeyPairExists(key_name=values['name'])
3023 
3024 
3025 @require_context
3026 @pick_context_manager_writer
3027 def key_pair_destroy(context, user_id, name):
3028     result = model_query(context, models.KeyPair).\
3029                          filter_by(user_id=user_id).\
3030                          filter_by(name=name).\
3031                          soft_delete()
3032     if not result:
3033         raise exception.KeypairNotFound(user_id=user_id, name=name)
3034 
3035 
3036 @require_context
3037 @pick_context_manager_reader
3038 def key_pair_get(context, user_id, name):
3039     result = model_query(context, models.KeyPair).\
3040                      filter_by(user_id=user_id).\
3041                      filter_by(name=name).\
3042                      first()
3043 
3044     if not result:
3045         raise exception.KeypairNotFound(user_id=user_id, name=name)
3046 
3047     return result
3048 
3049 
3050 @require_context
3051 @pick_context_manager_reader
3052 def key_pair_get_all_by_user(context, user_id, limit=None, marker=None):
3053     marker_row = None
3054     if marker is not None:
3055         marker_row = model_query(context, models.KeyPair, read_deleted="no").\
3056             filter_by(name=marker).filter_by(user_id=user_id).first()
3057         if not marker_row:
3058             raise exception.MarkerNotFound(marker=marker)
3059 
3060     query = model_query(context, models.KeyPair, read_deleted="no").\
3061         filter_by(user_id=user_id)
3062 
3063     query = sqlalchemyutils.paginate_query(
3064         query, models.KeyPair, limit, ['name'], marker=marker_row)
3065 
3066     return query.all()
3067 
3068 
3069 @require_context
3070 @pick_context_manager_reader
3071 def key_pair_count_by_user(context, user_id):
3072     return model_query(context, models.KeyPair, read_deleted="no").\
3073                    filter_by(user_id=user_id).\
3074                    count()
3075 
3076 
3077 ###################
3078 
3079 @pick_context_manager_writer
3080 def network_associate(context, project_id, network_id=None, force=False):
3081     """Associate a project with a network.
3082 
3083     called by project_get_networks under certain conditions
3084     and network manager add_network_to_project()
3085 
3086     only associate if the project doesn't already have a network
3087     or if force is True
3088 
3089     force solves race condition where a fresh project has multiple instance
3090     builds simultaneously picked up by multiple network hosts which attempt
3091     to associate the project with multiple networks
3092     force should only be used as a direct consequence of user request
3093     all automated requests should not use force
3094     """
3095     def network_query(project_filter, id=None):
3096         filter_kwargs = {'project_id': project_filter}
3097         if id is not None:
3098             filter_kwargs['id'] = id
3099         return model_query(context, models.Network, read_deleted="no").\
3100                        filter_by(**filter_kwargs).\
3101                        with_lockmode('update').\
3102                        first()
3103 
3104     if not force:
3105         # find out if project has a network
3106         network_ref = network_query(project_id)
3107 
3108     if force or not network_ref:
3109         # in force mode or project doesn't have a network so associate
3110         # with a new network
3111 
3112         # get new network
3113         network_ref = network_query(None, network_id)
3114         if not network_ref:
3115             raise exception.NoMoreNetworks()
3116 
3117         # associate with network
3118         # NOTE(vish): if with_lockmode isn't supported, as in sqlite,
3119         #             then this has concurrency issues
3120         network_ref['project_id'] = project_id
3121         context.session.add(network_ref)
3122     return network_ref
3123 
3124 
3125 def _network_ips_query(context, network_id):
3126     return model_query(context, models.FixedIp, read_deleted="no").\
3127                    filter_by(network_id=network_id)
3128 
3129 
3130 @pick_context_manager_reader
3131 def network_count_reserved_ips(context, network_id):
3132     return _network_ips_query(context, network_id).\
3133                     filter_by(reserved=True).\
3134                     count()
3135 
3136 
3137 @pick_context_manager_writer
3138 def network_create_safe(context, values):
3139     network_ref = models.Network()
3140     network_ref['uuid'] = uuidutils.generate_uuid()
3141     network_ref.update(values)
3142 
3143     try:
3144         network_ref.save(context.session)
3145         return network_ref
3146     except db_exc.DBDuplicateEntry:
3147         raise exception.DuplicateVlan(vlan=values['vlan'])
3148 
3149 
3150 @pick_context_manager_writer
3151 def network_delete_safe(context, network_id):
3152     result = model_query(context, models.FixedIp, read_deleted="no").\
3153                      filter_by(network_id=network_id).\
3154                      filter_by(allocated=True).\
3155                      count()
3156     if result != 0:
3157         raise exception.NetworkInUse(network_id=network_id)
3158     network_ref = _network_get(context, network_id=network_id)
3159 
3160     model_query(context, models.FixedIp, read_deleted="no").\
3161             filter_by(network_id=network_id).\
3162             soft_delete()
3163 
3164     context.session.delete(network_ref)
3165 
3166 
3167 @pick_context_manager_writer
3168 def network_disassociate(context, network_id, disassociate_host,
3169                          disassociate_project):
3170     net_update = {}
3171     if disassociate_project:
3172         net_update['project_id'] = None
3173     if disassociate_host:
3174         net_update['host'] = None
3175     network_update(context, network_id, net_update)
3176 
3177 
3178 def _network_get(context, network_id, project_only='allow_none'):
3179     result = model_query(context, models.Network, project_only=project_only).\
3180                     filter_by(id=network_id).\
3181                     first()
3182 
3183     if not result:
3184         raise exception.NetworkNotFound(network_id=network_id)
3185 
3186     return result
3187 
3188 
3189 @require_context
3190 @pick_context_manager_reader
3191 def network_get(context, network_id, project_only='allow_none'):
3192     return _network_get(context, network_id, project_only=project_only)
3193 
3194 
3195 @require_context
3196 @pick_context_manager_reader
3197 def network_get_all(context, project_only):
3198     result = model_query(context, models.Network, read_deleted="no",
3199                          project_only=project_only).all()
3200 
3201     if not result:
3202         raise exception.NoNetworksFound()
3203 
3204     return result
3205 
3206 
3207 @require_context
3208 @pick_context_manager_reader
3209 def network_get_all_by_uuids(context, network_uuids, project_only):
3210     result = model_query(context, models.Network, read_deleted="no",
3211                          project_only=project_only).\
3212                 filter(models.Network.uuid.in_(network_uuids)).\
3213                 all()
3214 
3215     if not result:
3216         raise exception.NoNetworksFound()
3217 
3218     # check if the result contains all the networks
3219     # we are looking for
3220     for network_uuid in network_uuids:
3221         for network in result:
3222             if network['uuid'] == network_uuid:
3223                 break
3224         else:
3225             if project_only:
3226                 raise exception.NetworkNotFoundForProject(
3227                       network_uuid=network_uuid, project_id=context.project_id)
3228             raise exception.NetworkNotFound(network_id=network_uuid)
3229 
3230     return result
3231 
3232 
3233 def _get_associated_fixed_ips_query(context, network_id, host=None):
3234     # NOTE(vish): The ugly joins here are to solve a performance issue and
3235     #             should be removed once we can add and remove leases
3236     #             without regenerating the whole list
3237     vif_and = and_(models.VirtualInterface.id ==
3238                    models.FixedIp.virtual_interface_id,
3239                    models.VirtualInterface.deleted == 0)
3240     inst_and = and_(models.Instance.uuid == models.FixedIp.instance_uuid,
3241                     models.Instance.deleted == 0)
3242     # NOTE(vish): This subquery left joins the minimum interface id for each
3243     #             instance. If the join succeeds (i.e. the 11th column is not
3244     #             null), then the fixed ip is on the first interface.
3245     subq = context.session.query(
3246         func.min(models.VirtualInterface.id).label("id"),
3247         models.VirtualInterface.instance_uuid).\
3248         group_by(models.VirtualInterface.instance_uuid).subquery()
3249     subq_and = and_(subq.c.id == models.FixedIp.virtual_interface_id,
3250             subq.c.instance_uuid == models.VirtualInterface.instance_uuid)
3251     query = context.session.query(
3252         models.FixedIp.address,
3253         models.FixedIp.instance_uuid,
3254         models.FixedIp.network_id,
3255         models.FixedIp.virtual_interface_id,
3256         models.VirtualInterface.address,
3257         models.Instance.hostname,
3258         models.Instance.updated_at,
3259         models.Instance.created_at,
3260         models.FixedIp.allocated,
3261         models.FixedIp.leased,
3262         subq.c.id).\
3263         filter(models.FixedIp.deleted == 0).\
3264         filter(models.FixedIp.network_id == network_id).\
3265         join((models.VirtualInterface, vif_and)).\
3266         join((models.Instance, inst_and)).\
3267         outerjoin((subq, subq_and)).\
3268         filter(models.FixedIp.instance_uuid != null()).\
3269         filter(models.FixedIp.virtual_interface_id != null())
3270     if host:
3271         query = query.filter(models.Instance.host == host)
3272     return query
3273 
3274 
3275 @pick_context_manager_reader
3276 def network_get_associated_fixed_ips(context, network_id, host=None):
3277     # FIXME(sirp): since this returns fixed_ips, this would be better named
3278     # fixed_ip_get_all_by_network.
3279     query = _get_associated_fixed_ips_query(context, network_id, host)
3280     result = query.all()
3281     data = []
3282     for datum in result:
3283         cleaned = {}
3284         cleaned['address'] = datum[0]
3285         cleaned['instance_uuid'] = datum[1]
3286         cleaned['network_id'] = datum[2]
3287         cleaned['vif_id'] = datum[3]
3288         cleaned['vif_address'] = datum[4]
3289         cleaned['instance_hostname'] = datum[5]
3290         cleaned['instance_updated'] = datum[6]
3291         cleaned['instance_created'] = datum[7]
3292         cleaned['allocated'] = datum[8]
3293         cleaned['leased'] = datum[9]
3294         # NOTE(vish): default_route is True if this fixed ip is on the first
3295         #             interface its instance.
3296         cleaned['default_route'] = datum[10] is not None
3297         data.append(cleaned)
3298     return data
3299 
3300 
3301 @pick_context_manager_reader
3302 def network_in_use_on_host(context, network_id, host):
3303     query = _get_associated_fixed_ips_query(context, network_id, host)
3304     return query.count() > 0
3305 
3306 
3307 def _network_get_query(context):
3308     return model_query(context, models.Network, read_deleted="no")
3309 
3310 
3311 @pick_context_manager_reader
3312 def network_get_by_uuid(context, uuid):
3313     result = _network_get_query(context).filter_by(uuid=uuid).first()
3314 
3315     if not result:
3316         raise exception.NetworkNotFoundForUUID(uuid=uuid)
3317 
3318     return result
3319 
3320 
3321 @pick_context_manager_reader
3322 def network_get_by_cidr(context, cidr):
3323     result = _network_get_query(context).\
3324                 filter(or_(models.Network.cidr == cidr,
3325                            models.Network.cidr_v6 == cidr)).\
3326                 first()
3327 
3328     if not result:
3329         raise exception.NetworkNotFoundForCidr(cidr=cidr)
3330 
3331     return result
3332 
3333 
3334 @pick_context_manager_reader
3335 def network_get_all_by_host(context, host):
3336     fixed_host_filter = or_(models.FixedIp.host == host,
3337             and_(models.FixedIp.instance_uuid != null(),
3338                  models.Instance.host == host))
3339     fixed_ip_query = model_query(context, models.FixedIp,
3340                                  (models.FixedIp.network_id,)).\
3341                      outerjoin((models.Instance,
3342                                 models.Instance.uuid ==
3343                                 models.FixedIp.instance_uuid)).\
3344                      filter(fixed_host_filter)
3345     # NOTE(vish): return networks that have host set
3346     #             or that have a fixed ip with host set
3347     #             or that have an instance with host set
3348     host_filter = or_(models.Network.host == host,
3349                       models.Network.id.in_(fixed_ip_query.subquery()))
3350     return _network_get_query(context).filter(host_filter).all()
3351 
3352 
3353 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
3354 @pick_context_manager_writer
3355 def network_set_host(context, network_id, host_id):
3356     network_ref = _network_get_query(context).\
3357         filter_by(id=network_id).\
3358         first()
3359 
3360     if not network_ref:
3361         raise exception.NetworkNotFound(network_id=network_id)
3362 
3363     if network_ref.host:
3364         return None
3365 
3366     rows_updated = _network_get_query(context).\
3367         filter_by(id=network_id).\
3368         filter_by(host=None).\
3369         update({'host': host_id})
3370 
3371     if not rows_updated:
3372         LOG.debug('The row was updated in a concurrent transaction, '
3373                   'we will fetch another row')
3374         raise db_exc.RetryRequest(
3375             exception.NetworkSetHostFailed(network_id=network_id))
3376 
3377 
3378 @require_context
3379 @pick_context_manager_writer
3380 def network_update(context, network_id, values):
3381     network_ref = _network_get(context, network_id)
3382     network_ref.update(values)
3383     try:
3384         network_ref.save(context.session)
3385     except db_exc.DBDuplicateEntry:
3386         raise exception.DuplicateVlan(vlan=values['vlan'])
3387     return network_ref
3388 
3389 
3390 ###################
3391 
3392 
3393 @require_context
3394 @pick_context_manager_reader
3395 def quota_get(context, project_id, resource, user_id=None):
3396     model = models.ProjectUserQuota if user_id else models.Quota
3397     query = model_query(context, model).\
3398                     filter_by(project_id=project_id).\
3399                     filter_by(resource=resource)
3400     if user_id:
3401         query = query.filter_by(user_id=user_id)
3402 
3403     result = query.first()
3404     if not result:
3405         if user_id:
3406             raise exception.ProjectUserQuotaNotFound(project_id=project_id,
3407                                                      user_id=user_id)
3408         else:
3409             raise exception.ProjectQuotaNotFound(project_id=project_id)
3410 
3411     return result
3412 
3413 
3414 @require_context
3415 @pick_context_manager_reader
3416 def quota_get_all_by_project_and_user(context, project_id, user_id):
3417     user_quotas = model_query(context, models.ProjectUserQuota,
3418                               (models.ProjectUserQuota.resource,
3419                                models.ProjectUserQuota.hard_limit)).\
3420                    filter_by(project_id=project_id).\
3421                    filter_by(user_id=user_id).\
3422                    all()
3423 
3424     result = {'project_id': project_id, 'user_id': user_id}
3425     for user_quota in user_quotas:
3426         result[user_quota.resource] = user_quota.hard_limit
3427 
3428     return result
3429 
3430 
3431 @require_context
3432 @pick_context_manager_reader
3433 def quota_get_all_by_project(context, project_id):
3434     rows = model_query(context, models.Quota, read_deleted="no").\
3435                    filter_by(project_id=project_id).\
3436                    all()
3437 
3438     result = {'project_id': project_id}
3439     for row in rows:
3440         result[row.resource] = row.hard_limit
3441 
3442     return result
3443 
3444 
3445 @require_context
3446 @pick_context_manager_reader
3447 def quota_get_all(context, project_id):
3448     result = model_query(context, models.ProjectUserQuota).\
3449                    filter_by(project_id=project_id).\
3450                    all()
3451 
3452     return result
3453 
3454 
3455 def quota_get_per_project_resources():
3456     return PER_PROJECT_QUOTAS
3457 
3458 
3459 @pick_context_manager_writer
3460 def quota_create(context, project_id, resource, limit, user_id=None):
3461     per_user = user_id and resource not in PER_PROJECT_QUOTAS
3462     quota_ref = models.ProjectUserQuota() if per_user else models.Quota()
3463     if per_user:
3464         quota_ref.user_id = user_id
3465     quota_ref.project_id = project_id
3466     quota_ref.resource = resource
3467     quota_ref.hard_limit = limit
3468     try:
3469         quota_ref.save(context.session)
3470     except db_exc.DBDuplicateEntry:
3471         raise exception.QuotaExists(project_id=project_id, resource=resource)
3472     return quota_ref
3473 
3474 
3475 @pick_context_manager_writer
3476 def quota_update(context, project_id, resource, limit, user_id=None):
3477     per_user = user_id and resource not in PER_PROJECT_QUOTAS
3478     model = models.ProjectUserQuota if per_user else models.Quota
3479     query = model_query(context, model).\
3480                 filter_by(project_id=project_id).\
3481                 filter_by(resource=resource)
3482     if per_user:
3483         query = query.filter_by(user_id=user_id)
3484 
3485     result = query.update({'hard_limit': limit})
3486     if not result:
3487         if per_user:
3488             raise exception.ProjectUserQuotaNotFound(project_id=project_id,
3489                                                      user_id=user_id)
3490         else:
3491             raise exception.ProjectQuotaNotFound(project_id=project_id)
3492 
3493 
3494 ###################
3495 
3496 
3497 @require_context
3498 @pick_context_manager_reader
3499 def quota_class_get(context, class_name, resource):
3500     result = model_query(context, models.QuotaClass, read_deleted="no").\
3501                      filter_by(class_name=class_name).\
3502                      filter_by(resource=resource).\
3503                      first()
3504 
3505     if not result:
3506         raise exception.QuotaClassNotFound(class_name=class_name)
3507 
3508     return result
3509 
3510 
3511 @pick_context_manager_reader
3512 def quota_class_get_default(context):
3513     rows = model_query(context, models.QuotaClass, read_deleted="no").\
3514                    filter_by(class_name=_DEFAULT_QUOTA_NAME).\
3515                    all()
3516 
3517     result = {'class_name': _DEFAULT_QUOTA_NAME}
3518     for row in rows:
3519         result[row.resource] = row.hard_limit
3520 
3521     return result
3522 
3523 
3524 @require_context
3525 @pick_context_manager_reader
3526 def quota_class_get_all_by_name(context, class_name):
3527     rows = model_query(context, models.QuotaClass, read_deleted="no").\
3528                    filter_by(class_name=class_name).\
3529                    all()
3530 
3531     result = {'class_name': class_name}
3532     for row in rows:
3533         result[row.resource] = row.hard_limit
3534 
3535     return result
3536 
3537 
3538 @pick_context_manager_writer
3539 def quota_class_create(context, class_name, resource, limit):
3540     quota_class_ref = models.QuotaClass()
3541     quota_class_ref.class_name = class_name
3542     quota_class_ref.resource = resource
3543     quota_class_ref.hard_limit = limit
3544     quota_class_ref.save(context.session)
3545     return quota_class_ref
3546 
3547 
3548 @pick_context_manager_writer
3549 def quota_class_update(context, class_name, resource, limit):
3550     result = model_query(context, models.QuotaClass, read_deleted="no").\
3551                      filter_by(class_name=class_name).\
3552                      filter_by(resource=resource).\
3553                      update({'hard_limit': limit})
3554 
3555     if not result:
3556         raise exception.QuotaClassNotFound(class_name=class_name)
3557 
3558 
3559 ###################
3560 
3561 
3562 @pick_context_manager_writer
3563 def quota_destroy_all_by_project_and_user(context, project_id, user_id):
3564     model_query(context, models.ProjectUserQuota, read_deleted="no").\
3565         filter_by(project_id=project_id).\
3566         filter_by(user_id=user_id).\
3567         soft_delete(synchronize_session=False)
3568 
3569 
3570 @pick_context_manager_writer
3571 def quota_destroy_all_by_project(context, project_id):
3572     model_query(context, models.Quota, read_deleted="no").\
3573         filter_by(project_id=project_id).\
3574         soft_delete(synchronize_session=False)
3575 
3576     model_query(context, models.ProjectUserQuota, read_deleted="no").\
3577         filter_by(project_id=project_id).\
3578         soft_delete(synchronize_session=False)
3579 
3580 
3581 ###################
3582 
3583 
3584 def _ec2_volume_get_query(context):
3585     return model_query(context, models.VolumeIdMapping, read_deleted='yes')
3586 
3587 
3588 def _ec2_snapshot_get_query(context):
3589     return model_query(context, models.SnapshotIdMapping, read_deleted='yes')
3590 
3591 
3592 @require_context
3593 @pick_context_manager_writer
3594 def ec2_volume_create(context, volume_uuid, id=None):
3595     """Create ec2 compatible volume by provided uuid."""
3596     ec2_volume_ref = models.VolumeIdMapping()
3597     ec2_volume_ref.update({'uuid': volume_uuid})
3598     if id is not None:
3599         ec2_volume_ref.update({'id': id})
3600 
3601     ec2_volume_ref.save(context.session)
3602 
3603     return ec2_volume_ref
3604 
3605 
3606 @require_context
3607 @pick_context_manager_reader
3608 def ec2_volume_get_by_uuid(context, volume_uuid):
3609     result = _ec2_volume_get_query(context).\
3610                     filter_by(uuid=volume_uuid).\
3611                     first()
3612 
3613     if not result:
3614         raise exception.VolumeNotFound(volume_id=volume_uuid)
3615 
3616     return result
3617 
3618 
3619 @require_context
3620 @pick_context_manager_reader
3621 def ec2_volume_get_by_id(context, volume_id):
3622     result = _ec2_volume_get_query(context).\
3623                     filter_by(id=volume_id).\
3624                     first()
3625 
3626     if not result:
3627         raise exception.VolumeNotFound(volume_id=volume_id)
3628 
3629     return result
3630 
3631 
3632 @require_context
3633 @pick_context_manager_writer
3634 def ec2_snapshot_create(context, snapshot_uuid, id=None):
3635     """Create ec2 compatible snapshot by provided uuid."""
3636     ec2_snapshot_ref = models.SnapshotIdMapping()
3637     ec2_snapshot_ref.update({'uuid': snapshot_uuid})
3638     if id is not None:
3639         ec2_snapshot_ref.update({'id': id})
3640 
3641     ec2_snapshot_ref.save(context.session)
3642 
3643     return ec2_snapshot_ref
3644 
3645 
3646 @require_context
3647 @pick_context_manager_reader
3648 def ec2_snapshot_get_by_ec2_id(context, ec2_id):
3649     result = _ec2_snapshot_get_query(context).\
3650                     filter_by(id=ec2_id).\
3651                     first()
3652 
3653     if not result:
3654         raise exception.SnapshotNotFound(snapshot_id=ec2_id)
3655 
3656     return result
3657 
3658 
3659 @require_context
3660 @pick_context_manager_reader
3661 def ec2_snapshot_get_by_uuid(context, snapshot_uuid):
3662     result = _ec2_snapshot_get_query(context).\
3663                     filter_by(uuid=snapshot_uuid).\
3664                     first()
3665 
3666     if not result:
3667         raise exception.SnapshotNotFound(snapshot_id=snapshot_uuid)
3668 
3669     return result
3670 
3671 
3672 ###################
3673 
3674 
3675 def _block_device_mapping_get_query(context, columns_to_join=None):
3676     if columns_to_join is None:
3677         columns_to_join = []
3678 
3679     query = model_query(context, models.BlockDeviceMapping)
3680 
3681     for column in columns_to_join:
3682         query = query.options(joinedload(column))
3683 
3684     return query
3685 
3686 
3687 def _scrub_empty_str_values(dct, keys_to_scrub):
3688     """Remove any keys found in sequence keys_to_scrub from the dict
3689     if they have the value ''.
3690     """
3691     for key in keys_to_scrub:
3692         if key in dct and dct[key] == '':
3693             del dct[key]
3694 
3695 
3696 def _from_legacy_values(values, legacy, allow_updates=False):
3697     if legacy:
3698         if allow_updates and block_device.is_safe_for_update(values):
3699             return values
3700         else:
3701             return block_device.BlockDeviceDict.from_legacy(values)
3702     else:
3703         return values
3704 
3705 
3706 def _set_or_validate_uuid(values):
3707     uuid = values.get('uuid')
3708 
3709     # values doesn't contain uuid, or it's blank
3710     if not uuid:
3711         values['uuid'] = uuidutils.generate_uuid()
3712 
3713     # values contains a uuid
3714     else:
3715         if not uuidutils.is_uuid_like(uuid):
3716             raise exception.InvalidUUID(uuid=uuid)
3717 
3718 
3719 @require_context
3720 @pick_context_manager_writer
3721 def block_device_mapping_create(context, values, legacy=True):
3722     _scrub_empty_str_values(values, ['volume_size'])
3723     values = _from_legacy_values(values, legacy)
3724     convert_objects_related_datetimes(values)
3725 
3726     _set_or_validate_uuid(values)
3727 
3728     bdm_ref = models.BlockDeviceMapping()
3729     bdm_ref.update(values)
3730     bdm_ref.save(context.session)
3731     return bdm_ref
3732 
3733 
3734 @require_context
3735 @pick_context_manager_writer
3736 def block_device_mapping_update(context, bdm_id, values, legacy=True):
3737     _scrub_empty_str_values(values, ['volume_size'])
3738     values = _from_legacy_values(values, legacy, allow_updates=True)
3739     convert_objects_related_datetimes(values)
3740 
3741     query = _block_device_mapping_get_query(context).filter_by(id=bdm_id)
3742     query.update(values)
3743     return query.first()
3744 
3745 
3746 @pick_context_manager_writer
3747 def block_device_mapping_update_or_create(context, values, legacy=True):
3748     # TODO(mdbooth): Remove this method entirely. Callers should know whether
3749     # they require update or create, and call the appropriate method.
3750 
3751     _scrub_empty_str_values(values, ['volume_size'])
3752     values = _from_legacy_values(values, legacy, allow_updates=True)
3753     convert_objects_related_datetimes(values)
3754 
3755     result = None
3756     # NOTE(xqueralt,danms): Only update a BDM when device_name or
3757     # uuid was provided. Prefer the uuid, if available, but fall
3758     # back to device_name if no uuid is provided, which can happen
3759     # for BDMs created before we had a uuid. We allow empty device
3760     # names so they will be set later by the manager.
3761     if 'uuid' in values:
3762         query = _block_device_mapping_get_query(context)
3763         result = query.filter_by(instance_uuid=values['instance_uuid'],
3764                                  uuid=values['uuid']).one_or_none()
3765 
3766     if not result and values['device_name']:
3767         query = _block_device_mapping_get_query(context)
3768         result = query.filter_by(instance_uuid=values['instance_uuid'],
3769                                  device_name=values['device_name']).first()
3770 
3771     if result:
3772         result.update(values)
3773     else:
3774         # Either the device_name or uuid doesn't exist in the database yet, or
3775         # neither was provided. Both cases mean creating a new BDM.
3776         _set_or_validate_uuid(values)
3777         result = models.BlockDeviceMapping(**values)
3778         result.save(context.session)
3779 
3780     # NOTE(xqueralt): Prevent from having multiple swap devices for the
3781     # same instance. This will delete all the existing ones.
3782     if block_device.new_format_is_swap(values):
3783         query = _block_device_mapping_get_query(context)
3784         query = query.filter_by(instance_uuid=values['instance_uuid'],
3785                                 source_type='blank', guest_format='swap')
3786         query = query.filter(models.BlockDeviceMapping.id != result.id)
3787         query.soft_delete()
3788 
3789     return result
3790 
3791 
3792 @require_context
3793 @pick_context_manager_reader_allow_async
3794 def block_device_mapping_get_all_by_instance_uuids(context, instance_uuids):
3795     if not instance_uuids:
3796         return []
3797     return _block_device_mapping_get_query(context).filter(
3798         models.BlockDeviceMapping.instance_uuid.in_(instance_uuids)).all()
3799 
3800 
3801 @require_context
3802 @pick_context_manager_reader_allow_async
3803 def block_device_mapping_get_all_by_instance(context, instance_uuid):
3804     return _block_device_mapping_get_query(context).\
3805                  filter_by(instance_uuid=instance_uuid).\
3806                  all()
3807 
3808 
3809 @require_context
3810 @pick_context_manager_reader
3811 def block_device_mapping_get_all_by_volume_id(context, volume_id,
3812         columns_to_join=None):
3813     return _block_device_mapping_get_query(context,
3814             columns_to_join=columns_to_join).\
3815                  filter_by(volume_id=volume_id).\
3816                  all()
3817 
3818 
3819 @require_context
3820 @pick_context_manager_reader
3821 def block_device_mapping_get_by_instance_and_volume_id(context, volume_id,
3822                                                        instance_uuid,
3823                                                        columns_to_join=None):
3824     return _block_device_mapping_get_query(context,
3825             columns_to_join=columns_to_join).\
3826                  filter_by(volume_id=volume_id).\
3827                  filter_by(instance_uuid=instance_uuid).\
3828                  first()
3829 
3830 
3831 @require_context
3832 @pick_context_manager_writer
3833 def block_device_mapping_destroy(context, bdm_id):
3834     _block_device_mapping_get_query(context).\
3835             filter_by(id=bdm_id).\
3836             soft_delete()
3837 
3838 
3839 @require_context
3840 @pick_context_manager_writer
3841 def block_device_mapping_destroy_by_instance_and_volume(context, instance_uuid,
3842                                                         volume_id):
3843     _block_device_mapping_get_query(context).\
3844             filter_by(instance_uuid=instance_uuid).\
3845             filter_by(volume_id=volume_id).\
3846             soft_delete()
3847 
3848 
3849 @require_context
3850 @pick_context_manager_writer
3851 def block_device_mapping_destroy_by_instance_and_device(context, instance_uuid,
3852                                                         device_name):
3853     _block_device_mapping_get_query(context).\
3854             filter_by(instance_uuid=instance_uuid).\
3855             filter_by(device_name=device_name).\
3856             soft_delete()
3857 
3858 
3859 ###################
3860 
3861 
3862 @require_context
3863 @pick_context_manager_writer
3864 def security_group_create(context, values):
3865     security_group_ref = models.SecurityGroup()
3866     # FIXME(devcamcar): Unless I do this, rules fails with lazy load exception
3867     # once save() is called.  This will get cleaned up in next orm pass.
3868     security_group_ref.rules
3869     security_group_ref.update(values)
3870     try:
3871         with get_context_manager(context).writer.savepoint.using(context):
3872             security_group_ref.save(context.session)
3873     except db_exc.DBDuplicateEntry:
3874         raise exception.SecurityGroupExists(
3875                 project_id=values['project_id'],
3876                 security_group_name=values['name'])
3877     return security_group_ref
3878 
3879 
3880 def _security_group_get_query(context, read_deleted=None,
3881                               project_only=False, join_rules=True):
3882     query = model_query(context, models.SecurityGroup,
3883             read_deleted=read_deleted, project_only=project_only)
3884     if join_rules:
3885         query = query.options(joinedload_all('rules.grantee_group'))
3886     return query
3887 
3888 
3889 def _security_group_get_by_names(context, group_names):
3890     """Get security group models for a project by a list of names.
3891     Raise SecurityGroupNotFoundForProject for a name not found.
3892     """
3893     query = _security_group_get_query(context, read_deleted="no",
3894                                       join_rules=False).\
3895             filter_by(project_id=context.project_id).\
3896             filter(models.SecurityGroup.name.in_(group_names))
3897     sg_models = query.all()
3898     if len(sg_models) == len(group_names):
3899         return sg_models
3900     # Find the first one missing and raise
3901     group_names_from_models = [x.name for x in sg_models]
3902     for group_name in group_names:
3903         if group_name not in group_names_from_models:
3904             raise exception.SecurityGroupNotFoundForProject(
3905                 project_id=context.project_id, security_group_id=group_name)
3906     # Not Reached
3907 
3908 
3909 @require_context
3910 @pick_context_manager_reader
3911 def security_group_get_all(context):
3912     return _security_group_get_query(context).all()
3913 
3914 
3915 @require_context
3916 @pick_context_manager_reader
3917 def security_group_get(context, security_group_id, columns_to_join=None):
3918     join_rules = columns_to_join and 'rules' in columns_to_join
3919     if join_rules:
3920         columns_to_join.remove('rules')
3921     query = _security_group_get_query(context, project_only=True,
3922                                       join_rules=join_rules).\
3923                     filter_by(id=security_group_id)
3924 
3925     if columns_to_join is None:
3926         columns_to_join = []
3927     for column in columns_to_join:
3928         if column.startswith('instances'):
3929             query = query.options(joinedload_all(column))
3930 
3931     result = query.first()
3932     if not result:
3933         raise exception.SecurityGroupNotFound(
3934                 security_group_id=security_group_id)
3935 
3936     return result
3937 
3938 
3939 @require_context
3940 @pick_context_manager_reader
3941 def security_group_get_by_name(context, project_id, group_name,
3942                                columns_to_join=None):
3943     query = _security_group_get_query(context,
3944                                       read_deleted="no", join_rules=False).\
3945             filter_by(project_id=project_id).\
3946             filter_by(name=group_name)
3947 
3948     if columns_to_join is None:
3949         columns_to_join = ['instances', 'rules.grantee_group']
3950 
3951     for column in columns_to_join:
3952         query = query.options(joinedload_all(column))
3953 
3954     result = query.first()
3955     if not result:
3956         raise exception.SecurityGroupNotFoundForProject(
3957                 project_id=project_id, security_group_id=group_name)
3958 
3959     return result
3960 
3961 
3962 @require_context
3963 @pick_context_manager_reader
3964 def security_group_get_by_project(context, project_id):
3965     return _security_group_get_query(context, read_deleted="no").\
3966                         filter_by(project_id=project_id).\
3967                         all()
3968 
3969 
3970 @require_context
3971 @pick_context_manager_reader
3972 def security_group_get_by_instance(context, instance_uuid):
3973     return _security_group_get_query(context, read_deleted="no").\
3974                    join(models.SecurityGroup.instances).\
3975                    filter_by(uuid=instance_uuid).\
3976                    all()
3977 
3978 
3979 @require_context
3980 @pick_context_manager_reader
3981 def security_group_in_use(context, group_id):
3982     # Are there any instances that haven't been deleted
3983     # that include this group?
3984     inst_assoc = model_query(context,
3985                              models.SecurityGroupInstanceAssociation,
3986                              read_deleted="no").\
3987                     filter_by(security_group_id=group_id).\
3988                     all()
3989     for ia in inst_assoc:
3990         num_instances = model_query(context, models.Instance,
3991                                     read_deleted="no").\
3992                     filter_by(uuid=ia.instance_uuid).\
3993                     count()
3994         if num_instances:
3995             return True
3996 
3997     return False
3998 
3999 
4000 @require_context
4001 @pick_context_manager_writer
4002 def security_group_update(context, security_group_id, values,
4003                           columns_to_join=None):
4004     query = model_query(context, models.SecurityGroup).filter_by(
4005         id=security_group_id)
4006     if columns_to_join:
4007         for column in columns_to_join:
4008             query = query.options(joinedload_all(column))
4009     security_group_ref = query.first()
4010 
4011     if not security_group_ref:
4012         raise exception.SecurityGroupNotFound(
4013                 security_group_id=security_group_id)
4014     security_group_ref.update(values)
4015     name = security_group_ref['name']
4016     project_id = security_group_ref['project_id']
4017     try:
4018         security_group_ref.save(context.session)
4019     except db_exc.DBDuplicateEntry:
4020         raise exception.SecurityGroupExists(
4021                 project_id=project_id,
4022                 security_group_name=name)
4023     return security_group_ref
4024 
4025 
4026 def security_group_ensure_default(context):
4027     """Ensure default security group exists for a project_id."""
4028 
4029     try:
4030         # NOTE(rpodolyaka): create the default security group, if it doesn't
4031         # exist. This must be done in a separate transaction, so that
4032         # this one is not aborted in case a concurrent one succeeds first
4033         # and the unique constraint for security group names is violated
4034         # by a concurrent INSERT
4035         with get_context_manager(context).writer.independent.using(context):
4036             return _security_group_ensure_default(context)
4037     except exception.SecurityGroupExists:
4038         # NOTE(rpodolyaka): a concurrent transaction has succeeded first,
4039         # suppress the error and proceed
4040         return security_group_get_by_name(context, context.project_id,
4041                                           'default')
4042 
4043 
4044 @pick_context_manager_writer
4045 def _security_group_ensure_default(context):
4046     try:
4047         default_group = _security_group_get_by_names(context, ['default'])[0]
4048     except exception.NotFound:
4049         values = {'name': 'default',
4050                   'description': 'default',
4051                   'user_id': context.user_id,
4052                   'project_id': context.project_id}
4053         default_group = security_group_create(context, values)
4054 
4055         default_rules = _security_group_rule_get_default_query(context).all()
4056         for default_rule in default_rules:
4057             # This is suboptimal, it should be programmatic to know
4058             # the values of the default_rule
4059             rule_values = {'protocol': default_rule.protocol,
4060                            'from_port': default_rule.from_port,
4061                            'to_port': default_rule.to_port,
4062                            'cidr': default_rule.cidr,
4063                            'parent_group_id': default_group.id,
4064             }
4065             _security_group_rule_create(context, rule_values)
4066     return default_group
4067 
4068 
4069 @require_context
4070 @pick_context_manager_writer
4071 def security_group_destroy(context, security_group_id):
4072     model_query(context, models.SecurityGroup).\
4073             filter_by(id=security_group_id).\
4074             soft_delete()
4075     model_query(context, models.SecurityGroupInstanceAssociation).\
4076             filter_by(security_group_id=security_group_id).\
4077             soft_delete()
4078     model_query(context, models.SecurityGroupIngressRule).\
4079             filter_by(group_id=security_group_id).\
4080             soft_delete()
4081     model_query(context, models.SecurityGroupIngressRule).\
4082             filter_by(parent_group_id=security_group_id).\
4083             soft_delete()
4084 
4085 
4086 def _security_group_count_by_project_and_user(context, project_id, user_id):
4087     nova.context.authorize_project_context(context, project_id)
4088     return model_query(context, models.SecurityGroup, read_deleted="no").\
4089                    filter_by(project_id=project_id).\
4090                    filter_by(user_id=user_id).\
4091                    count()
4092 
4093 
4094 ###################
4095 
4096 
4097 def _security_group_rule_create(context, values):
4098     security_group_rule_ref = models.SecurityGroupIngressRule()
4099     security_group_rule_ref.update(values)
4100     security_group_rule_ref.save(context.session)
4101     return security_group_rule_ref
4102 
4103 
4104 def _security_group_rule_get_query(context):
4105     return model_query(context, models.SecurityGroupIngressRule)
4106 
4107 
4108 @require_context
4109 @pick_context_manager_reader
4110 def security_group_rule_get(context, security_group_rule_id):
4111     result = (_security_group_rule_get_query(context).
4112                          filter_by(id=security_group_rule_id).
4113                          first())
4114 
4115     if not result:
4116         raise exception.SecurityGroupNotFoundForRule(
4117                                                rule_id=security_group_rule_id)
4118 
4119     return result
4120 
4121 
4122 @require_context
4123 @pick_context_manager_reader
4124 def security_group_rule_get_by_security_group(context, security_group_id,
4125                                               columns_to_join=None):
4126     if columns_to_join is None:
4127         columns_to_join = ['grantee_group.instances.system_metadata',
4128                            'grantee_group.instances.info_cache']
4129     query = (_security_group_rule_get_query(context).
4130              filter_by(parent_group_id=security_group_id))
4131     for column in columns_to_join:
4132         query = query.options(joinedload_all(column))
4133     return query.all()
4134 
4135 
4136 @require_context
4137 @pick_context_manager_reader
4138 def security_group_rule_get_by_instance(context, instance_uuid):
4139     return (_security_group_rule_get_query(context).
4140             join('parent_group', 'instances').
4141             filter_by(uuid=instance_uuid).
4142             options(joinedload('grantee_group')).
4143             all())
4144 
4145 
4146 @require_context
4147 @pick_context_manager_writer
4148 def security_group_rule_create(context, values):
4149     return _security_group_rule_create(context, values)
4150 
4151 
4152 @require_context
4153 @pick_context_manager_writer
4154 def security_group_rule_destroy(context, security_group_rule_id):
4155     count = (_security_group_rule_get_query(context).
4156                     filter_by(id=security_group_rule_id).
4157                     soft_delete())
4158     if count == 0:
4159         raise exception.SecurityGroupNotFoundForRule(
4160                                             rule_id=security_group_rule_id)
4161 
4162 
4163 @require_context
4164 @pick_context_manager_reader
4165 def security_group_rule_count_by_group(context, security_group_id):
4166     return (model_query(context, models.SecurityGroupIngressRule,
4167                    read_deleted="no").
4168                    filter_by(parent_group_id=security_group_id).
4169                    count())
4170 
4171 
4172 ###################
4173 
4174 
4175 def _security_group_rule_get_default_query(context):
4176     return model_query(context, models.SecurityGroupIngressDefaultRule)
4177 
4178 
4179 @require_context
4180 @pick_context_manager_reader
4181 def security_group_default_rule_get(context, security_group_rule_default_id):
4182     result = _security_group_rule_get_default_query(context).\
4183                         filter_by(id=security_group_rule_default_id).\
4184                         first()
4185 
4186     if not result:
4187         raise exception.SecurityGroupDefaultRuleNotFound(
4188                                         rule_id=security_group_rule_default_id)
4189 
4190     return result
4191 
4192 
4193 @pick_context_manager_writer
4194 def security_group_default_rule_destroy(context,
4195                                         security_group_rule_default_id):
4196     count = _security_group_rule_get_default_query(context).\
4197                         filter_by(id=security_group_rule_default_id).\
4198                         soft_delete()
4199     if count == 0:
4200         raise exception.SecurityGroupDefaultRuleNotFound(
4201                                     rule_id=security_group_rule_default_id)
4202 
4203 
4204 @pick_context_manager_writer
4205 def security_group_default_rule_create(context, values):
4206     security_group_default_rule_ref = models.SecurityGroupIngressDefaultRule()
4207     security_group_default_rule_ref.update(values)
4208     security_group_default_rule_ref.save(context.session)
4209     return security_group_default_rule_ref
4210 
4211 
4212 @require_context
4213 @pick_context_manager_reader
4214 def security_group_default_rule_list(context):
4215     return _security_group_rule_get_default_query(context).all()
4216 
4217 
4218 ###################
4219 
4220 
4221 @pick_context_manager_writer
4222 def provider_fw_rule_create(context, rule):
4223     fw_rule_ref = models.ProviderFirewallRule()
4224     fw_rule_ref.update(rule)
4225     fw_rule_ref.save(context.session)
4226     return fw_rule_ref
4227 
4228 
4229 @pick_context_manager_reader
4230 def provider_fw_rule_get_all(context):
4231     return model_query(context, models.ProviderFirewallRule).all()
4232 
4233 
4234 @pick_context_manager_writer
4235 def provider_fw_rule_destroy(context, rule_id):
4236     context.session.query(models.ProviderFirewallRule).\
4237         filter_by(id=rule_id).\
4238         soft_delete()
4239 
4240 
4241 ###################
4242 
4243 
4244 @require_context
4245 @pick_context_manager_writer
4246 def project_get_networks(context, project_id, associate=True):
4247     # NOTE(tr3buchet): as before this function will associate
4248     # a project with a network if it doesn't have one and
4249     # associate is true
4250     result = model_query(context, models.Network, read_deleted="no").\
4251                      filter_by(project_id=project_id).\
4252                      all()
4253 
4254     if not result:
4255         if not associate:
4256             return []
4257 
4258         return [network_associate(context, project_id)]
4259 
4260     return result
4261 
4262 
4263 ###################
4264 
4265 
4266 @pick_context_manager_writer
4267 def migration_create(context, values):
4268     migration = models.Migration()
4269     migration.update(values)
4270     migration.save(context.session)
4271     return migration
4272 
4273 
4274 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
4275 @pick_context_manager_writer
4276 def migration_update(context, id, values):
4277     migration = migration_get(context, id)
4278     migration.update(values)
4279 
4280     return migration
4281 
4282 
4283 @pick_context_manager_reader
4284 def migration_get(context, id):
4285     result = model_query(context, models.Migration, read_deleted="yes").\
4286                      filter_by(id=id).\
4287                      first()
4288 
4289     if not result:
4290         raise exception.MigrationNotFound(migration_id=id)
4291 
4292     return result
4293 
4294 
4295 @pick_context_manager_reader
4296 def migration_get_by_uuid(context, migration_uuid):
4297     result = model_query(context, models.Migration, read_deleted="yes").\
4298                      filter_by(uuid=migration_uuid).\
4299                      first()
4300 
4301     if not result:
4302         raise exception.MigrationNotFound(migration_id=migration_uuid)
4303 
4304     return result
4305 
4306 
4307 @pick_context_manager_reader
4308 def migration_get_by_id_and_instance(context, id, instance_uuid):
4309     result = model_query(context, models.Migration).\
4310                      filter_by(id=id).\
4311                      filter_by(instance_uuid=instance_uuid).\
4312                      first()
4313 
4314     if not result:
4315         raise exception.MigrationNotFoundForInstance(migration_id=id,
4316                                                      instance_id=instance_uuid)
4317 
4318     return result
4319 
4320 
4321 @pick_context_manager_reader
4322 def migration_get_by_instance_and_status(context, instance_uuid, status):
4323     result = model_query(context, models.Migration, read_deleted="yes").\
4324                      filter_by(instance_uuid=instance_uuid).\
4325                      filter_by(status=status).\
4326                      first()
4327 
4328     if not result:
4329         raise exception.MigrationNotFoundByStatus(instance_id=instance_uuid,
4330                                                   status=status)
4331 
4332     return result
4333 
4334 
4335 @pick_context_manager_reader_allow_async
4336 def migration_get_unconfirmed_by_dest_compute(context, confirm_window,
4337                                               dest_compute):
4338     confirm_window = (timeutils.utcnow() -
4339                       datetime.timedelta(seconds=confirm_window))
4340 
4341     return model_query(context, models.Migration, read_deleted="yes").\
4342              filter(models.Migration.updated_at <= confirm_window).\
4343              filter_by(status="finished").\
4344              filter_by(dest_compute=dest_compute).\
4345              all()
4346 
4347 
4348 @pick_context_manager_reader
4349 def migration_get_in_progress_by_host_and_node(context, host, node):
4350     # TODO(mriedem): Tracking what various code flows set for
4351     # migration status is nutty, since it happens all over the place
4352     # and several of the statuses are redundant (done and completed).
4353     # We need to define these in an enum somewhere and just update
4354     # that one central place that defines what "in progress" means.
4355     # NOTE(mriedem): The 'finished' status is not in this list because
4356     # 'finished' means a resize is finished on the destination host
4357     # and the instance is in VERIFY_RESIZE state, so the end state
4358     # for a resize is actually 'confirmed' or 'reverted'.
4359     return model_query(context, models.Migration).\
4360             filter(or_(and_(models.Migration.source_compute == host,
4361                             models.Migration.source_node == node),
4362                        and_(models.Migration.dest_compute == host,
4363                             models.Migration.dest_node == node))).\
4364             filter(~models.Migration.status.in_(['accepted', 'confirmed',
4365                                                  'reverted', 'error',
4366                                                  'failed', 'completed',
4367                                                  'cancelled', 'done'])).\
4368             options(joinedload_all('instance.system_metadata')).\
4369             all()
4370 
4371 
4372 @pick_context_manager_reader
4373 def migration_get_in_progress_by_instance(context, instance_uuid,
4374                                           migration_type=None):
4375     # TODO(Shaohe Feng) we should share the in-progress list.
4376     # TODO(Shaohe Feng) will also summarize all status to a new
4377     # MigrationStatus class.
4378     query = model_query(context, models.Migration).\
4379             filter_by(instance_uuid=instance_uuid).\
4380             filter(models.Migration.status.in_(['queued', 'preparing',
4381                                                 'running',
4382                                                 'post-migrating']))
4383     if migration_type:
4384         query = query.filter(models.Migration.migration_type == migration_type)
4385 
4386     return query.all()
4387 
4388 
4389 @pick_context_manager_reader
4390 def migration_get_all_by_filters(context, filters,
4391                                  sort_keys=None, sort_dirs=None,
4392                                  limit=None, marker=None):
4393     if limit == 0:
4394         return []
4395 
4396     query = model_query(context, models.Migration)
4397     if "uuid" in filters:
4398         # The uuid filter is here for the MigrationLister and multi-cell
4399         # paging support in the compute API.
4400         uuid = filters["uuid"]
4401         uuid = [uuid] if isinstance(uuid, six.string_types) else uuid
4402         query = query.filter(models.Migration.uuid.in_(uuid))
4403 
4404     model_object = models.Migration
4405     query = _get_query_nova_resource_by_changes_time(query,
4406                                                      filters,
4407                                                      model_object)
4408 
4409     if "status" in filters:
4410         status = filters["status"]
4411         status = [status] if isinstance(status, six.string_types) else status
4412         query = query.filter(models.Migration.status.in_(status))
4413     if "host" in filters:
4414         host = filters["host"]
4415         query = query.filter(or_(models.Migration.source_compute == host,
4416                                  models.Migration.dest_compute == host))
4417     elif "source_compute" in filters:
4418         host = filters['source_compute']
4419         query = query.filter(models.Migration.source_compute == host)
4420     if "migration_type" in filters:
4421         migtype = filters["migration_type"]
4422         query = query.filter(models.Migration.migration_type == migtype)
4423     if "hidden" in filters:
4424         hidden = filters["hidden"]
4425         query = query.filter(models.Migration.hidden == hidden)
4426     if "instance_uuid" in filters:
4427         instance_uuid = filters["instance_uuid"]
4428         query = query.filter(models.Migration.instance_uuid == instance_uuid)
4429     if marker:
4430         try:
4431             marker = migration_get_by_uuid(context, marker)
4432         except exception.MigrationNotFound:
4433             raise exception.MarkerNotFound(marker=marker)
4434     if limit or marker or sort_keys or sort_dirs:
4435         # Default sort by desc(['created_at', 'id'])
4436         sort_keys, sort_dirs = process_sort_params(sort_keys, sort_dirs,
4437                                                    default_dir='desc')
4438         return sqlalchemyutils.paginate_query(query,
4439                                               models.Migration,
4440                                               limit=limit,
4441                                               sort_keys=sort_keys,
4442                                               marker=marker,
4443                                               sort_dirs=sort_dirs).all()
4444     else:
4445         return query.all()
4446 
4447 
4448 @require_context
4449 @pick_context_manager_reader_allow_async
4450 def migration_get_by_sort_filters(context, sort_keys, sort_dirs, values):
4451     """Attempt to get a single migration based on a combination of sort
4452     keys, directions and filter values. This is used to try to find a
4453     marker migration when we don't have a marker uuid.
4454 
4455     This returns just a uuid of the migration that matched.
4456     """
4457     model = models.Migration
4458     return _model_get_uuid_by_sort_filters(context, model, sort_keys,
4459                                            sort_dirs, values)
4460 
4461 
4462 @pick_context_manager_writer
4463 def migration_migrate_to_uuid(context, count):
4464     # Avoid circular import
4465     from nova import objects
4466 
4467     db_migrations = model_query(context, models.Migration).filter_by(
4468         uuid=None).limit(count).all()
4469 
4470     done = 0
4471     for db_migration in db_migrations:
4472         mig = objects.Migration(context)
4473         mig._from_db_object(context, mig, db_migration)
4474         done += 1
4475 
4476     # We don't have any situation where we can (detectably) not
4477     # migrate a thing, so report anything that matched as "completed".
4478     return done, done
4479 
4480 
4481 ##################
4482 
4483 
4484 @pick_context_manager_writer
4485 def console_pool_create(context, values):
4486     pool = models.ConsolePool()
4487     pool.update(values)
4488     try:
4489         pool.save(context.session)
4490     except db_exc.DBDuplicateEntry:
4491         raise exception.ConsolePoolExists(
4492             host=values["host"],
4493             console_type=values["console_type"],
4494             compute_host=values["compute_host"],
4495         )
4496     return pool
4497 
4498 
4499 @pick_context_manager_reader
4500 def console_pool_get_by_host_type(context, compute_host, host,
4501                                   console_type):
4502 
4503     result = model_query(context, models.ConsolePool, read_deleted="no").\
4504                    filter_by(host=host).\
4505                    filter_by(console_type=console_type).\
4506                    filter_by(compute_host=compute_host).\
4507                    options(joinedload('consoles')).\
4508                    first()
4509 
4510     if not result:
4511         raise exception.ConsolePoolNotFoundForHostType(
4512                 host=host, console_type=console_type,
4513                 compute_host=compute_host)
4514 
4515     return result
4516 
4517 
4518 @pick_context_manager_reader
4519 def console_pool_get_all_by_host_type(context, host, console_type):
4520     return model_query(context, models.ConsolePool, read_deleted="no").\
4521                    filter_by(host=host).\
4522                    filter_by(console_type=console_type).\
4523                    options(joinedload('consoles')).\
4524                    all()
4525 
4526 
4527 ##################
4528 
4529 
4530 @pick_context_manager_writer
4531 def console_create(context, values):
4532     console = models.Console()
4533     console.update(values)
4534     console.save(context.session)
4535     return console
4536 
4537 
4538 @pick_context_manager_writer
4539 def console_delete(context, console_id):
4540     # NOTE(mdragon): consoles are meant to be transient.
4541     context.session.query(models.Console).\
4542         filter_by(id=console_id).\
4543         delete()
4544 
4545 
4546 @pick_context_manager_reader
4547 def console_get_by_pool_instance(context, pool_id, instance_uuid):
4548     result = model_query(context, models.Console, read_deleted="yes").\
4549                    filter_by(pool_id=pool_id).\
4550                    filter_by(instance_uuid=instance_uuid).\
4551                    options(joinedload('pool')).\
4552                    first()
4553 
4554     if not result:
4555         raise exception.ConsoleNotFoundInPoolForInstance(
4556                 pool_id=pool_id, instance_uuid=instance_uuid)
4557 
4558     return result
4559 
4560 
4561 @pick_context_manager_reader
4562 def console_get_all_by_instance(context, instance_uuid, columns_to_join=None):
4563     query = model_query(context, models.Console, read_deleted="yes").\
4564                 filter_by(instance_uuid=instance_uuid)
4565     if columns_to_join:
4566         for column in columns_to_join:
4567             query = query.options(joinedload(column))
4568     return query.all()
4569 
4570 
4571 @pick_context_manager_reader
4572 def console_get(context, console_id, instance_uuid=None):
4573     query = model_query(context, models.Console, read_deleted="yes").\
4574                     filter_by(id=console_id).\
4575                     options(joinedload('pool'))
4576 
4577     if instance_uuid is not None:
4578         query = query.filter_by(instance_uuid=instance_uuid)
4579 
4580     result = query.first()
4581 
4582     if not result:
4583         if instance_uuid:
4584             raise exception.ConsoleNotFoundForInstance(
4585                     instance_uuid=instance_uuid)
4586         else:
4587             raise exception.ConsoleNotFound(console_id=console_id)
4588 
4589     return result
4590 
4591 
4592 ##################
4593 
4594 
4595 @pick_context_manager_writer
4596 def cell_create(context, values):
4597     cell = models.Cell()
4598     cell.update(values)
4599     try:
4600         cell.save(context.session)
4601     except db_exc.DBDuplicateEntry:
4602         raise exception.CellExists(name=values['name'])
4603     return cell
4604 
4605 
4606 def _cell_get_by_name_query(context, cell_name):
4607     return model_query(context, models.Cell).filter_by(name=cell_name)
4608 
4609 
4610 @pick_context_manager_writer
4611 def cell_update(context, cell_name, values):
4612     cell_query = _cell_get_by_name_query(context, cell_name)
4613     if not cell_query.update(values):
4614         raise exception.CellNotFound(cell_name=cell_name)
4615     cell = cell_query.first()
4616     return cell
4617 
4618 
4619 @pick_context_manager_writer
4620 def cell_delete(context, cell_name):
4621     return _cell_get_by_name_query(context, cell_name).soft_delete()
4622 
4623 
4624 @pick_context_manager_reader
4625 def cell_get(context, cell_name):
4626     result = _cell_get_by_name_query(context, cell_name).first()
4627     if not result:
4628         raise exception.CellNotFound(cell_name=cell_name)
4629     return result
4630 
4631 
4632 @pick_context_manager_reader
4633 def cell_get_all(context):
4634     return model_query(context, models.Cell, read_deleted="no").all()
4635 
4636 
4637 ########################
4638 # User-provided metadata
4639 
4640 def _instance_metadata_get_multi(context, instance_uuids):
4641     if not instance_uuids:
4642         return []
4643     return model_query(context, models.InstanceMetadata).filter(
4644         models.InstanceMetadata.instance_uuid.in_(instance_uuids))
4645 
4646 
4647 def _instance_metadata_get_query(context, instance_uuid):
4648     return model_query(context, models.InstanceMetadata, read_deleted="no").\
4649                     filter_by(instance_uuid=instance_uuid)
4650 
4651 
4652 @require_context
4653 @pick_context_manager_reader
4654 def instance_metadata_get(context, instance_uuid):
4655     rows = _instance_metadata_get_query(context, instance_uuid).all()
4656     return {row['key']: row['value'] for row in rows}
4657 
4658 
4659 @require_context
4660 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
4661 @pick_context_manager_writer
4662 def instance_metadata_delete(context, instance_uuid, key):
4663     _instance_metadata_get_query(context, instance_uuid).\
4664         filter_by(key=key).\
4665         soft_delete()
4666 
4667 
4668 @require_context
4669 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
4670 @pick_context_manager_writer
4671 def instance_metadata_update(context, instance_uuid, metadata, delete):
4672     all_keys = metadata.keys()
4673     if delete:
4674         _instance_metadata_get_query(context, instance_uuid).\
4675             filter(~models.InstanceMetadata.key.in_(all_keys)).\
4676             soft_delete(synchronize_session=False)
4677 
4678     already_existing_keys = []
4679     meta_refs = _instance_metadata_get_query(context, instance_uuid).\
4680         filter(models.InstanceMetadata.key.in_(all_keys)).\
4681         all()
4682 
4683     for meta_ref in meta_refs:
4684         already_existing_keys.append(meta_ref.key)
4685         meta_ref.update({"value": metadata[meta_ref.key]})
4686 
4687     new_keys = set(all_keys) - set(already_existing_keys)
4688     for key in new_keys:
4689         meta_ref = models.InstanceMetadata()
4690         meta_ref.update({"key": key, "value": metadata[key],
4691                          "instance_uuid": instance_uuid})
4692         context.session.add(meta_ref)
4693 
4694     return metadata
4695 
4696 
4697 #######################
4698 # System-owned metadata
4699 
4700 
4701 def _instance_system_metadata_get_multi(context, instance_uuids):
4702     if not instance_uuids:
4703         return []
4704     return model_query(context, models.InstanceSystemMetadata,
4705                        read_deleted='yes').filter(
4706         models.InstanceSystemMetadata.instance_uuid.in_(instance_uuids))
4707 
4708 
4709 def _instance_system_metadata_get_query(context, instance_uuid):
4710     return model_query(context, models.InstanceSystemMetadata).\
4711                     filter_by(instance_uuid=instance_uuid)
4712 
4713 
4714 @require_context
4715 @pick_context_manager_reader
4716 def instance_system_metadata_get(context, instance_uuid):
4717     rows = _instance_system_metadata_get_query(context, instance_uuid).all()
4718     return {row['key']: row['value'] for row in rows}
4719 
4720 
4721 @require_context
4722 @pick_context_manager_writer
4723 def instance_system_metadata_update(context, instance_uuid, metadata, delete):
4724     all_keys = metadata.keys()
4725     if delete:
4726         _instance_system_metadata_get_query(context, instance_uuid).\
4727             filter(~models.InstanceSystemMetadata.key.in_(all_keys)).\
4728             soft_delete(synchronize_session=False)
4729 
4730     already_existing_keys = []
4731     meta_refs = _instance_system_metadata_get_query(context, instance_uuid).\
4732         filter(models.InstanceSystemMetadata.key.in_(all_keys)).\
4733         all()
4734 
4735     for meta_ref in meta_refs:
4736         already_existing_keys.append(meta_ref.key)
4737         meta_ref.update({"value": metadata[meta_ref.key]})
4738 
4739     new_keys = set(all_keys) - set(already_existing_keys)
4740     for key in new_keys:
4741         meta_ref = models.InstanceSystemMetadata()
4742         meta_ref.update({"key": key, "value": metadata[key],
4743                          "instance_uuid": instance_uuid})
4744         context.session.add(meta_ref)
4745 
4746     return metadata
4747 
4748 
4749 ####################
4750 
4751 
4752 @pick_context_manager_writer
4753 def agent_build_create(context, values):
4754     agent_build_ref = models.AgentBuild()
4755     agent_build_ref.update(values)
4756     try:
4757         agent_build_ref.save(context.session)
4758     except db_exc.DBDuplicateEntry:
4759         raise exception.AgentBuildExists(hypervisor=values['hypervisor'],
4760                         os=values['os'], architecture=values['architecture'])
4761     return agent_build_ref
4762 
4763 
4764 @pick_context_manager_reader
4765 def agent_build_get_by_triple(context, hypervisor, os, architecture):
4766     return model_query(context, models.AgentBuild, read_deleted="no").\
4767                    filter_by(hypervisor=hypervisor).\
4768                    filter_by(os=os).\
4769                    filter_by(architecture=architecture).\
4770                    first()
4771 
4772 
4773 @pick_context_manager_reader
4774 def agent_build_get_all(context, hypervisor=None):
4775     if hypervisor:
4776         return model_query(context, models.AgentBuild, read_deleted="no").\
4777                    filter_by(hypervisor=hypervisor).\
4778                    all()
4779     else:
4780         return model_query(context, models.AgentBuild, read_deleted="no").\
4781                    all()
4782 
4783 
4784 @pick_context_manager_writer
4785 def agent_build_destroy(context, agent_build_id):
4786     rows_affected = model_query(context, models.AgentBuild).filter_by(
4787                                         id=agent_build_id).soft_delete()
4788     if rows_affected == 0:
4789         raise exception.AgentBuildNotFound(id=agent_build_id)
4790 
4791 
4792 @pick_context_manager_writer
4793 def agent_build_update(context, agent_build_id, values):
4794     rows_affected = model_query(context, models.AgentBuild).\
4795                    filter_by(id=agent_build_id).\
4796                    update(values)
4797     if rows_affected == 0:
4798         raise exception.AgentBuildNotFound(id=agent_build_id)
4799 
4800 
4801 ####################
4802 
4803 @require_context
4804 @pick_context_manager_reader_allow_async
4805 def bw_usage_get(context, uuid, start_period, mac):
4806     values = {'start_period': start_period}
4807     values = convert_objects_related_datetimes(values, 'start_period')
4808     return model_query(context, models.BandwidthUsage, read_deleted="yes").\
4809                            filter_by(start_period=values['start_period']).\
4810                            filter_by(uuid=uuid).\
4811                            filter_by(mac=mac).\
4812                            first()
4813 
4814 
4815 @require_context
4816 @pick_context_manager_reader_allow_async
4817 def bw_usage_get_by_uuids(context, uuids, start_period):
4818     values = {'start_period': start_period}
4819     values = convert_objects_related_datetimes(values, 'start_period')
4820     return (
4821         model_query(context, models.BandwidthUsage, read_deleted="yes").
4822         filter(models.BandwidthUsage.uuid.in_(uuids)).
4823         filter_by(start_period=values['start_period']).
4824         all()
4825     )
4826 
4827 
4828 @require_context
4829 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
4830 @pick_context_manager_writer
4831 def bw_usage_update(context, uuid, mac, start_period, bw_in, bw_out,
4832                     last_ctr_in, last_ctr_out, last_refreshed=None):
4833 
4834     if last_refreshed is None:
4835         last_refreshed = timeutils.utcnow()
4836 
4837     # NOTE(comstud): More often than not, we'll be updating records vs
4838     # creating records.  Optimize accordingly, trying to update existing
4839     # records.  Fall back to creation when no rows are updated.
4840     ts_values = {'last_refreshed': last_refreshed,
4841                  'start_period': start_period}
4842     ts_keys = ('start_period', 'last_refreshed')
4843     ts_values = convert_objects_related_datetimes(ts_values, *ts_keys)
4844     values = {'last_refreshed': ts_values['last_refreshed'],
4845               'last_ctr_in': last_ctr_in,
4846               'last_ctr_out': last_ctr_out,
4847               'bw_in': bw_in,
4848               'bw_out': bw_out}
4849     # NOTE(pkholkin): order_by() is needed here to ensure that the
4850     # same record is updated every time. It can be removed after adding
4851     # unique constraint to this model.
4852     bw_usage = model_query(context, models.BandwidthUsage,
4853             read_deleted='yes').\
4854                     filter_by(start_period=ts_values['start_period']).\
4855                     filter_by(uuid=uuid).\
4856                     filter_by(mac=mac).\
4857                     order_by(asc(models.BandwidthUsage.id)).first()
4858 
4859     if bw_usage:
4860         bw_usage.update(values)
4861         return bw_usage
4862 
4863     bwusage = models.BandwidthUsage()
4864     bwusage.start_period = ts_values['start_period']
4865     bwusage.uuid = uuid
4866     bwusage.mac = mac
4867     bwusage.last_refreshed = ts_values['last_refreshed']
4868     bwusage.bw_in = bw_in
4869     bwusage.bw_out = bw_out
4870     bwusage.last_ctr_in = last_ctr_in
4871     bwusage.last_ctr_out = last_ctr_out
4872     bwusage.save(context.session)
4873 
4874     return bwusage
4875 
4876 
4877 ####################
4878 
4879 
4880 @require_context
4881 @pick_context_manager_reader
4882 def vol_get_usage_by_time(context, begin):
4883     """Return volumes usage that have been updated after a specified time."""
4884     return model_query(context, models.VolumeUsage, read_deleted="yes").\
4885                    filter(or_(models.VolumeUsage.tot_last_refreshed == null(),
4886                               models.VolumeUsage.tot_last_refreshed > begin,
4887                               models.VolumeUsage.curr_last_refreshed == null(),
4888                               models.VolumeUsage.curr_last_refreshed > begin,
4889                               )).all()
4890 
4891 
4892 @require_context
4893 @pick_context_manager_writer
4894 def vol_usage_update(context, id, rd_req, rd_bytes, wr_req, wr_bytes,
4895                      instance_id, project_id, user_id, availability_zone,
4896                      update_totals=False):
4897 
4898     refreshed = timeutils.utcnow()
4899 
4900     values = {}
4901     # NOTE(dricco): We will be mostly updating current usage records vs
4902     # updating total or creating records. Optimize accordingly.
4903     if not update_totals:
4904         values = {'curr_last_refreshed': refreshed,
4905                   'curr_reads': rd_req,
4906                   'curr_read_bytes': rd_bytes,
4907                   'curr_writes': wr_req,
4908                   'curr_write_bytes': wr_bytes,
4909                   'instance_uuid': instance_id,
4910                   'project_id': project_id,
4911                   'user_id': user_id,
4912                   'availability_zone': availability_zone}
4913     else:
4914         values = {'tot_last_refreshed': refreshed,
4915                   'tot_reads': models.VolumeUsage.tot_reads + rd_req,
4916                   'tot_read_bytes': models.VolumeUsage.tot_read_bytes +
4917                                     rd_bytes,
4918                   'tot_writes': models.VolumeUsage.tot_writes + wr_req,
4919                   'tot_write_bytes': models.VolumeUsage.tot_write_bytes +
4920                                      wr_bytes,
4921                   'curr_reads': 0,
4922                   'curr_read_bytes': 0,
4923                   'curr_writes': 0,
4924                   'curr_write_bytes': 0,
4925                   'instance_uuid': instance_id,
4926                   'project_id': project_id,
4927                   'user_id': user_id,
4928                   'availability_zone': availability_zone}
4929 
4930     current_usage = model_query(context, models.VolumeUsage,
4931                         read_deleted="yes").\
4932                         filter_by(volume_id=id).\
4933                         first()
4934     if current_usage:
4935         if (rd_req < current_usage['curr_reads'] or
4936             rd_bytes < current_usage['curr_read_bytes'] or
4937             wr_req < current_usage['curr_writes'] or
4938                 wr_bytes < current_usage['curr_write_bytes']):
4939             LOG.info("Volume(%s) has lower stats then what is in "
4940                      "the database. Instance must have been rebooted "
4941                      "or crashed. Updating totals.", id)
4942             if not update_totals:
4943                 values['tot_reads'] = (models.VolumeUsage.tot_reads +
4944                                        current_usage['curr_reads'])
4945                 values['tot_read_bytes'] = (
4946                     models.VolumeUsage.tot_read_bytes +
4947                     current_usage['curr_read_bytes'])
4948                 values['tot_writes'] = (models.VolumeUsage.tot_writes +
4949                                         current_usage['curr_writes'])
4950                 values['tot_write_bytes'] = (
4951                     models.VolumeUsage.tot_write_bytes +
4952                     current_usage['curr_write_bytes'])
4953             else:
4954                 values['tot_reads'] = (models.VolumeUsage.tot_reads +
4955                                        current_usage['curr_reads'] +
4956                                        rd_req)
4957                 values['tot_read_bytes'] = (
4958                     models.VolumeUsage.tot_read_bytes +
4959                     current_usage['curr_read_bytes'] + rd_bytes)
4960                 values['tot_writes'] = (models.VolumeUsage.tot_writes +
4961                                         current_usage['curr_writes'] +
4962                                         wr_req)
4963                 values['tot_write_bytes'] = (
4964                     models.VolumeUsage.tot_write_bytes +
4965                     current_usage['curr_write_bytes'] + wr_bytes)
4966 
4967         current_usage.update(values)
4968         current_usage.save(context.session)
4969         context.session.refresh(current_usage)
4970         return current_usage
4971 
4972     vol_usage = models.VolumeUsage()
4973     vol_usage.volume_id = id
4974     vol_usage.instance_uuid = instance_id
4975     vol_usage.project_id = project_id
4976     vol_usage.user_id = user_id
4977     vol_usage.availability_zone = availability_zone
4978 
4979     if not update_totals:
4980         vol_usage.curr_last_refreshed = refreshed
4981         vol_usage.curr_reads = rd_req
4982         vol_usage.curr_read_bytes = rd_bytes
4983         vol_usage.curr_writes = wr_req
4984         vol_usage.curr_write_bytes = wr_bytes
4985     else:
4986         vol_usage.tot_last_refreshed = refreshed
4987         vol_usage.tot_reads = rd_req
4988         vol_usage.tot_read_bytes = rd_bytes
4989         vol_usage.tot_writes = wr_req
4990         vol_usage.tot_write_bytes = wr_bytes
4991 
4992     vol_usage.save(context.session)
4993 
4994     return vol_usage
4995 
4996 
4997 ####################
4998 
4999 
5000 @pick_context_manager_reader
5001 def s3_image_get(context, image_id):
5002     """Find local s3 image represented by the provided id."""
5003     result = model_query(context, models.S3Image, read_deleted="yes").\
5004                  filter_by(id=image_id).\
5005                  first()
5006 
5007     if not result:
5008         raise exception.ImageNotFound(image_id=image_id)
5009 
5010     return result
5011 
5012 
5013 @pick_context_manager_reader
5014 def s3_image_get_by_uuid(context, image_uuid):
5015     """Find local s3 image represented by the provided uuid."""
5016     result = model_query(context, models.S3Image, read_deleted="yes").\
5017                  filter_by(uuid=image_uuid).\
5018                  first()
5019 
5020     if not result:
5021         raise exception.ImageNotFound(image_id=image_uuid)
5022 
5023     return result
5024 
5025 
5026 @pick_context_manager_writer
5027 def s3_image_create(context, image_uuid):
5028     """Create local s3 image represented by provided uuid."""
5029     try:
5030         s3_image_ref = models.S3Image()
5031         s3_image_ref.update({'uuid': image_uuid})
5032         s3_image_ref.save(context.session)
5033     except Exception as e:
5034         raise db_exc.DBError(e)
5035 
5036     return s3_image_ref
5037 
5038 
5039 ####################
5040 
5041 
5042 @pick_context_manager_writer
5043 def instance_fault_create(context, values):
5044     """Create a new InstanceFault."""
5045     fault_ref = models.InstanceFault()
5046     fault_ref.update(values)
5047     fault_ref.save(context.session)
5048     return dict(fault_ref)
5049 
5050 
5051 @pick_context_manager_reader
5052 def instance_fault_get_by_instance_uuids(context, instance_uuids,
5053                                          latest=False):
5054     """Get all instance faults for the provided instance_uuids.
5055 
5056     :param instance_uuids: List of UUIDs of instances to grab faults for
5057     :param latest: Optional boolean indicating we should only return the latest
5058                    fault for the instance
5059     """
5060     if not instance_uuids:
5061         return {}
5062 
5063     faults_tbl = models.InstanceFault.__table__
5064     # NOTE(rpodolyaka): filtering by instance_uuids is performed in both
5065     # code branches below for the sake of a better query plan. On change,
5066     # make sure to update the other one as well.
5067     query = model_query(context, models.InstanceFault,
5068                         [faults_tbl],
5069                         read_deleted='no')
5070 
5071     if latest:
5072         # NOTE(jaypipes): We join instance_faults to a derived table of the
5073         # latest faults per instance UUID. The SQL produced below looks like
5074         # this:
5075         #
5076         #  SELECT instance_faults.*
5077         #  FROM instance_faults
5078         #  JOIN (
5079         #    SELECT instance_uuid, MAX(id) AS max_id
5080         #    FROM instance_faults
5081         #    WHERE instance_uuid IN ( ... )
5082         #    AND deleted = 0
5083         #    GROUP BY instance_uuid
5084         #  ) AS latest_faults
5085         #    ON instance_faults.id = latest_faults.max_id;
5086         latest_faults = model_query(
5087             context, models.InstanceFault,
5088             [faults_tbl.c.instance_uuid,
5089              sql.func.max(faults_tbl.c.id).label('max_id')],
5090             read_deleted='no'
5091         ).filter(
5092             faults_tbl.c.instance_uuid.in_(instance_uuids)
5093         ).group_by(
5094             faults_tbl.c.instance_uuid
5095         ).subquery(name="latest_faults")
5096 
5097         query = query.join(latest_faults,
5098                            faults_tbl.c.id == latest_faults.c.max_id)
5099     else:
5100         query = query.filter(models.InstanceFault.instance_uuid.in_(
5101                                         instance_uuids)).order_by(desc("id"))
5102 
5103     output = {}
5104     for instance_uuid in instance_uuids:
5105         output[instance_uuid] = []
5106 
5107     for row in query:
5108         output[row.instance_uuid].append(row._asdict())
5109 
5110     return output
5111 
5112 
5113 ##################
5114 
5115 
5116 @pick_context_manager_writer
5117 def action_start(context, values):
5118     convert_objects_related_datetimes(values, 'start_time', 'updated_at')
5119     action_ref = models.InstanceAction()
5120     action_ref.update(values)
5121     action_ref.save(context.session)
5122     return action_ref
5123 
5124 
5125 @pick_context_manager_writer
5126 def action_finish(context, values):
5127     convert_objects_related_datetimes(values, 'start_time', 'finish_time',
5128                                       'updated_at')
5129     query = model_query(context, models.InstanceAction).\
5130                         filter_by(instance_uuid=values['instance_uuid']).\
5131                         filter_by(request_id=values['request_id'])
5132     if query.update(values) != 1:
5133         raise exception.InstanceActionNotFound(
5134                                     request_id=values['request_id'],
5135                                     instance_uuid=values['instance_uuid'])
5136     return query.one()
5137 
5138 
5139 @pick_context_manager_reader
5140 def actions_get(context, instance_uuid, limit=None, marker=None,
5141                 filters=None):
5142     """Get all instance actions for the provided uuid and filters."""
5143     if limit == 0:
5144         return []
5145 
5146     sort_keys = ['created_at', 'id']
5147     sort_dirs = ['desc', 'desc']
5148 
5149     query_prefix = model_query(context, models.InstanceAction).\
5150         filter_by(instance_uuid=instance_uuid)
5151 
5152     model_object = models.InstanceAction
5153     query_prefix = _get_query_nova_resource_by_changes_time(query_prefix,
5154                                                             filters,
5155                                                             model_object)
5156 
5157     if marker is not None:
5158         marker = action_get_by_request_id(context, instance_uuid, marker)
5159         if not marker:
5160             raise exception.MarkerNotFound(marker=marker)
5161     actions = sqlalchemyutils.paginate_query(query_prefix,
5162                                              models.InstanceAction, limit,
5163                                              sort_keys, marker=marker,
5164                                              sort_dirs=sort_dirs).all()
5165     return actions
5166 
5167 
5168 @pick_context_manager_reader
5169 def action_get_by_request_id(context, instance_uuid, request_id):
5170     """Get the action by request_id and given instance."""
5171     action = _action_get_by_request_id(context, instance_uuid, request_id)
5172     return action
5173 
5174 
5175 def _action_get_by_request_id(context, instance_uuid, request_id):
5176     result = model_query(context, models.InstanceAction).\
5177                          filter_by(instance_uuid=instance_uuid).\
5178                          filter_by(request_id=request_id).\
5179                          order_by(desc("created_at"), desc("id")).\
5180                          first()
5181     return result
5182 
5183 
5184 def _action_get_last_created_by_instance_uuid(context, instance_uuid):
5185     result = (model_query(context, models.InstanceAction).
5186                      filter_by(instance_uuid=instance_uuid).
5187                      order_by(desc("created_at"), desc("id")).
5188                      first())
5189     return result
5190 
5191 
5192 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
5193 @pick_context_manager_writer
5194 def action_event_start(context, values):
5195     """Start an event on an instance action."""
5196     convert_objects_related_datetimes(values, 'start_time')
5197     action = _action_get_by_request_id(context, values['instance_uuid'],
5198                                        values['request_id'])
5199     # When nova-compute restarts, the context is generated again in
5200     # init_host workflow, the request_id was different with the request_id
5201     # recorded in InstanceAction, so we can't get the original record
5202     # according to request_id. Try to get the last created action so that
5203     # init_instance can continue to finish the recovery action, like:
5204     # powering_off, unpausing, and so on.
5205     update_action = True
5206     if not action and not context.project_id:
5207         action = _action_get_last_created_by_instance_uuid(
5208             context, values['instance_uuid'])
5209         # If we couldn't find an action by the request_id, we don't want to
5210         # update this action since it likely represents an inactive action.
5211         update_action = False
5212 
5213     if not action:
5214         raise exception.InstanceActionNotFound(
5215                                     request_id=values['request_id'],
5216                                     instance_uuid=values['instance_uuid'])
5217 
5218     values['action_id'] = action['id']
5219 
5220     event_ref = models.InstanceActionEvent()
5221     event_ref.update(values)
5222     context.session.add(event_ref)
5223 
5224     # Update action updated_at.
5225     if update_action:
5226         action.update({'updated_at': values['start_time']})
5227         action.save(context.session)
5228 
5229     return event_ref
5230 
5231 
5232 # NOTE: We need the retry_on_deadlock decorator for cases like resize where
5233 # a lot of events are happening at once between multiple hosts trying to
5234 # update the same action record in a small time window.
5235 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
5236 @pick_context_manager_writer
5237 def action_event_finish(context, values):
5238     """Finish an event on an instance action."""
5239     convert_objects_related_datetimes(values, 'start_time', 'finish_time')
5240     action = _action_get_by_request_id(context, values['instance_uuid'],
5241                                        values['request_id'])
5242     # When nova-compute restarts, the context is generated again in
5243     # init_host workflow, the request_id was different with the request_id
5244     # recorded in InstanceAction, so we can't get the original record
5245     # according to request_id. Try to get the last created action so that
5246     # init_instance can continue to finish the recovery action, like:
5247     # powering_off, unpausing, and so on.
5248     update_action = True
5249     if not action and not context.project_id:
5250         action = _action_get_last_created_by_instance_uuid(
5251             context, values['instance_uuid'])
5252         # If we couldn't find an action by the request_id, we don't want to
5253         # update this action since it likely represents an inactive action.
5254         update_action = False
5255 
5256     if not action:
5257         raise exception.InstanceActionNotFound(
5258                                     request_id=values['request_id'],
5259                                     instance_uuid=values['instance_uuid'])
5260 
5261     event_ref = model_query(context, models.InstanceActionEvent).\
5262                             filter_by(action_id=action['id']).\
5263                             filter_by(event=values['event']).\
5264                             first()
5265 
5266     if not event_ref:
5267         raise exception.InstanceActionEventNotFound(action_id=action['id'],
5268                                                     event=values['event'])
5269     event_ref.update(values)
5270 
5271     if values['result'].lower() == 'error':
5272         action.update({'message': 'Error'})
5273 
5274     # Update action updated_at.
5275     if update_action:
5276         action.update({'updated_at': values['finish_time']})
5277         action.save(context.session)
5278 
5279     return event_ref
5280 
5281 
5282 @pick_context_manager_reader
5283 def action_events_get(context, action_id):
5284     events = model_query(context, models.InstanceActionEvent).\
5285                          filter_by(action_id=action_id).\
5286                          order_by(desc("created_at"), desc("id")).\
5287                          all()
5288 
5289     return events
5290 
5291 
5292 @pick_context_manager_reader
5293 def action_event_get_by_id(context, action_id, event_id):
5294     event = model_query(context, models.InstanceActionEvent).\
5295                         filter_by(action_id=action_id).\
5296                         filter_by(id=event_id).\
5297                         first()
5298 
5299     return event
5300 
5301 
5302 ##################
5303 
5304 
5305 @require_context
5306 @pick_context_manager_writer
5307 def ec2_instance_create(context, instance_uuid, id=None):
5308     """Create ec2 compatible instance by provided uuid."""
5309     ec2_instance_ref = models.InstanceIdMapping()
5310     ec2_instance_ref.update({'uuid': instance_uuid})
5311     if id is not None:
5312         ec2_instance_ref.update({'id': id})
5313 
5314     ec2_instance_ref.save(context.session)
5315 
5316     return ec2_instance_ref
5317 
5318 
5319 @require_context
5320 @pick_context_manager_reader
5321 def ec2_instance_get_by_uuid(context, instance_uuid):
5322     result = _ec2_instance_get_query(context).\
5323                     filter_by(uuid=instance_uuid).\
5324                     first()
5325 
5326     if not result:
5327         raise exception.InstanceNotFound(instance_id=instance_uuid)
5328 
5329     return result
5330 
5331 
5332 @require_context
5333 @pick_context_manager_reader
5334 def ec2_instance_get_by_id(context, instance_id):
5335     result = _ec2_instance_get_query(context).\
5336                     filter_by(id=instance_id).\
5337                     first()
5338 
5339     if not result:
5340         raise exception.InstanceNotFound(instance_id=instance_id)
5341 
5342     return result
5343 
5344 
5345 @require_context
5346 @pick_context_manager_reader
5347 def get_instance_uuid_by_ec2_id(context, ec2_id):
5348     result = ec2_instance_get_by_id(context, ec2_id)
5349     return result['uuid']
5350 
5351 
5352 def _ec2_instance_get_query(context):
5353     return model_query(context, models.InstanceIdMapping, read_deleted='yes')
5354 
5355 
5356 ##################
5357 
5358 
5359 def _task_log_get_query(context, task_name, period_beginning,
5360                         period_ending, host=None, state=None):
5361     values = {'period_beginning': period_beginning,
5362               'period_ending': period_ending}
5363     values = convert_objects_related_datetimes(values, *values.keys())
5364 
5365     query = model_query(context, models.TaskLog).\
5366                      filter_by(task_name=task_name).\
5367                      filter_by(period_beginning=values['period_beginning']).\
5368                      filter_by(period_ending=values['period_ending'])
5369     if host is not None:
5370         query = query.filter_by(host=host)
5371     if state is not None:
5372         query = query.filter_by(state=state)
5373     return query
5374 
5375 
5376 @pick_context_manager_reader
5377 def task_log_get(context, task_name, period_beginning, period_ending, host,
5378                  state=None):
5379     return _task_log_get_query(context, task_name, period_beginning,
5380                                period_ending, host, state).first()
5381 
5382 
5383 @pick_context_manager_reader
5384 def task_log_get_all(context, task_name, period_beginning, period_ending,
5385                      host=None, state=None):
5386     return _task_log_get_query(context, task_name, period_beginning,
5387                                period_ending, host, state).all()
5388 
5389 
5390 @pick_context_manager_writer
5391 def task_log_begin_task(context, task_name, period_beginning, period_ending,
5392                         host, task_items=None, message=None):
5393     values = {'period_beginning': period_beginning,
5394               'period_ending': period_ending}
5395     values = convert_objects_related_datetimes(values, *values.keys())
5396 
5397     task = models.TaskLog()
5398     task.task_name = task_name
5399     task.period_beginning = values['period_beginning']
5400     task.period_ending = values['period_ending']
5401     task.host = host
5402     task.state = "RUNNING"
5403     if message:
5404         task.message = message
5405     if task_items:
5406         task.task_items = task_items
5407     try:
5408         task.save(context.session)
5409     except db_exc.DBDuplicateEntry:
5410         raise exception.TaskAlreadyRunning(task_name=task_name, host=host)
5411 
5412 
5413 @pick_context_manager_writer
5414 def task_log_end_task(context, task_name, period_beginning, period_ending,
5415                       host, errors, message=None):
5416     values = dict(state="DONE", errors=errors)
5417     if message:
5418         values["message"] = message
5419 
5420     rows = _task_log_get_query(context, task_name, period_beginning,
5421                                period_ending, host).update(values)
5422     if rows == 0:
5423         # It's not running!
5424         raise exception.TaskNotRunning(task_name=task_name, host=host)
5425 
5426 
5427 ##################
5428 
5429 
5430 def _archive_if_instance_deleted(table, shadow_table, instances, conn,
5431                                  max_rows):
5432     """Look for records that pertain to deleted instances, but may not be
5433     deleted themselves. This catches cases where we delete an instance,
5434     but leave some residue because of a failure in a cleanup path or
5435     similar.
5436 
5437     Logic is: if I have a column called instance_uuid, and that instance
5438     is deleted, then I can be deleted.
5439     """
5440     query_insert = shadow_table.insert(inline=True).\
5441         from_select(
5442             [c.name for c in table.c],
5443             sql.select(
5444                 [table],
5445                 and_(instances.c.deleted != instances.c.deleted.default.arg,
5446                      instances.c.uuid == table.c.instance_uuid)).
5447             order_by(table.c.id).limit(max_rows))
5448 
5449     query_delete = sql.select(
5450         [table.c.id],
5451         and_(instances.c.deleted != instances.c.deleted.default.arg,
5452              instances.c.uuid == table.c.instance_uuid)).\
5453         order_by(table.c.id).limit(max_rows)
5454     delete_statement = DeleteFromSelect(table, query_delete,
5455                                         table.c.id)
5456 
5457     try:
5458         with conn.begin():
5459             conn.execute(query_insert)
5460             result_delete = conn.execute(delete_statement)
5461             return result_delete.rowcount
5462     except db_exc.DBReferenceError as ex:
5463         LOG.warning('Failed to archive %(table)s: %(error)s',
5464                     {'table': table.name,
5465                      'error': six.text_type(ex)})
5466         return 0
5467 
5468 
5469 def _archive_deleted_rows_for_table(tablename, max_rows):
5470     """Move up to max_rows rows from one tables to the corresponding
5471     shadow table.
5472 
5473     :returns: number of rows archived
5474     """
5475     engine = get_engine()
5476     conn = engine.connect()
5477     metadata = MetaData()
5478     metadata.bind = engine
5479     # NOTE(tdurakov): table metadata should be received
5480     # from models, not db tables. Default value specified by SoftDeleteMixin
5481     # is known only by models, not DB layer.
5482     # IMPORTANT: please do not change source of metadata information for table.
5483     table = models.BASE.metadata.tables[tablename]
5484 
5485     shadow_tablename = _SHADOW_TABLE_PREFIX + tablename
5486     rows_archived = 0
5487     deleted_instance_uuids = []
5488     try:
5489         shadow_table = Table(shadow_tablename, metadata, autoload=True)
5490     except NoSuchTableError:
5491         # No corresponding shadow table; skip it.
5492         return rows_archived, deleted_instance_uuids
5493 
5494     if tablename == "dns_domains":
5495         # We have one table (dns_domains) where the key is called
5496         # "domain" rather than "id"
5497         column = table.c.domain
5498     else:
5499         column = table.c.id
5500     # NOTE(guochbo): Use DeleteFromSelect to avoid
5501     # database's limit of maximum parameter in one SQL statement.
5502     deleted_column = table.c.deleted
5503     columns = [c.name for c in table.c]
5504 
5505     # NOTE(clecomte): Tables instance_actions and instances_actions_events
5506     # have to be manage differently so we soft-delete them here to let
5507     # the archive work the same for all tables
5508     # NOTE(takashin): The record in table migrations should be
5509     # soft deleted when the instance is deleted.
5510     # This is just for upgrading.
5511     if tablename in ("instance_actions", "migrations"):
5512         instances = models.BASE.metadata.tables["instances"]
5513         deleted_instances = sql.select([instances.c.uuid]).\
5514             where(instances.c.deleted != instances.c.deleted.default.arg)
5515         update_statement = table.update().values(deleted=table.c.id).\
5516             where(table.c.instance_uuid.in_(deleted_instances))
5517 
5518         conn.execute(update_statement)
5519 
5520     elif tablename == "instance_actions_events":
5521         # NOTE(clecomte): we have to grab all the relation from
5522         # instances because instance_actions_events rely on
5523         # action_id and not uuid
5524         instances = models.BASE.metadata.tables["instances"]
5525         instance_actions = models.BASE.metadata.tables["instance_actions"]
5526         deleted_instances = sql.select([instances.c.uuid]).\
5527             where(instances.c.deleted != instances.c.deleted.default.arg)
5528         deleted_actions = sql.select([instance_actions.c.id]).\
5529             where(instance_actions.c.instance_uuid.in_(deleted_instances))
5530 
5531         update_statement = table.update().values(deleted=table.c.id).\
5532             where(table.c.action_id.in_(deleted_actions))
5533 
5534         conn.execute(update_statement)
5535 
5536     select = sql.select([column],
5537                         deleted_column != deleted_column.default.arg).\
5538                         order_by(column).limit(max_rows)
5539     rows = conn.execute(select).fetchall()
5540     records = [r[0] for r in rows]
5541 
5542     if records:
5543         insert = shadow_table.insert(inline=True).\
5544                 from_select(columns, sql.select([table], column.in_(records)))
5545         delete = table.delete().where(column.in_(records))
5546         # NOTE(tssurya): In order to facilitate the deletion of records from
5547         # instance_mappings, request_specs and instance_group_member tables in
5548         # the nova_api DB, the rows of deleted instances from the instances
5549         # table are stored prior to their deletion. Basically the uuids of the
5550         # archived instances are queried and returned.
5551         if tablename == "instances":
5552             query_select = sql.select([table.c.uuid], table.c.id.in_(records))
5553             rows = conn.execute(query_select).fetchall()
5554             deleted_instance_uuids = [r[0] for r in rows]
5555 
5556         try:
5557             # Group the insert and delete in a transaction.
5558             with conn.begin():
5559                 conn.execute(insert)
5560                 result_delete = conn.execute(delete)
5561             rows_archived = result_delete.rowcount
5562         except db_exc.DBReferenceError as ex:
5563             # A foreign key constraint keeps us from deleting some of
5564             # these rows until we clean up a dependent table.  Just
5565             # skip this table for now; we'll come back to it later.
5566             LOG.warning("IntegrityError detected when archiving table "
5567                         "%(tablename)s: %(error)s",
5568                         {'tablename': tablename, 'error': six.text_type(ex)})
5569 
5570     if ((max_rows is None or rows_archived < max_rows)
5571             and 'instance_uuid' in columns):
5572         instances = models.BASE.metadata.tables['instances']
5573         limit = max_rows - rows_archived if max_rows is not None else None
5574         extra = _archive_if_instance_deleted(table, shadow_table, instances,
5575                                              conn, limit)
5576         rows_archived += extra
5577 
5578     return rows_archived, deleted_instance_uuids
5579 
5580 
5581 def archive_deleted_rows(max_rows=None):
5582     """Move up to max_rows rows from production tables to the corresponding
5583     shadow tables.
5584 
5585     :returns: dict that maps table name to number of rows archived from that
5586               table, for example:
5587 
5588     ::
5589 
5590         {
5591             'instances': 5,
5592             'block_device_mapping': 5,
5593             'pci_devices': 2,
5594         }
5595 
5596     """
5597     table_to_rows_archived = {}
5598     deleted_instance_uuids = []
5599     total_rows_archived = 0
5600     meta = MetaData(get_engine(use_slave=True))
5601     meta.reflect()
5602     # Reverse sort the tables so we get the leaf nodes first for processing.
5603     for table in reversed(meta.sorted_tables):
5604         tablename = table.name
5605         rows_archived = 0
5606         # skip the special sqlalchemy-migrate migrate_version table and any
5607         # shadow tables
5608         if (tablename == 'migrate_version' or
5609                 tablename.startswith(_SHADOW_TABLE_PREFIX)):
5610             continue
5611         rows_archived,\
5612         deleted_instance_uuid = _archive_deleted_rows_for_table(
5613                 tablename, max_rows=max_rows - total_rows_archived)
5614         total_rows_archived += rows_archived
5615         if tablename == 'instances':
5616             deleted_instance_uuids = deleted_instance_uuid
5617         # Only report results for tables that had updates.
5618         if rows_archived:
5619             table_to_rows_archived[tablename] = rows_archived
5620         if total_rows_archived >= max_rows:
5621             break
5622     return table_to_rows_archived, deleted_instance_uuids
5623 
5624 
5625 def _purgeable_tables(metadata):
5626     return [t for t in metadata.sorted_tables
5627             if (t.name.startswith(_SHADOW_TABLE_PREFIX) and not
5628                 t.name.endswith('migrate_version'))]
5629 
5630 
5631 def purge_shadow_tables(context, before_date, status_fn=None):
5632     engine = get_engine(context=context)
5633     conn = engine.connect()
5634     metadata = MetaData()
5635     metadata.bind = engine
5636     metadata.reflect()
5637     total_deleted = 0
5638 
5639     if status_fn is None:
5640         status_fn = lambda m: None
5641 
5642     # Some things never get formally deleted, and thus deleted_at
5643     # is never set. So, prefer specific timestamp columns here
5644     # for those special cases.
5645     overrides = {
5646         'shadow_instance_actions': 'created_at',
5647         'shadow_instance_actions_events': 'created_at',
5648     }
5649 
5650     for table in _purgeable_tables(metadata):
5651         if before_date is None:
5652             col = None
5653         elif table.name in overrides:
5654             col = getattr(table.c, overrides[table.name])
5655         elif hasattr(table.c, 'deleted_at'):
5656             col = table.c.deleted_at
5657         elif hasattr(table.c, 'updated_at'):
5658             col = table.c.updated_at
5659         elif hasattr(table.c, 'created_at'):
5660             col = table.c.created_at
5661         else:
5662             status_fn(_('Unable to purge table %(table)s because it '
5663                         'has no timestamp column') % {
5664                             'table': table.name})
5665             continue
5666 
5667         if col is not None:
5668             delete = table.delete().where(col < before_date)
5669         else:
5670             delete = table.delete()
5671 
5672         deleted = conn.execute(delete)
5673         if deleted.rowcount > 0:
5674             status_fn(_('Deleted %(rows)i rows from %(table)s based on '
5675                         'timestamp column %(col)s') % {
5676                             'rows': deleted.rowcount,
5677                             'table': table.name,
5678                             'col': col is None and '(n/a)' or col.name})
5679         total_deleted += deleted.rowcount
5680 
5681     return total_deleted
5682 
5683 
5684 @pick_context_manager_writer
5685 def service_uuids_online_data_migration(context, max_count):
5686     from nova.objects import service
5687 
5688     count_all = 0
5689     count_hit = 0
5690 
5691     db_services = model_query(context, models.Service).filter_by(
5692         uuid=None).limit(max_count)
5693     for db_service in db_services:
5694         count_all += 1
5695         service_obj = service.Service._from_db_object(
5696             context, service.Service(), db_service)
5697         if 'uuid' in service_obj:
5698             count_hit += 1
5699     return count_all, count_hit
5700 
5701 
5702 ####################
5703 
5704 
5705 @pick_context_manager_reader
5706 def pci_device_get_by_addr(context, node_id, dev_addr):
5707     pci_dev_ref = model_query(context, models.PciDevice).\
5708                         filter_by(compute_node_id=node_id).\
5709                         filter_by(address=dev_addr).\
5710                         first()
5711     if not pci_dev_ref:
5712         raise exception.PciDeviceNotFound(node_id=node_id, address=dev_addr)
5713     return pci_dev_ref
5714 
5715 
5716 @pick_context_manager_reader
5717 def pci_device_get_by_id(context, id):
5718     pci_dev_ref = model_query(context, models.PciDevice).\
5719                         filter_by(id=id).\
5720                         first()
5721     if not pci_dev_ref:
5722         raise exception.PciDeviceNotFoundById(id=id)
5723     return pci_dev_ref
5724 
5725 
5726 @pick_context_manager_reader
5727 def pci_device_get_all_by_node(context, node_id):
5728     return model_query(context, models.PciDevice).\
5729                        filter_by(compute_node_id=node_id).\
5730                        all()
5731 
5732 
5733 @pick_context_manager_reader
5734 def pci_device_get_all_by_parent_addr(context, node_id, parent_addr):
5735     return model_query(context, models.PciDevice).\
5736                        filter_by(compute_node_id=node_id).\
5737                        filter_by(parent_addr=parent_addr).\
5738                        all()
5739 
5740 
5741 @require_context
5742 @pick_context_manager_reader
5743 def pci_device_get_all_by_instance_uuid(context, instance_uuid):
5744     return model_query(context, models.PciDevice).\
5745                        filter_by(status='allocated').\
5746                        filter_by(instance_uuid=instance_uuid).\
5747                        all()
5748 
5749 
5750 @pick_context_manager_reader
5751 def _instance_pcidevs_get_multi(context, instance_uuids):
5752     if not instance_uuids:
5753         return []
5754     return model_query(context, models.PciDevice).\
5755         filter_by(status='allocated').\
5756         filter(models.PciDevice.instance_uuid.in_(instance_uuids))
5757 
5758 
5759 @pick_context_manager_writer
5760 def pci_device_destroy(context, node_id, address):
5761     result = model_query(context, models.PciDevice).\
5762                          filter_by(compute_node_id=node_id).\
5763                          filter_by(address=address).\
5764                          soft_delete()
5765     if not result:
5766         raise exception.PciDeviceNotFound(node_id=node_id, address=address)
5767 
5768 
5769 @pick_context_manager_writer
5770 def pci_device_update(context, node_id, address, values):
5771     query = model_query(context, models.PciDevice, read_deleted="no").\
5772                     filter_by(compute_node_id=node_id).\
5773                     filter_by(address=address)
5774     if query.update(values) == 0:
5775         device = models.PciDevice()
5776         device.update(values)
5777         context.session.add(device)
5778     return query.one()
5779 
5780 
5781 ####################
5782 
5783 
5784 @pick_context_manager_writer
5785 def instance_tag_add(context, instance_uuid, tag):
5786     tag_ref = models.Tag()
5787     tag_ref.resource_id = instance_uuid
5788     tag_ref.tag = tag
5789 
5790     try:
5791         _check_instance_exists_in_project(context, instance_uuid)
5792         with get_context_manager(context).writer.savepoint.using(context):
5793             context.session.add(tag_ref)
5794     except db_exc.DBDuplicateEntry:
5795         # NOTE(snikitin): We should ignore tags duplicates
5796         pass
5797 
5798     return tag_ref
5799 
5800 
5801 @pick_context_manager_writer
5802 def instance_tag_set(context, instance_uuid, tags):
5803     _check_instance_exists_in_project(context, instance_uuid)
5804 
5805     existing = context.session.query(models.Tag.tag).filter_by(
5806         resource_id=instance_uuid).all()
5807 
5808     existing = set(row.tag for row in existing)
5809     tags = set(tags)
5810     to_delete = existing - tags
5811     to_add = tags - existing
5812 
5813     if to_delete:
5814         context.session.query(models.Tag).filter_by(
5815             resource_id=instance_uuid).filter(
5816             models.Tag.tag.in_(to_delete)).delete(
5817             synchronize_session=False)
5818 
5819     if to_add:
5820         data = [
5821             {'resource_id': instance_uuid, 'tag': tag} for tag in to_add]
5822         context.session.execute(models.Tag.__table__.insert(), data)
5823 
5824     return context.session.query(models.Tag).filter_by(
5825         resource_id=instance_uuid).all()
5826 
5827 
5828 @pick_context_manager_reader
5829 def instance_tag_get_by_instance_uuid(context, instance_uuid):
5830     _check_instance_exists_in_project(context, instance_uuid)
5831     return context.session.query(models.Tag).filter_by(
5832         resource_id=instance_uuid).all()
5833 
5834 
5835 @pick_context_manager_writer
5836 def instance_tag_delete(context, instance_uuid, tag):
5837     _check_instance_exists_in_project(context, instance_uuid)
5838     result = context.session.query(models.Tag).filter_by(
5839         resource_id=instance_uuid, tag=tag).delete()
5840 
5841     if not result:
5842         raise exception.InstanceTagNotFound(instance_id=instance_uuid,
5843                                             tag=tag)
5844 
5845 
5846 @pick_context_manager_writer
5847 def instance_tag_delete_all(context, instance_uuid):
5848     _check_instance_exists_in_project(context, instance_uuid)
5849     context.session.query(models.Tag).filter_by(
5850         resource_id=instance_uuid).delete()
5851 
5852 
5853 @pick_context_manager_reader
5854 def instance_tag_exists(context, instance_uuid, tag):
5855     _check_instance_exists_in_project(context, instance_uuid)
5856     q = context.session.query(models.Tag).filter_by(
5857         resource_id=instance_uuid, tag=tag)
5858     return context.session.query(q.exists()).scalar()
5859 
5860 
5861 ####################
5862 
5863 
5864 @pick_context_manager_writer
5865 def console_auth_token_create(context, values):
5866     instance_uuid = values.get('instance_uuid')
5867     _check_instance_exists_in_project(context, instance_uuid)
5868     token_ref = models.ConsoleAuthToken()
5869     token_ref.update(values)
5870     context.session.add(token_ref)
5871     return token_ref
5872 
5873 
5874 @pick_context_manager_reader
5875 def console_auth_token_get_valid(context, token_hash, instance_uuid=None):
5876     if instance_uuid is not None:
5877         _check_instance_exists_in_project(context, instance_uuid)
5878     query = context.session.query(models.ConsoleAuthToken).\
5879         filter_by(token_hash=token_hash)
5880     if instance_uuid is not None:
5881         query = query.filter_by(instance_uuid=instance_uuid)
5882     return query.filter(
5883         models.ConsoleAuthToken.expires > timeutils.utcnow_ts()).first()
5884 
5885 
5886 @pick_context_manager_writer
5887 def console_auth_token_destroy_all_by_instance(context, instance_uuid):
5888     context.session.query(models.ConsoleAuthToken).\
5889         filter_by(instance_uuid=instance_uuid).delete()
5890 
5891 
5892 @pick_context_manager_writer
5893 def console_auth_token_destroy_expired_by_host(context, host):
5894     context.session.query(models.ConsoleAuthToken).\
5895         filter_by(host=host).\
5896         filter(models.ConsoleAuthToken.expires <= timeutils.utcnow_ts()).\
5897         delete()
