I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import binascii
29 import collections
30 from collections import deque
31 import contextlib
32 import errno
33 import functools
34 import glob
35 import itertools
36 import operator
37 import os
38 import pwd
39 import random
40 import shutil
41 import tempfile
42 import time
43 import uuid
44 
45 from castellan import key_manager
46 import eventlet
47 from eventlet import greenthread
48 from eventlet import tpool
49 from lxml import etree
50 from os_brick import encryptors
51 from os_brick.encryptors import luks as luks_encryptor
52 from os_brick import exception as brick_exception
53 from os_brick.initiator import connector
54 from oslo_concurrency import processutils
55 from oslo_log import log as logging
56 from oslo_serialization import base64
57 from oslo_serialization import jsonutils
58 from oslo_service import loopingcall
59 from oslo_utils import encodeutils
60 from oslo_utils import excutils
61 from oslo_utils import fileutils
62 from oslo_utils import importutils
63 from oslo_utils import strutils
64 from oslo_utils import timeutils
65 from oslo_utils import units
66 from oslo_utils import uuidutils
67 import six
68 from six.moves import range
69 
70 from nova.api.metadata import base as instance_metadata
71 from nova.api.metadata import password
72 from nova import block_device
73 from nova.compute import power_state
74 from nova.compute import task_states
75 from nova.compute import utils as compute_utils
76 import nova.conf
77 from nova.console import serial as serial_console
78 from nova.console import type as ctype
79 from nova import context as nova_context
80 from nova import crypto
81 from nova import exception
82 from nova.i18n import _
83 from nova import image
84 from nova.network import model as network_model
85 from nova import objects
86 from nova.objects import diagnostics as diagnostics_obj
87 from nova.objects import fields
88 from nova.objects import migrate_data as migrate_data_obj
89 from nova.pci import manager as pci_manager
90 from nova.pci import utils as pci_utils
91 import nova.privsep.libvirt
92 import nova.privsep.path
93 from nova import rc_fields
94 from nova import utils
95 from nova import version
96 from nova.virt import block_device as driver_block_device
97 from nova.virt import configdrive
98 from nova.virt.disk import api as disk_api
99 from nova.virt.disk.vfs import guestfs
100 from nova.virt import driver
101 from nova.virt import firewall
102 from nova.virt import hardware
103 from nova.virt.image import model as imgmodel
104 from nova.virt import images
105 from nova.virt.libvirt import blockinfo
106 from nova.virt.libvirt import config as vconfig
107 from nova.virt.libvirt import firewall as libvirt_firewall
108 from nova.virt.libvirt import guest as libvirt_guest
109 from nova.virt.libvirt import host
110 from nova.virt.libvirt import imagebackend
111 from nova.virt.libvirt import imagecache
112 from nova.virt.libvirt import instancejobtracker
113 from nova.virt.libvirt import migration as libvirt_migrate
114 from nova.virt.libvirt.storage import dmcrypt
115 from nova.virt.libvirt.storage import lvm
116 from nova.virt.libvirt.storage import rbd_utils
117 from nova.virt.libvirt import utils as libvirt_utils
118 from nova.virt.libvirt import vif as libvirt_vif
119 from nova.virt.libvirt.volume import mount
120 from nova.virt.libvirt.volume import remotefs
121 from nova.virt import netutils
122 from nova.volume import cinder
123 
124 libvirt = None
125 
126 uefi_logged = False
127 
128 LOG = logging.getLogger(__name__)
129 
130 CONF = nova.conf.CONF
131 
132 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
133     libvirt_firewall.__name__,
134     libvirt_firewall.IptablesFirewallDriver.__name__)
135 
136 DEFAULT_UEFI_LOADER_PATH = {
137     "x86_64": "/usr/share/OVMF/OVMF_CODE.fd",
138     "aarch64": "/usr/share/AAVMF/AAVMF_CODE.fd"
139 }
140 
141 MAX_CONSOLE_BYTES = 100 * units.Ki
142 
143 # The libvirt driver will prefix any disable reason codes with this string.
144 DISABLE_PREFIX = 'AUTO: '
145 # Disable reason for the service which was enabled or disabled without reason
146 DISABLE_REASON_UNDEFINED = None
147 
148 # Guest config console string
149 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
150 
151 GuestNumaConfig = collections.namedtuple(
152     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
153 
154 
155 class InjectionInfo(collections.namedtuple(
156         'InjectionInfo', ['network_info', 'files', 'admin_pass'])):
157     __slots__ = ()
158 
159     def __repr__(self):
160         return ('InjectionInfo(network_info=%r, files=%r, '
161                 'admin_pass=<SANITIZED>)') % (self.network_info, self.files)
162 
163 libvirt_volume_drivers = [
164     'iscsi=nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
165     'iser=nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
166     'local=nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
167     'drbd=nova.virt.libvirt.volume.drbd.LibvirtDRBDVolumeDriver',
168     'fake=nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
169     'rbd=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
170     'sheepdog=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
171     'nfs=nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
172     'smbfs=nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
173     'aoe=nova.virt.libvirt.volume.aoe.LibvirtAOEVolumeDriver',
174     'fibre_channel='
175         'nova.virt.libvirt.volume.fibrechannel.'
176         'LibvirtFibreChannelVolumeDriver',
177     'gpfs=nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
178     'quobyte=nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
179     'hgst=nova.virt.libvirt.volume.hgst.LibvirtHGSTVolumeDriver',
180     'scaleio=nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
181     'disco=nova.virt.libvirt.volume.disco.LibvirtDISCOVolumeDriver',
182     'vzstorage='
183         'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',
184     'veritas_hyperscale='
185         'nova.virt.libvirt.volume.vrtshyperscale.'
186         'LibvirtHyperScaleVolumeDriver',
187     'storpool=nova.virt.libvirt.volume.storpool.LibvirtStorPoolVolumeDriver',
188 ]
189 
190 
191 def patch_tpool_proxy():
192     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
193     or __repr__() calls. See bug #962840 for details.
194     We perform a monkey patch to replace those two instance methods.
195     """
196     def str_method(self):
197         return str(self._obj)
198 
199     def repr_method(self):
200         return repr(self._obj)
201 
202     tpool.Proxy.__str__ = str_method
203     tpool.Proxy.__repr__ = repr_method
204 
205 
206 patch_tpool_proxy()
207 
208 # For information about when MIN_LIBVIRT_VERSION and
209 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
210 #
211 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
212 #
213 # Currently this is effectively the min version for i686/x86_64
214 # + KVM/QEMU, as other architectures/hypervisors require newer
215 # versions. Over time, this will become a common min version
216 # for all architectures/hypervisors, as this value rises to
217 # meet them.
218 MIN_LIBVIRT_VERSION = (1, 2, 9)
219 MIN_QEMU_VERSION = (2, 1, 0)
220 # TODO(berrange): Re-evaluate this at start of each release cycle
221 # to decide if we want to plan a future min version bump.
222 # MIN_LIBVIRT_VERSION can be updated to match this after
223 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
224 # one cycle
225 NEXT_MIN_LIBVIRT_VERSION = (1, 3, 1)
226 NEXT_MIN_QEMU_VERSION = (2, 5, 0)
227 
228 # When the above version matches/exceeds this version
229 # delete it & corresponding code using it
230 # Libvirt version 1.2.17 is required for successful block live migration
231 # of vm booted from image with attached devices
232 MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION = (1, 2, 17)
233 # PowerPC based hosts that support NUMA using libvirt
234 MIN_LIBVIRT_NUMA_VERSION_PPC = (1, 2, 19)
235 # Versions of libvirt with known NUMA topology issues
236 # See bug #1449028
237 BAD_LIBVIRT_NUMA_VERSIONS = [(1, 2, 9, 2)]
238 # Versions of libvirt with broken cpu pinning support. This excludes
239 # versions of libvirt with broken NUMA support since pinning needs
240 # NUMA
241 # See bug #1438226
242 BAD_LIBVIRT_CPU_POLICY_VERSIONS = [(1, 2, 10)]
243 
244 # Virtuozzo driver support
245 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
246 MIN_LIBVIRT_VIRTUOZZO_VERSION = (1, 2, 12)
247 
248 # Ability to set the user guest password with Qemu
249 MIN_LIBVIRT_SET_ADMIN_PASSWD = (1, 2, 16)
250 
251 # Ability to set the user guest password with parallels
252 MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD = (2, 0, 0)
253 
254 # s/390 & s/390x architectures with KVM
255 MIN_LIBVIRT_KVM_S390_VERSION = (1, 2, 13)
256 MIN_QEMU_S390_VERSION = (2, 3, 0)
257 
258 # libvirt < 1.3 reported virt_functions capability
259 # only when VFs are enabled.
260 # libvirt 1.3 fix f391889f4e942e22b9ef8ecca492de05106ce41e
261 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION = (1, 3, 0)
262 
263 # Use the "logd" backend for handling stdout/stderr from QEMU processes.
264 MIN_LIBVIRT_VIRTLOGD = (1, 3, 3)
265 MIN_QEMU_VIRTLOGD = (2, 7, 0)
266 
267 # ppc64/ppc64le architectures with KVM
268 # NOTE(rfolco): Same levels for Libvirt/Qemu on Big Endian and Little
269 # Endian giving the nuance around guest vs host architectures
270 MIN_LIBVIRT_KVM_PPC64_VERSION = (1, 2, 12)
271 
272 # aarch64 architecture with KVM
273 # 'chardev' support got sorted out in 3.6.0
274 MIN_LIBVIRT_KVM_AARCH64_VERSION = (3, 6, 0)
275 
276 # Names of the types that do not get compressed during migration
277 NO_COMPRESSION_TYPES = ('qcow2',)
278 
279 
280 # number of serial console limit
281 QEMU_MAX_SERIAL_PORTS = 4
282 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
283 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
284 
285 # realtime support
286 MIN_LIBVIRT_REALTIME_VERSION = (1, 2, 13)
287 
288 # libvirt postcopy support
289 MIN_LIBVIRT_POSTCOPY_VERSION = (1, 3, 3)
290 
291 # qemu postcopy support
292 MIN_QEMU_POSTCOPY_VERSION = (2, 5, 0)
293 
294 MIN_LIBVIRT_OTHER_ARCH = {
295     fields.Architecture.S390: MIN_LIBVIRT_KVM_S390_VERSION,
296     fields.Architecture.S390X: MIN_LIBVIRT_KVM_S390_VERSION,
297     fields.Architecture.PPC: MIN_LIBVIRT_KVM_PPC64_VERSION,
298     fields.Architecture.PPC64: MIN_LIBVIRT_KVM_PPC64_VERSION,
299     fields.Architecture.PPC64LE: MIN_LIBVIRT_KVM_PPC64_VERSION,
300     fields.Architecture.AARCH64: MIN_LIBVIRT_KVM_AARCH64_VERSION,
301 }
302 
303 MIN_QEMU_OTHER_ARCH = {
304     fields.Architecture.S390: MIN_QEMU_S390_VERSION,
305     fields.Architecture.S390X: MIN_QEMU_S390_VERSION,
306 }
307 
308 # perf events support
309 MIN_LIBVIRT_PERF_VERSION = (2, 0, 0)
310 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
311 
312 PERF_EVENTS_CPU_FLAG_MAPPING = {'cmt': 'cmt',
313                                 'mbml': 'mbm_local',
314                                 'mbmt': 'mbm_total',
315                                }
316 
317 # Mediated devices support
318 MIN_LIBVIRT_MDEV_SUPPORT = (3, 4, 0)
319 
320 # libvirt>=3.10 is required for volume multiattach if qemu<2.10.
321 # See https://bugzilla.redhat.com/show_bug.cgi?id=1378242
322 # for details.
323 MIN_LIBVIRT_MULTIATTACH = (3, 10, 0)
324 
325 MIN_LIBVIRT_LUKS_VERSION = (2, 2, 0)
326 MIN_QEMU_LUKS_VERSION = (2, 6, 0)
327 
328 
329 VGPU_RESOURCE_SEMAPHORE = "vgpu_resources"
330 
331 
332 class LibvirtDriver(driver.ComputeDriver):
333     capabilities = {
334         "has_imagecache": True,
335         "supports_recreate": True,
336         "supports_migrate_to_same_host": False,
337         "supports_attach_interface": True,
338         "supports_device_tagging": True,
339         "supports_tagged_attach_interface": True,
340         "supports_tagged_attach_volume": True,
341         "supports_extend_volume": True,
342         # Multiattach support is conditional on qemu and libvirt versions
343         # determined in init_host.
344         "supports_multiattach": False
345     }
346 
347     def __init__(self, virtapi, read_only=False):
348         super(LibvirtDriver, self).__init__(virtapi)
349 
350         global libvirt
351         if libvirt is None:
352             libvirt = importutils.import_module('libvirt')
353             libvirt_migrate.libvirt = libvirt
354 
355         self._host = host.Host(self._uri(), read_only,
356                                lifecycle_event_handler=self.emit_event,
357                                conn_event_handler=self._handle_conn_event)
358         self._initiator = None
359         self._fc_wwnns = None
360         self._fc_wwpns = None
361         self._caps = None
362         self._supported_perf_events = []
363         self.firewall_driver = firewall.load_driver(
364             DEFAULT_FIREWALL_DRIVER,
365             host=self._host)
366 
367         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
368 
369         # TODO(mriedem): Long-term we should load up the volume drivers on
370         # demand as needed rather than doing this on startup, as there might
371         # be unsupported volume drivers in this list based on the underlying
372         # platform.
373         self.volume_drivers = self._get_volume_drivers()
374 
375         self._disk_cachemode = None
376         self.image_cache_manager = imagecache.ImageCacheManager()
377         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
378 
379         self.disk_cachemodes = {}
380 
381         self.valid_cachemodes = ["default",
382                                  "none",
383                                  "writethrough",
384                                  "writeback",
385                                  "directsync",
386                                  "unsafe",
387                                 ]
388         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
389                                                                       'qemu')
390 
391         for mode_str in CONF.libvirt.disk_cachemodes:
392             disk_type, sep, cache_mode = mode_str.partition('=')
393             if cache_mode not in self.valid_cachemodes:
394                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
395                             'for disk type %(disk_type)s.',
396                             {'cache_mode': cache_mode, 'disk_type': disk_type})
397                 continue
398             self.disk_cachemodes[disk_type] = cache_mode
399 
400         self._volume_api = cinder.API()
401         self._image_api = image.API()
402 
403         sysinfo_serial_funcs = {
404             'none': lambda: None,
405             'hardware': self._get_host_sysinfo_serial_hardware,
406             'os': self._get_host_sysinfo_serial_os,
407             'auto': self._get_host_sysinfo_serial_auto,
408         }
409 
410         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
411             CONF.libvirt.sysinfo_serial)
412 
413         self.job_tracker = instancejobtracker.InstanceJobTracker()
414         self._remotefs = remotefs.RemoteFilesystem()
415 
416         self._live_migration_flags = self._block_migration_flags = 0
417         self.active_migrations = {}
418 
419         # Compute reserved hugepages from conf file at the very
420         # beginning to ensure any syntax error will be reported and
421         # avoid any re-calculation when computing resources.
422         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
423 
424     def _get_volume_drivers(self):
425         driver_registry = dict()
426 
427         for driver_str in libvirt_volume_drivers:
428             driver_type, _sep, driver = driver_str.partition('=')
429             driver_class = importutils.import_class(driver)
430             try:
431                 driver_registry[driver_type] = driver_class(self._host)
432             except brick_exception.InvalidConnectorProtocol:
433                 LOG.debug('Unable to load volume driver %s. It is not '
434                           'supported on this host.', driver)
435 
436         return driver_registry
437 
438     @property
439     def disk_cachemode(self):
440         if self._disk_cachemode is None:
441             # We prefer 'none' for consistent performance, host crash
442             # safety & migration correctness by avoiding host page cache.
443             # Some filesystems don't support O_DIRECT though. For those we
444             # fallback to 'writethrough' which gives host crash safety, and
445             # is safe for migration provided the filesystem is cache coherent
446             # (cluster filesystems typically are, but things like NFS are not).
447             self._disk_cachemode = "none"
448             if not utils.supports_direct_io(CONF.instances_path):
449                 self._disk_cachemode = "writethrough"
450         return self._disk_cachemode
451 
452     def _set_cache_mode(self, conf):
453         """Set cache mode on LibvirtConfigGuestDisk object."""
454         try:
455             source_type = conf.source_type
456             driver_cache = conf.driver_cache
457         except AttributeError:
458             return
459 
460         # Shareable disks like for a multi-attach volume need to have the
461         # driver cache disabled.
462         if getattr(conf, 'shareable', False):
463             conf.driver_cache = 'none'
464         else:
465             cache_mode = self.disk_cachemodes.get(source_type,
466                                                   driver_cache)
467             conf.driver_cache = cache_mode
468 
469     def _do_quality_warnings(self):
470         """Warn about potential configuration issues.
471 
472         This will log a warning message for things such as untested driver or
473         host arch configurations in order to indicate potential issues to
474         administrators.
475         """
476         caps = self._host.get_capabilities()
477         hostarch = caps.host.cpu.arch
478         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
479             hostarch not in (fields.Architecture.I686,
480                              fields.Architecture.X86_64)):
481             LOG.warning('The libvirt driver is not tested on '
482                         '%(type)s/%(arch)s by the OpenStack project and '
483                         'thus its quality can not be ensured. For more '
484                         'information, see: https://docs.openstack.org/'
485                         'nova/latest/user/support-matrix.html',
486                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
487 
488         if CONF.vnc.keymap:
489             LOG.warning('The option "[vnc] keymap" has been deprecated '
490                         'in favor of configuration within the guest. '
491                         'Update nova.conf to address this change and '
492                         'refer to bug #1682020 for more information.')
493 
494         if CONF.spice.keymap:
495             LOG.warning('The option "[spice] keymap" has been deprecated '
496                         'in favor of configuration within the guest. '
497                         'Update nova.conf to address this change and '
498                         'refer to bug #1682020 for more information.')
499 
500     def _handle_conn_event(self, enabled, reason):
501         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
502                  {'enabled': enabled, 'reason': reason})
503         self._set_host_enabled(enabled, reason)
504 
505     def _version_to_string(self, version):
506         return '.'.join([str(x) for x in version])
507 
508     def init_host(self, host):
509         self._host.initialize()
510 
511         self._do_quality_warnings()
512 
513         self._parse_migration_flags()
514 
515         self._supported_perf_events = self._get_supported_perf_events()
516 
517         self._set_multiattach_support()
518 
519         if (CONF.libvirt.virt_type == 'lxc' and
520                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
521             LOG.warning("Running libvirt-lxc without user namespaces is "
522                         "dangerous. Containers spawned by Nova will be run "
523                         "as the host's root user. It is highly suggested "
524                         "that user namespaces be used in a public or "
525                         "multi-tenant environment.")
526 
527         # Stop libguestfs using KVM unless we're also configured
528         # to use this. This solves problem where people need to
529         # stop Nova use of KVM because nested-virt is broken
530         if CONF.libvirt.virt_type != "kvm":
531             guestfs.force_tcg()
532 
533         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
534             raise exception.InternalError(
535                 _('Nova requires libvirt version %s or greater.') %
536                 self._version_to_string(MIN_LIBVIRT_VERSION))
537 
538         if CONF.libvirt.virt_type in ("qemu", "kvm"):
539             if self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
540                 # "qemu-img info" calls are version dependent, so we need to
541                 # store the version in the images module.
542                 images.QEMU_VERSION = self._host.get_connection().getVersion()
543             else:
544                 raise exception.InternalError(
545                     _('Nova requires QEMU version %s or greater.') %
546                     self._version_to_string(MIN_QEMU_VERSION))
547 
548         if CONF.libvirt.virt_type == 'parallels':
549             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
550                 raise exception.InternalError(
551                     _('Nova requires Virtuozzo version %s or greater.') %
552                     self._version_to_string(MIN_VIRTUOZZO_VERSION))
553             if not self._host.has_min_version(MIN_LIBVIRT_VIRTUOZZO_VERSION):
554                 raise exception.InternalError(
555                     _('Running Nova with parallels virt_type requires '
556                       'libvirt version %s') %
557                     self._version_to_string(MIN_LIBVIRT_VIRTUOZZO_VERSION))
558 
559         # Give the cloud admin a heads up if we are intending to
560         # change the MIN_LIBVIRT_VERSION in the next release.
561         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
562             LOG.warning('Running Nova with a libvirt version less than '
563                         '%(version)s is deprecated. The required minimum '
564                         'version of libvirt will be raised to %(version)s '
565                         'in the next release.',
566                         {'version': self._version_to_string(
567                             NEXT_MIN_LIBVIRT_VERSION)})
568         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
569             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
570             LOG.warning('Running Nova with a QEMU version less than '
571                         '%(version)s is deprecated. The required minimum '
572                         'version of QEMU will be raised to %(version)s '
573                         'in the next release.',
574                         {'version': self._version_to_string(
575                             NEXT_MIN_QEMU_VERSION)})
576 
577         kvm_arch = fields.Architecture.from_host()
578         if (CONF.libvirt.virt_type in ('kvm', 'qemu') and
579             kvm_arch in MIN_LIBVIRT_OTHER_ARCH and
580                 not self._host.has_min_version(
581                                         MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch),
582                                         MIN_QEMU_OTHER_ARCH.get(kvm_arch))):
583             if MIN_QEMU_OTHER_ARCH.get(kvm_arch):
584                 raise exception.InternalError(
585                     _('Running Nova with qemu/kvm virt_type on %(arch)s '
586                       'requires libvirt version %(libvirt_ver)s and '
587                       'qemu version %(qemu_ver)s, or greater') %
588                     {'arch': kvm_arch,
589                      'libvirt_ver': self._version_to_string(
590                          MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch)),
591                      'qemu_ver': self._version_to_string(
592                          MIN_QEMU_OTHER_ARCH.get(kvm_arch))})
593             # no qemu version in the error message
594             raise exception.InternalError(
595                 _('Running Nova with qemu/kvm virt_type on %(arch)s '
596                   'requires libvirt version %(libvirt_ver)s or greater') %
597                 {'arch': kvm_arch,
598                  'libvirt_ver': self._version_to_string(
599                      MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))})
600 
601         # TODO(sbauza): Remove this code once mediated devices are persisted
602         # across reboots.
603         if self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
604             self._recreate_assigned_mediated_devices()
605 
606     @staticmethod
607     def _is_existing_mdev(uuid):
608         # FIXME(sbauza): Some kernel can have a uevent race meaning that the
609         # libvirt daemon won't know when a mediated device is created unless
610         # you restart that daemon. Until all kernels we support are not having
611         # that possible race, check the sysfs directly instead of asking the
612         # libvirt API.
613         # See https://bugzilla.redhat.com/show_bug.cgi?id=1376907 for ref.
614         return os.path.exists('/sys/bus/mdev/devices/{0}'.format(uuid))
615 
616     def _recreate_assigned_mediated_devices(self):
617         """Recreate assigned mdevs that could have disappeared if we reboot
618         the host.
619         """
620         mdevs = self._get_all_assigned_mediated_devices()
621         requested_types = self._get_supported_vgpu_types()
622         for (mdev_uuid, instance_uuid) in six.iteritems(mdevs):
623             if not self._is_existing_mdev(mdev_uuid):
624                 self._create_new_mediated_device(requested_types, mdev_uuid)
625 
626     def _set_multiattach_support(self):
627         # Check to see if multiattach is supported. Based on bugzilla
628         # https://bugzilla.redhat.com/show_bug.cgi?id=1378242 and related
629         # clones, the shareable flag on a disk device will only work with
630         # qemu<2.10 or libvirt>=3.10. So check those versions here and set
631         # the capability appropriately.
632         if (self._host.has_min_version(lv_ver=MIN_LIBVIRT_MULTIATTACH) or
633                 not self._host.has_min_version(hv_ver=(2, 10, 0))):
634             self.capabilities['supports_multiattach'] = True
635         else:
636             LOG.debug('Volume multiattach is not supported based on current '
637                       'versions of QEMU and libvirt. QEMU must be less than '
638                       '2.10 or libvirt must be greater than or equal to 3.10.')
639 
640     def _prepare_migration_flags(self):
641         migration_flags = 0
642 
643         migration_flags |= libvirt.VIR_MIGRATE_LIVE
644 
645         # Adding p2p flag only if xen is not in use, because xen does not
646         # support p2p migrations
647         if CONF.libvirt.virt_type != 'xen':
648             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
649 
650         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
651         # instance will remain defined on the source host
652         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
653 
654         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
655         # destination host
656         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
657 
658         live_migration_flags = block_migration_flags = migration_flags
659 
660         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
661         # will be live-migrations instead
662         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
663 
664         return (live_migration_flags, block_migration_flags)
665 
666     def _handle_live_migration_tunnelled(self, migration_flags):
667         if (CONF.libvirt.live_migration_tunnelled is None or
668                 CONF.libvirt.live_migration_tunnelled):
669             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
670         return migration_flags
671 
672     def _is_post_copy_available(self):
673         if self._host.has_min_version(lv_ver=MIN_LIBVIRT_POSTCOPY_VERSION,
674                                       hv_ver=MIN_QEMU_POSTCOPY_VERSION):
675             return True
676         return False
677 
678     def _is_virtlogd_available(self):
679         return self._host.has_min_version(MIN_LIBVIRT_VIRTLOGD,
680                                           MIN_QEMU_VIRTLOGD)
681 
682     def _is_native_luks_available(self):
683         return self._host.has_min_version(MIN_LIBVIRT_LUKS_VERSION,
684                                           MIN_QEMU_LUKS_VERSION)
685 
686     def _handle_live_migration_post_copy(self, migration_flags):
687         if CONF.libvirt.live_migration_permit_post_copy:
688             if self._is_post_copy_available():
689                 migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
690             else:
691                 LOG.info('The live_migration_permit_post_copy is set '
692                          'to True, but it is not supported.')
693         return migration_flags
694 
695     def _handle_live_migration_auto_converge(self, migration_flags):
696         if (self._is_post_copy_available() and
697                 (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0):
698             LOG.info('The live_migration_permit_post_copy is set to '
699                      'True and post copy live migration is available '
700                      'so auto-converge will not be in use.')
701         elif CONF.libvirt.live_migration_permit_auto_converge:
702             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
703         return migration_flags
704 
705     def _parse_migration_flags(self):
706         (live_migration_flags,
707             block_migration_flags) = self._prepare_migration_flags()
708 
709         live_migration_flags = self._handle_live_migration_tunnelled(
710             live_migration_flags)
711         block_migration_flags = self._handle_live_migration_tunnelled(
712             block_migration_flags)
713 
714         live_migration_flags = self._handle_live_migration_post_copy(
715             live_migration_flags)
716         block_migration_flags = self._handle_live_migration_post_copy(
717             block_migration_flags)
718 
719         live_migration_flags = self._handle_live_migration_auto_converge(
720             live_migration_flags)
721         block_migration_flags = self._handle_live_migration_auto_converge(
722             block_migration_flags)
723 
724         self._live_migration_flags = live_migration_flags
725         self._block_migration_flags = block_migration_flags
726 
727     # TODO(sahid): This method is targeted for removal when the tests
728     # have been updated to avoid its use
729     #
730     # All libvirt API calls on the libvirt.Connect object should be
731     # encapsulated by methods on the nova.virt.libvirt.host.Host
732     # object, rather than directly invoking the libvirt APIs. The goal
733     # is to avoid a direct dependency on the libvirt API from the
734     # driver.py file.
735     def _get_connection(self):
736         return self._host.get_connection()
737 
738     _conn = property(_get_connection)
739 
740     @staticmethod
741     def _uri():
742         if CONF.libvirt.virt_type == 'uml':
743             uri = CONF.libvirt.connection_uri or 'uml:///system'
744         elif CONF.libvirt.virt_type == 'xen':
745             uri = CONF.libvirt.connection_uri or 'xen:///'
746         elif CONF.libvirt.virt_type == 'lxc':
747             uri = CONF.libvirt.connection_uri or 'lxc:///'
748         elif CONF.libvirt.virt_type == 'parallels':
749             uri = CONF.libvirt.connection_uri or 'parallels:///system'
750         else:
751             uri = CONF.libvirt.connection_uri or 'qemu:///system'
752         return uri
753 
754     @staticmethod
755     def _live_migration_uri(dest):
756         uris = {
757             'kvm': 'qemu+%s://%s/system',
758             'qemu': 'qemu+%s://%s/system',
759             'xen': 'xenmigr://%s/system',
760             'parallels': 'parallels+tcp://%s/system',
761         }
762         virt_type = CONF.libvirt.virt_type
763         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
764         uri = CONF.libvirt.live_migration_uri
765         if uri:
766             return uri % dest
767 
768         uri = uris.get(virt_type)
769         if uri is None:
770             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
771 
772         str_format = (dest,)
773         if virt_type in ('kvm', 'qemu'):
774             scheme = CONF.libvirt.live_migration_scheme or 'tcp'
775             str_format = (scheme, dest)
776         return uris.get(virt_type) % str_format
777 
778     @staticmethod
779     def _migrate_uri(dest):
780         uri = None
781         # Only QEMU live migrations supports migrate-uri parameter
782         virt_type = CONF.libvirt.virt_type
783         if virt_type in ('qemu', 'kvm'):
784             # QEMU accept two schemes: tcp and rdma.  By default
785             # libvirt build the URI using the remote hostname and the
786             # tcp schema.
787             uri = 'tcp://%s' % dest
788         # Because dest might be of type unicode, here we might return value of
789         # type unicode as well which is not acceptable by libvirt python
790         # binding when Python 2.7 is in use, so let's convert it explicitly
791         # back to string. When Python 3.x is in use, libvirt python binding
792         # accepts unicode type so it is completely fine to do a no-op str(uri)
793         # conversion which will return value of type unicode.
794         return uri and str(uri)
795 
796     def instance_exists(self, instance):
797         """Efficient override of base instance_exists method."""
798         try:
799             self._host.get_guest(instance)
800             return True
801         except (exception.InternalError, exception.InstanceNotFound):
802             return False
803 
804     def estimate_instance_overhead(self, instance_info):
805         overhead = super(LibvirtDriver, self).estimate_instance_overhead(
806             instance_info)
807         if isinstance(instance_info, objects.Flavor):
808             # A flavor object is passed during case of migrate
809             # TODO(sahid): We do not have any way to retrieve the
810             # image meta related to the instance so if the cpu_policy
811             # has been set in image_meta we will get an
812             # exception. Until we fix it we specifically set the
813             # cpu_policy in dedicated in an ImageMeta object so if the
814             # emulator threads has been requested nothing is going to
815             # fail.
816             image_meta = objects.ImageMeta.from_dict({"properties": {
817                 "hw_cpu_policy": fields.CPUAllocationPolicy.DEDICATED,
818             }})
819             if (hardware.get_emulator_threads_constraint(
820                     instance_info, image_meta)
821                 == fields.CPUEmulatorThreadsPolicy.ISOLATE):
822                 overhead['vcpus'] += 1
823         else:
824             # An instance object is passed during case of spawing or a
825             # dict is passed when computing resource for an instance
826             numa_topology = hardware.instance_topology_from_instance(
827                 instance_info)
828             if numa_topology and numa_topology.emulator_threads_isolated:
829                 overhead['vcpus'] += 1
830         return overhead
831 
832     def list_instances(self):
833         names = []
834         for guest in self._host.list_guests(only_running=False):
835             names.append(guest.name)
836 
837         return names
838 
839     def list_instance_uuids(self):
840         uuids = []
841         for guest in self._host.list_guests(only_running=False):
842             uuids.append(guest.uuid)
843 
844         return uuids
845 
846     def plug_vifs(self, instance, network_info):
847         """Plug VIFs into networks."""
848         for vif in network_info:
849             self.vif_driver.plug(instance, vif)
850 
851     def _unplug_vifs(self, instance, network_info, ignore_errors):
852         """Unplug VIFs from networks."""
853         for vif in network_info:
854             try:
855                 self.vif_driver.unplug(instance, vif)
856             except exception.NovaException:
857                 if not ignore_errors:
858                     raise
859 
860     def unplug_vifs(self, instance, network_info):
861         self._unplug_vifs(instance, network_info, False)
862 
863     def _teardown_container(self, instance):
864         inst_path = libvirt_utils.get_instance_path(instance)
865         container_dir = os.path.join(inst_path, 'rootfs')
866         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
867         LOG.debug('Attempting to teardown container at path %(dir)s with '
868                   'root device: %(rootfs_dev)s',
869                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
870                   instance=instance)
871         disk_api.teardown_container(container_dir, rootfs_dev)
872 
873     def _destroy(self, instance, attempt=1):
874         try:
875             guest = self._host.get_guest(instance)
876             if CONF.serial_console.enabled:
877                 # This method is called for several events: destroy,
878                 # rebuild, hard-reboot, power-off - For all of these
879                 # events we want to release the serial ports acquired
880                 # for the guest before destroying it.
881                 serials = self._get_serial_ports_from_guest(guest)
882                 for hostname, port in serials:
883                     serial_console.release_port(host=hostname, port=port)
884         except exception.InstanceNotFound:
885             guest = None
886 
887         # If the instance is already terminated, we're still happy
888         # Otherwise, destroy it
889         old_domid = -1
890         if guest is not None:
891             try:
892                 old_domid = guest.id
893                 guest.poweroff()
894 
895             except libvirt.libvirtError as e:
896                 is_okay = False
897                 errcode = e.get_error_code()
898                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
899                     # Domain already gone. This can safely be ignored.
900                     is_okay = True
901                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
902                     # If the instance is already shut off, we get this:
903                     # Code=55 Error=Requested operation is not valid:
904                     # domain is not running
905 
906                     state = guest.get_power_state(self._host)
907                     if state == power_state.SHUTDOWN:
908                         is_okay = True
909                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
910                     errmsg = e.get_error_message()
911                     if (CONF.libvirt.virt_type == 'lxc' and
912                         errmsg == 'internal error: '
913                                   'Some processes refused to die'):
914                         # Some processes in the container didn't die
915                         # fast enough for libvirt. The container will
916                         # eventually die. For now, move on and let
917                         # the wait_for_destroy logic take over.
918                         is_okay = True
919                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
920                     LOG.warning("Cannot destroy instance, operation time out",
921                                 instance=instance)
922                     reason = _("operation time out")
923                     raise exception.InstancePowerOffFailure(reason=reason)
924                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
925                     if e.get_int1() == errno.EBUSY:
926                         # NOTE(danpb): When libvirt kills a process it sends it
927                         # SIGTERM first and waits 10 seconds. If it hasn't gone
928                         # it sends SIGKILL and waits another 5 seconds. If it
929                         # still hasn't gone then you get this EBUSY error.
930                         # Usually when a QEMU process fails to go away upon
931                         # SIGKILL it is because it is stuck in an
932                         # uninterruptible kernel sleep waiting on I/O from
933                         # some non-responsive server.
934                         # Given the CPU load of the gate tests though, it is
935                         # conceivable that the 15 second timeout is too short,
936                         # particularly if the VM running tempest has a high
937                         # steal time from the cloud host. ie 15 wallclock
938                         # seconds may have passed, but the VM might have only
939                         # have a few seconds of scheduled run time.
940                         LOG.warning('Error from libvirt during destroy. '
941                                     'Code=%(errcode)s Error=%(e)s; '
942                                     'attempt %(attempt)d of 3',
943                                     {'errcode': errcode, 'e': e,
944                                      'attempt': attempt},
945                                     instance=instance)
946                         with excutils.save_and_reraise_exception() as ctxt:
947                             # Try up to 3 times before giving up.
948                             if attempt < 3:
949                                 ctxt.reraise = False
950                                 self._destroy(instance, attempt + 1)
951                                 return
952 
953                 if not is_okay:
954                     with excutils.save_and_reraise_exception():
955                         LOG.error('Error from libvirt during destroy. '
956                                   'Code=%(errcode)s Error=%(e)s',
957                                   {'errcode': errcode, 'e': e},
958                                   instance=instance)
959 
960         def _wait_for_destroy(expected_domid):
961             """Called at an interval until the VM is gone."""
962             # NOTE(vish): If the instance disappears during the destroy
963             #             we ignore it so the cleanup can still be
964             #             attempted because we would prefer destroy to
965             #             never fail.
966             try:
967                 dom_info = self.get_info(instance)
968                 state = dom_info.state
969                 new_domid = dom_info.internal_id
970             except exception.InstanceNotFound:
971                 LOG.debug("During wait destroy, instance disappeared.",
972                           instance=instance)
973                 state = power_state.SHUTDOWN
974 
975             if state == power_state.SHUTDOWN:
976                 LOG.info("Instance destroyed successfully.", instance=instance)
977                 raise loopingcall.LoopingCallDone()
978 
979             # NOTE(wangpan): If the instance was booted again after destroy,
980             #                this may be an endless loop, so check the id of
981             #                domain here, if it changed and the instance is
982             #                still running, we should destroy it again.
983             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
984             if new_domid != expected_domid:
985                 LOG.info("Instance may be started again.", instance=instance)
986                 kwargs['is_running'] = True
987                 raise loopingcall.LoopingCallDone()
988 
989         kwargs = {'is_running': False}
990         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
991                                                      old_domid)
992         timer.start(interval=0.5).wait()
993         if kwargs['is_running']:
994             LOG.info("Going to destroy instance again.", instance=instance)
995             self._destroy(instance)
996         else:
997             # NOTE(GuanQiang): teardown container to avoid resource leak
998             if CONF.libvirt.virt_type == 'lxc':
999                 self._teardown_container(instance)
1000 
1001     def destroy(self, context, instance, network_info, block_device_info=None,
1002                 destroy_disks=True):
1003         self._destroy(instance)
1004         self.cleanup(context, instance, network_info, block_device_info,
1005                      destroy_disks)
1006 
1007     def _undefine_domain(self, instance):
1008         try:
1009             guest = self._host.get_guest(instance)
1010             try:
1011                 support_uefi = self._has_uefi_support()
1012                 guest.delete_configuration(support_uefi)
1013             except libvirt.libvirtError as e:
1014                 with excutils.save_and_reraise_exception() as ctxt:
1015                     errcode = e.get_error_code()
1016                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1017                         LOG.debug("Called undefine, but domain already gone.",
1018                                   instance=instance)
1019                         ctxt.reraise = False
1020                     else:
1021                         LOG.error('Error from libvirt during undefine. '
1022                                   'Code=%(errcode)s Error=%(e)s',
1023                                   {'errcode': errcode,
1024                                    'e': encodeutils.exception_to_unicode(e)},
1025                                   instance=instance)
1026         except exception.InstanceNotFound:
1027             pass
1028 
1029     def cleanup(self, context, instance, network_info, block_device_info=None,
1030                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
1031         if destroy_vifs:
1032             self._unplug_vifs(instance, network_info, True)
1033 
1034         # Continue attempting to remove firewall filters for the instance
1035         # until it's done or there is a failure to remove the filters. If
1036         # unfilter fails because the instance is not yet shutdown, try to
1037         # destroy the guest again and then retry the unfilter.
1038         while True:
1039             try:
1040                 self.unfilter_instance(instance, network_info)
1041                 break
1042             except libvirt.libvirtError as e:
1043                 try:
1044                     state = self.get_info(instance).state
1045                 except exception.InstanceNotFound:
1046                     state = power_state.SHUTDOWN
1047 
1048                 if state != power_state.SHUTDOWN:
1049                     LOG.warning("Instance may be still running, destroy "
1050                                 "it again.", instance=instance)
1051                     self._destroy(instance)
1052                 else:
1053                     errcode = e.get_error_code()
1054                     LOG.exception(_('Error from libvirt during unfilter. '
1055                                     'Code=%(errcode)s Error=%(e)s'),
1056                                   {'errcode': errcode, 'e': e},
1057                                   instance=instance)
1058                     reason = _("Error unfiltering instance.")
1059                     raise exception.InstanceTerminationFailure(reason=reason)
1060             except Exception:
1061                 raise
1062 
1063         # FIXME(wangpan): if the instance is booted again here, such as the
1064         #                 soft reboot operation boot it here, it will become
1065         #                 "running deleted", should we check and destroy it
1066         #                 at the end of this method?
1067 
1068         # NOTE(vish): we disconnect from volumes regardless
1069         block_device_mapping = driver.block_device_info_get_mapping(
1070             block_device_info)
1071         for vol in block_device_mapping:
1072             connection_info = vol['connection_info']
1073             disk_dev = vol['mount_device']
1074             if disk_dev is not None:
1075                 disk_dev = disk_dev.rpartition("/")[2]
1076             try:
1077                 self._disconnect_volume(context, connection_info, instance)
1078             except Exception as exc:
1079                 with excutils.save_and_reraise_exception() as ctxt:
1080                     if destroy_disks:
1081                         # Don't block on Volume errors if we're trying to
1082                         # delete the instance as we may be partially created
1083                         # or deleted
1084                         ctxt.reraise = False
1085                         LOG.warning(
1086                             "Ignoring Volume Error on vol %(vol_id)s "
1087                             "during delete %(exc)s",
1088                             {'vol_id': vol.get('volume_id'),
1089                              'exc': encodeutils.exception_to_unicode(exc)},
1090                             instance=instance)
1091 
1092         if destroy_disks:
1093             # NOTE(haomai): destroy volumes if needed
1094             if CONF.libvirt.images_type == 'lvm':
1095                 self._cleanup_lvm(instance, block_device_info)
1096             if CONF.libvirt.images_type == 'rbd':
1097                 self._cleanup_rbd(instance)
1098 
1099         is_shared_block_storage = False
1100         if migrate_data and 'is_shared_block_storage' in migrate_data:
1101             is_shared_block_storage = migrate_data.is_shared_block_storage
1102         if destroy_disks or is_shared_block_storage:
1103             attempts = int(instance.system_metadata.get('clean_attempts',
1104                                                         '0'))
1105             success = self.delete_instance_files(instance)
1106             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1107             # task in the compute manager. The tight coupling is not great...
1108             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1109             if success:
1110                 instance.cleaned = True
1111             instance.save()
1112 
1113         self._undefine_domain(instance)
1114 
1115     def _detach_encrypted_volumes(self, instance, block_device_info):
1116         """Detaches encrypted volumes attached to instance."""
1117         disks = self._get_instance_disk_info(instance, block_device_info)
1118         encrypted_volumes = filter(dmcrypt.is_encrypted,
1119                                    [disk['path'] for disk in disks])
1120         for path in encrypted_volumes:
1121             dmcrypt.delete_volume(path)
1122 
1123     def _get_serial_ports_from_guest(self, guest, mode=None):
1124         """Returns an iterator over serial port(s) configured on guest.
1125 
1126         :param mode: Should be a value in (None, bind, connect)
1127         """
1128         xml = guest.get_xml_desc()
1129         tree = etree.fromstring(xml)
1130 
1131         # The 'serial' device is the base for x86 platforms. Other platforms
1132         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1133         xpath_mode = "[@mode='%s']" % mode if mode else ""
1134         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1135         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1136 
1137         tcp_devices = tree.findall(serial_tcp)
1138         if len(tcp_devices) == 0:
1139             tcp_devices = tree.findall(console_tcp)
1140         for source in tcp_devices:
1141             yield (source.get("host"), int(source.get("service")))
1142 
1143     def _get_scsi_controller_max_unit(self, guest):
1144         """Returns the max disk unit used by scsi controller"""
1145         xml = guest.get_xml_desc()
1146         tree = etree.fromstring(xml)
1147         addrs = "./devices/disk[@device='disk']/address[@type='drive']"
1148 
1149         ret = []
1150         for obj in tree.findall(addrs):
1151             ret.append(int(obj.get('unit', 0)))
1152         return max(ret)
1153 
1154     @staticmethod
1155     def _get_rbd_driver():
1156         return rbd_utils.RBDDriver(
1157                 pool=CONF.libvirt.images_rbd_pool,
1158                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
1159                 rbd_user=CONF.libvirt.rbd_user)
1160 
1161     def _cleanup_rbd(self, instance):
1162         # NOTE(nic): On revert_resize, the cleanup steps for the root
1163         # volume are handled with an "rbd snap rollback" command,
1164         # and none of this is needed (and is, in fact, harmful) so
1165         # filter out non-ephemerals from the list
1166         if instance.task_state == task_states.RESIZE_REVERTING:
1167             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1168                                       disk.endswith('disk.local'))
1169         else:
1170             filter_fn = lambda disk: disk.startswith(instance.uuid)
1171         LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
1172 
1173     def _cleanup_lvm(self, instance, block_device_info):
1174         """Delete all LVM disks for given instance object."""
1175         if instance.get('ephemeral_key_uuid') is not None:
1176             self._detach_encrypted_volumes(instance, block_device_info)
1177 
1178         disks = self._lvm_disks(instance)
1179         if disks:
1180             lvm.remove_volumes(disks)
1181 
1182     def _lvm_disks(self, instance):
1183         """Returns all LVM disks for given instance object."""
1184         if CONF.libvirt.images_volume_group:
1185             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1186             if not os.path.exists(vg):
1187                 return []
1188             pattern = '%s_' % instance.uuid
1189 
1190             def belongs_to_instance(disk):
1191                 return disk.startswith(pattern)
1192 
1193             def fullpath(name):
1194                 return os.path.join(vg, name)
1195 
1196             logical_volumes = lvm.list_volumes(vg)
1197 
1198             disks = [fullpath(disk) for disk in logical_volumes
1199                      if belongs_to_instance(disk)]
1200             return disks
1201         return []
1202 
1203     def get_volume_connector(self, instance):
1204         root_helper = utils.get_root_helper()
1205         return connector.get_connector_properties(
1206             root_helper, CONF.my_block_storage_ip,
1207             CONF.libvirt.volume_use_multipath,
1208             enforce_multipath=True,
1209             host=CONF.host)
1210 
1211     def _cleanup_resize(self, context, instance, network_info):
1212         inst_base = libvirt_utils.get_instance_path(instance)
1213         target = inst_base + '_resize'
1214 
1215         if os.path.exists(target):
1216             # Deletion can fail over NFS, so retry the deletion as required.
1217             # Set maximum attempt as 5, most test can remove the directory
1218             # for the second time.
1219             attempts = 0
1220             while(os.path.exists(target) and attempts < 5):
1221                 shutil.rmtree(target)
1222                 if os.path.exists(target):
1223                     time.sleep(random.randint(20, 200) / 100.0)
1224                 attempts += 1
1225 
1226         root_disk = self.image_backend.by_name(instance, 'disk')
1227         # TODO(nic): Set ignore_errors=False in a future release.
1228         # It is set to True here to avoid any upgrade issues surrounding
1229         # instances being in pending resize state when the software is updated;
1230         # in that case there will be no snapshot to remove.  Once it can be
1231         # reasonably assumed that no such instances exist in the wild
1232         # anymore, it should be set back to False (the default) so it will
1233         # throw errors, like it should.
1234         if root_disk.exists():
1235             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
1236                                   ignore_errors=True)
1237 
1238         # NOTE(mjozefcz):
1239         # self.image_backend.image for some backends recreates instance
1240         # directory and image disk.info - remove it here if exists
1241         # Do not remove inst_base for volume-backed instances since that
1242         # could potentially remove the files on the destination host
1243         # if using shared storage.
1244         if (os.path.exists(inst_base) and not root_disk.exists() and
1245                 not compute_utils.is_volume_backed_instance(
1246                     context, instance)):
1247             try:
1248                 shutil.rmtree(inst_base)
1249             except OSError as e:
1250                 if e.errno != errno.ENOENT:
1251                     raise
1252 
1253         if instance.host != CONF.host:
1254             self._undefine_domain(instance)
1255             self.unplug_vifs(instance, network_info)
1256             self.unfilter_instance(instance, network_info)
1257 
1258     def _get_volume_driver(self, connection_info):
1259         driver_type = connection_info.get('driver_volume_type')
1260         if driver_type not in self.volume_drivers:
1261             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1262         return self.volume_drivers[driver_type]
1263 
1264     def _connect_volume(self, context, connection_info, instance,
1265                         encryption=None, allow_native_luks=True):
1266         vol_driver = self._get_volume_driver(connection_info)
1267         vol_driver.connect_volume(connection_info, instance)
1268         self._attach_encryptor(context, connection_info, encryption,
1269                                allow_native_luks)
1270 
1271     def _should_disconnect_target(self, context, connection_info, instance):
1272         connection_count = 0
1273 
1274         # NOTE(jdg): Multiattach is a special case (not to be confused
1275         # with shared_targets). With multiattach we may have a single volume
1276         # attached multiple times to *this* compute node (ie Server-1 and
1277         # Server-2).  So, if we receive a call to delete the attachment for
1278         # Server-1 we need to take special care to make sure that the Volume
1279         # isn't also attached to another Server on this Node.  Otherwise we
1280         # will indiscriminantly delete the connection for all Server and that's
1281         # no good.  So check if it's attached multiple times on this node
1282         # if it is we skip the call to brick to delete the connection.
1283         if connection_info.get('multiattach', False):
1284             volume = self._volume_api.get(
1285                 context,
1286                 driver_block_device.get_volume_id(connection_info))
1287             attachments = volume.get('attachments', {})
1288             if len(attachments) > 1:
1289                 # First we get a list of all Server UUID's associated with
1290                 # this Host (Compute Node).  We're going to use this to
1291                 # determine if the Volume being detached is also in-use by
1292                 # another Server on this Host, ie just check to see if more
1293                 # than one attachment.server_id for this volume is in our
1294                 # list of Server UUID's for this Host
1295                 servers_this_host = objects.InstanceList.get_uuids_by_host(
1296                     context, instance.host)
1297 
1298                 # NOTE(jdg): nova.volume.cinder translates the
1299                 # volume['attachments'] response into a dict which includes
1300                 # the Server UUID as the key, so we're using that
1301                 # here to check against our server_this_host list
1302                 for server_id, data in attachments.items():
1303                     if server_id in servers_this_host:
1304                         connection_count += 1
1305         return (False if connection_count > 1 else True)
1306 
1307     def _disconnect_volume(self, context, connection_info, instance,
1308                            encryption=None):
1309         self._detach_encryptor(context, connection_info, encryption=encryption)
1310         if self._should_disconnect_target(context, connection_info, instance):
1311             vol_driver = self._get_volume_driver(connection_info)
1312             vol_driver.disconnect_volume(connection_info, instance)
1313         else:
1314             LOG.info("Detected multiple connections on this host for volume: "
1315                      "%s, skipping target disconnect.",
1316                      driver_block_device.get_volume_id(connection_info),
1317                      instance=instance)
1318 
1319     def _extend_volume(self, connection_info, instance):
1320         vol_driver = self._get_volume_driver(connection_info)
1321         return vol_driver.extend_volume(connection_info, instance)
1322 
1323     def _use_native_luks(self, encryption=None):
1324         """Is LUKS the required provider and native QEMU LUKS available
1325         """
1326         provider = None
1327         if encryption:
1328             provider = encryption.get('provider', None)
1329         if provider in encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP:
1330             provider = encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP[provider]
1331         return provider == encryptors.LUKS and self._is_native_luks_available()
1332 
1333     def _get_volume_config(self, connection_info, disk_info):
1334         vol_driver = self._get_volume_driver(connection_info)
1335         conf = vol_driver.get_config(connection_info, disk_info)
1336         self._set_cache_mode(conf)
1337         return conf
1338 
1339     def _get_volume_encryptor(self, connection_info, encryption):
1340         root_helper = utils.get_root_helper()
1341         return encryptors.get_volume_encryptor(root_helper=root_helper,
1342                                                keymgr=key_manager.API(CONF),
1343                                                connection_info=connection_info,
1344                                                **encryption)
1345 
1346     def _get_volume_encryption(self, context, connection_info):
1347         """Get the encryption metadata dict if it is not provided
1348         """
1349         encryption = {}
1350         volume_id = driver_block_device.get_volume_id(connection_info)
1351         if volume_id:
1352             encryption = encryptors.get_encryption_metadata(context,
1353                             self._volume_api, volume_id, connection_info)
1354         return encryption
1355 
1356     def _attach_encryptor(self, context, connection_info, encryption,
1357                           allow_native_luks):
1358         """Attach the frontend encryptor if one is required by the volume.
1359 
1360         The request context is only used when an encryption metadata dict is
1361         not provided. The encryption metadata dict being populated is then used
1362         to determine if an attempt to attach the encryptor should be made.
1363 
1364         If native LUKS decryption is enabled then create a Libvirt volume
1365         secret containing the LUKS passphrase for the volume.
1366         """
1367         if encryption is None:
1368             encryption = self._get_volume_encryption(context, connection_info)
1369 
1370         if (encryption and allow_native_luks and
1371             self._use_native_luks(encryption)):
1372             # NOTE(lyarwood): Fetch the associated key for the volume and
1373             # decode the passphrase from the key.
1374             # FIXME(lyarwood): c-vol currently creates symmetric keys for use
1375             # with volumes, leading to the binary to hex to string conversion
1376             # below.
1377             keymgr = key_manager.API(CONF)
1378             key = keymgr.get(context, encryption['encryption_key_id'])
1379             key_encoded = key.get_encoded()
1380             passphrase = binascii.hexlify(key_encoded).decode('utf-8')
1381 
1382             # NOTE(lyarwood): Retain the behaviour of the original os-brick
1383             # encryptors and format any volume that does not identify as
1384             # encrypted with LUKS.
1385             # FIXME(lyarwood): Remove this once c-vol correctly formats
1386             # encrypted volumes during their initial creation:
1387             # https://bugs.launchpad.net/cinder/+bug/1739442
1388             device_path = connection_info.get('data').get('device_path')
1389             if device_path:
1390                 root_helper = utils.get_root_helper()
1391                 if not luks_encryptor.is_luks(root_helper, device_path):
1392                     encryptor = self._get_volume_encryptor(connection_info,
1393                                                            encryption)
1394                     encryptor._format_volume(passphrase, **encryption)
1395 
1396             # NOTE(lyarwood): Store the passphrase as a libvirt secret locally
1397             # on the compute node. This secret is used later when generating
1398             # the volume config.
1399             volume_id = driver_block_device.get_volume_id(connection_info)
1400             self._host.create_secret('volume', volume_id, password=passphrase)
1401         elif encryption:
1402             encryptor = self._get_volume_encryptor(connection_info,
1403                                                    encryption)
1404             encryptor.attach_volume(context, **encryption)
1405 
1406     def _detach_encryptor(self, context, connection_info, encryption):
1407         """Detach the frontend encryptor if one is required by the volume.
1408 
1409         The request context is only used when an encryption metadata dict is
1410         not provided. The encryption metadata dict being populated is then used
1411         to determine if an attempt to detach the encryptor should be made.
1412 
1413         If native LUKS decryption is enabled then delete previously created
1414         Libvirt volume secret from the host.
1415         """
1416         volume_id = driver_block_device.get_volume_id(connection_info)
1417         if volume_id and self._host.find_secret('volume', volume_id):
1418             return self._host.delete_secret('volume', volume_id)
1419         if encryption is None:
1420             encryption = self._get_volume_encryption(context, connection_info)
1421         if encryption:
1422             encryptor = self._get_volume_encryptor(connection_info,
1423                                                    encryption)
1424             encryptor.detach_volume(**encryption)
1425 
1426     def _check_discard_for_attach_volume(self, conf, instance):
1427         """Perform some checks for volumes configured for discard support.
1428 
1429         If discard is configured for the volume, and the guest is using a
1430         configuration known to not work, we will log a message explaining
1431         the reason why.
1432         """
1433         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1434             LOG.debug('Attempting to attach volume %(id)s with discard '
1435                       'support enabled to an instance using an '
1436                       'unsupported configuration. target_bus = '
1437                       '%(bus)s. Trim commands will not be issued to '
1438                       'the storage device.',
1439                       {'bus': conf.target_bus,
1440                        'id': conf.serial},
1441                       instance=instance)
1442 
1443     def attach_volume(self, context, connection_info, instance, mountpoint,
1444                       disk_bus=None, device_type=None, encryption=None):
1445         guest = self._host.get_guest(instance)
1446 
1447         disk_dev = mountpoint.rpartition("/")[2]
1448         bdm = {
1449             'device_name': disk_dev,
1450             'disk_bus': disk_bus,
1451             'device_type': device_type}
1452 
1453         # Note(cfb): If the volume has a custom block size, check that
1454         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1455         #            presence of a block size is considered mandatory by
1456         #            cinder so we fail if we can't honor the request.
1457         data = {}
1458         if ('data' in connection_info):
1459             data = connection_info['data']
1460         if ('logical_block_size' in data or 'physical_block_size' in data):
1461             if ((CONF.libvirt.virt_type != "kvm" and
1462                  CONF.libvirt.virt_type != "qemu")):
1463                 msg = _("Volume sets block size, but the current "
1464                         "libvirt hypervisor '%s' does not support custom "
1465                         "block size") % CONF.libvirt.virt_type
1466                 raise exception.InvalidHypervisorType(msg)
1467 
1468         self._connect_volume(context, connection_info, instance,
1469                              encryption=encryption)
1470         disk_info = blockinfo.get_info_from_bdm(
1471             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1472         if disk_info['bus'] == 'scsi':
1473             disk_info['unit'] = self._get_scsi_controller_max_unit(guest) + 1
1474 
1475         conf = self._get_volume_config(connection_info, disk_info)
1476 
1477         self._check_discard_for_attach_volume(conf, instance)
1478 
1479         try:
1480             state = guest.get_power_state(self._host)
1481             live = state in (power_state.RUNNING, power_state.PAUSED)
1482 
1483             guest.attach_device(conf, persistent=True, live=live)
1484             # NOTE(artom) If we're attaching with a device role tag, we need to
1485             # rebuild device_metadata. If we're attaching without a role
1486             # tag, we're rebuilding it here needlessly anyways. This isn't a
1487             # massive deal, and it helps reduce code complexity by not having
1488             # to indicate to the virt driver that the attach is tagged. The
1489             # really important optimization of not calling the database unless
1490             # device_metadata has actually changed is done for us by
1491             # instance.save().
1492             instance.device_metadata = self._build_device_metadata(
1493                 context, instance)
1494             instance.save()
1495         except Exception:
1496             LOG.exception(_('Failed to attach volume at mountpoint: %s'),
1497                           mountpoint, instance=instance)
1498             with excutils.save_and_reraise_exception():
1499                 self._disconnect_volume(context, connection_info, instance,
1500                                         encryption=encryption)
1501 
1502     def _swap_volume(self, guest, disk_path, conf, resize_to):
1503         """Swap existing disk with a new block device."""
1504         dev = guest.get_block_device(disk_path)
1505 
1506         # Save a copy of the domain's persistent XML file. We'll use this
1507         # to redefine the domain if anything fails during the volume swap.
1508         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1509 
1510         # Abort is an idempotent operation, so make sure any block
1511         # jobs which may have failed are ended.
1512         try:
1513             dev.abort_job()
1514         except Exception:
1515             pass
1516 
1517         try:
1518             # NOTE (rmk): blockRebase cannot be executed on persistent
1519             #             domains, so we need to temporarily undefine it.
1520             #             If any part of this block fails, the domain is
1521             #             re-defined regardless.
1522             if guest.has_persistent_configuration():
1523                 support_uefi = self._has_uefi_support()
1524                 guest.delete_configuration(support_uefi)
1525 
1526             try:
1527                 # Start copy with VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT flag to
1528                 # allow writing to existing external volume file. Use
1529                 # VIR_DOMAIN_BLOCK_REBASE_COPY_DEV if it's a block device to
1530                 # make sure XML is generated correctly (bug 1691195)
1531                 copy_dev = conf.source_type == 'block'
1532                 dev.rebase(conf.source_path, copy=True, reuse_ext=True,
1533                            copy_dev=copy_dev)
1534                 while not dev.is_job_complete():
1535                     time.sleep(0.5)
1536 
1537                 dev.abort_job(pivot=True)
1538 
1539             except Exception as exc:
1540                 LOG.exception("Failure rebasing volume %(new_path)s on "
1541                     "%(old_path)s.", {'new_path': conf.source_path,
1542                                       'old_path': disk_path})
1543                 raise exception.VolumeRebaseFailed(reason=six.text_type(exc))
1544 
1545             if resize_to:
1546                 dev.resize(resize_to * units.Gi / units.Ki)
1547 
1548             # Make sure we will redefine the domain using the updated
1549             # configuration after the volume was swapped. The dump_inactive
1550             # keyword arg controls whether we pull the inactive (persistent)
1551             # or active (live) config from the domain. We want to pull the
1552             # live config after the volume was updated to use when we redefine
1553             # the domain.
1554             xml = guest.get_xml_desc(dump_inactive=False, dump_sensitive=True)
1555         finally:
1556             self._host.write_instance_config(xml)
1557 
1558     def swap_volume(self, context, old_connection_info,
1559                     new_connection_info, instance, mountpoint, resize_to):
1560 
1561         # NOTE(lyarwood): https://bugzilla.redhat.com/show_bug.cgi?id=760547
1562         encryption = self._get_volume_encryption(context, old_connection_info)
1563         if encryption and self._use_native_luks(encryption):
1564             raise NotImplementedError(_("Swap volume is not supported for"
1565                 "encrypted volumes when native LUKS decryption is enabled."))
1566 
1567         guest = self._host.get_guest(instance)
1568 
1569         disk_dev = mountpoint.rpartition("/")[2]
1570         if not guest.get_disk(disk_dev):
1571             raise exception.DiskNotFound(location=disk_dev)
1572         disk_info = {
1573             'dev': disk_dev,
1574             'bus': blockinfo.get_disk_bus_for_disk_dev(
1575                 CONF.libvirt.virt_type, disk_dev),
1576             'type': 'disk',
1577             }
1578         # NOTE (lyarwood): new_connection_info will be modified by the
1579         # following _connect_volume call down into the volume drivers. The
1580         # majority of the volume drivers will add a device_path that is in turn
1581         # used by _get_volume_config to set the source_path of the
1582         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
1583         # this to the BDM here as the upper compute swap_volume method will
1584         # eventually do this for us.
1585         self._connect_volume(context, new_connection_info, instance)
1586         conf = self._get_volume_config(new_connection_info, disk_info)
1587         if not conf.source_path:
1588             self._disconnect_volume(context, new_connection_info, instance)
1589             raise NotImplementedError(_("Swap only supports host devices"))
1590 
1591         try:
1592             self._swap_volume(guest, disk_dev, conf, resize_to)
1593         except exception.VolumeRebaseFailed:
1594             with excutils.save_and_reraise_exception():
1595                 self._disconnect_volume(context, new_connection_info, instance)
1596 
1597         self._disconnect_volume(context, old_connection_info, instance)
1598 
1599     def _get_existing_domain_xml(self, instance, network_info,
1600                                  block_device_info=None):
1601         try:
1602             guest = self._host.get_guest(instance)
1603             xml = guest.get_xml_desc()
1604         except exception.InstanceNotFound:
1605             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1606                                                 instance,
1607                                                 instance.image_meta,
1608                                                 block_device_info)
1609             xml = self._get_guest_xml(nova_context.get_admin_context(),
1610                                       instance, network_info, disk_info,
1611                                       instance.image_meta,
1612                                       block_device_info=block_device_info)
1613         return xml
1614 
1615     def detach_volume(self, context, connection_info, instance, mountpoint,
1616                       encryption=None):
1617         disk_dev = mountpoint.rpartition("/")[2]
1618         try:
1619             guest = self._host.get_guest(instance)
1620 
1621             state = guest.get_power_state(self._host)
1622             live = state in (power_state.RUNNING, power_state.PAUSED)
1623             # NOTE(lyarwood): The volume must be detached from the VM before
1624             # detaching any attached encryptors or disconnecting the underlying
1625             # volume in _disconnect_volume. Otherwise, the encryptor or volume
1626             # driver may report that the volume is still in use.
1627             wait_for_detach = guest.detach_device_with_retry(guest.get_disk,
1628                                                              disk_dev,
1629                                                              live=live)
1630             wait_for_detach()
1631 
1632         except exception.InstanceNotFound:
1633             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1634             #                will throw InstanceNotFound exception. Need to
1635             #                disconnect volume under this circumstance.
1636             LOG.warning("During detach_volume, instance disappeared.",
1637                         instance=instance)
1638         except exception.DeviceNotFound:
1639             # We should still try to disconnect logical device from
1640             # host, an error might have happened during a previous
1641             # call.
1642             LOG.info("Device %s not found in instance.",
1643                      disk_dev, instance=instance)
1644         except libvirt.libvirtError as ex:
1645             # NOTE(vish): This is called to cleanup volumes after live
1646             #             migration, so we should still disconnect even if
1647             #             the instance doesn't exist here anymore.
1648             error_code = ex.get_error_code()
1649             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1650                 # NOTE(vish):
1651                 LOG.warning("During detach_volume, instance disappeared.",
1652                             instance=instance)
1653             else:
1654                 raise
1655 
1656         self._disconnect_volume(context, connection_info, instance,
1657                                 encryption=encryption)
1658 
1659     def extend_volume(self, connection_info, instance):
1660         try:
1661             new_size = self._extend_volume(connection_info, instance)
1662         except NotImplementedError:
1663             raise exception.ExtendVolumeNotSupported()
1664 
1665         # Resize the device in QEMU so its size is updated and
1666         # detected by the instance without rebooting.
1667         try:
1668             guest = self._host.get_guest(instance)
1669             state = guest.get_power_state(self._host)
1670             active_state = state in (power_state.RUNNING, power_state.PAUSED)
1671             if active_state:
1672                 disk_path = connection_info['data']['device_path']
1673                 LOG.debug('resizing block device %(dev)s to %(size)u kb',
1674                           {'dev': disk_path, 'size': new_size})
1675                 dev = guest.get_block_device(disk_path)
1676                 dev.resize(new_size // units.Ki)
1677             else:
1678                 LOG.debug('Skipping block device resize, guest is not running',
1679                           instance=instance)
1680         except exception.InstanceNotFound:
1681             with excutils.save_and_reraise_exception():
1682                 LOG.warning('During extend_volume, instance disappeared.',
1683                             instance=instance)
1684         except libvirt.libvirtError:
1685             with excutils.save_and_reraise_exception():
1686                 LOG.exception('resizing block device failed.',
1687                               instance=instance)
1688 
1689     def attach_interface(self, context, instance, image_meta, vif):
1690         guest = self._host.get_guest(instance)
1691 
1692         self.vif_driver.plug(instance, vif)
1693         self.firewall_driver.setup_basic_filtering(instance, [vif])
1694         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1695                                          instance.flavor,
1696                                          CONF.libvirt.virt_type,
1697                                          self._host)
1698         try:
1699             state = guest.get_power_state(self._host)
1700             live = state in (power_state.RUNNING, power_state.PAUSED)
1701             guest.attach_device(cfg, persistent=True, live=live)
1702         except libvirt.libvirtError:
1703             LOG.error('attaching network adapter failed.',
1704                       instance=instance, exc_info=True)
1705             self.vif_driver.unplug(instance, vif)
1706             raise exception.InterfaceAttachFailed(
1707                     instance_uuid=instance.uuid)
1708         try:
1709             # NOTE(artom) If we're attaching with a device role tag, we need to
1710             # rebuild device_metadata. If we're attaching without a role
1711             # tag, we're rebuilding it here needlessly anyways. This isn't a
1712             # massive deal, and it helps reduce code complexity by not having
1713             # to indicate to the virt driver that the attach is tagged. The
1714             # really important optimization of not calling the database unless
1715             # device_metadata has actually changed is done for us by
1716             # instance.save().
1717             instance.device_metadata = self._build_device_metadata(
1718                 context, instance)
1719             instance.save()
1720         except Exception:
1721             # NOTE(artom) If we fail here it means the interface attached
1722             # successfully but building and/or saving the device metadata
1723             # failed. Just unplugging the vif is therefore not enough cleanup,
1724             # we need to detach the interface.
1725             with excutils.save_and_reraise_exception(reraise=False):
1726                 LOG.error('Interface attached successfully but building '
1727                           'and/or saving device metadata failed.',
1728                           instance=instance, exc_info=True)
1729                 self.detach_interface(context, instance, vif)
1730                 raise exception.InterfaceAttachFailed(
1731                     instance_uuid=instance.uuid)
1732 
1733     def detach_interface(self, context, instance, vif):
1734         guest = self._host.get_guest(instance)
1735         cfg = self.vif_driver.get_config(instance, vif,
1736                                          instance.image_meta,
1737                                          instance.flavor,
1738                                          CONF.libvirt.virt_type, self._host)
1739         interface = guest.get_interface_by_cfg(cfg)
1740         try:
1741             self.vif_driver.unplug(instance, vif)
1742             # NOTE(mriedem): When deleting an instance and using Neutron,
1743             # we can be racing against Neutron deleting the port and
1744             # sending the vif-deleted event which then triggers a call to
1745             # detach the interface, so if the interface is not found then
1746             # we can just log it as a warning.
1747             if not interface:
1748                 mac = vif.get('address')
1749                 # The interface is gone so just log it as a warning.
1750                 LOG.warning('Detaching interface %(mac)s failed because '
1751                             'the device is no longer found on the guest.',
1752                             {'mac': mac}, instance=instance)
1753                 return
1754 
1755             state = guest.get_power_state(self._host)
1756             live = state in (power_state.RUNNING, power_state.PAUSED)
1757             # Now we are going to loop until the interface is detached or we
1758             # timeout.
1759             wait_for_detach = guest.detach_device_with_retry(
1760                 guest.get_interface_by_cfg, cfg, live=live,
1761                 alternative_device_name=self.vif_driver.get_vif_devname(vif))
1762             wait_for_detach()
1763         except exception.DeviceDetachFailed:
1764             # We failed to detach the device even with the retry loop, so let's
1765             # dump some debug information to the logs before raising back up.
1766             with excutils.save_and_reraise_exception():
1767                 devname = self.vif_driver.get_vif_devname(vif)
1768                 interface = guest.get_interface_by_cfg(cfg)
1769                 if interface:
1770                     LOG.warning(
1771                         'Failed to detach interface %(devname)s after '
1772                         'repeated attempts. Final interface xml:\n'
1773                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
1774                         {'devname': devname,
1775                          'interface_xml': interface.to_xml(),
1776                          'guest_xml': guest.get_xml_desc()},
1777                         instance=instance)
1778         except exception.DeviceNotFound:
1779             # The interface is gone so just log it as a warning.
1780             LOG.warning('Detaching interface %(mac)s failed because '
1781                         'the device is no longer found on the guest.',
1782                         {'mac': vif.get('address')}, instance=instance)
1783         except libvirt.libvirtError as ex:
1784             error_code = ex.get_error_code()
1785             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1786                 LOG.warning("During detach_interface, instance disappeared.",
1787                             instance=instance)
1788             else:
1789                 # NOTE(mriedem): When deleting an instance and using Neutron,
1790                 # we can be racing against Neutron deleting the port and
1791                 # sending the vif-deleted event which then triggers a call to
1792                 # detach the interface, so we might have failed because the
1793                 # network device no longer exists. Libvirt will fail with
1794                 # "operation failed: no matching network device was found"
1795                 # which unfortunately does not have a unique error code so we
1796                 # need to look up the interface by config and if it's not found
1797                 # then we can just log it as a warning rather than tracing an
1798                 # error.
1799                 mac = vif.get('address')
1800                 interface = guest.get_interface_by_cfg(cfg)
1801                 if interface:
1802                     LOG.error('detaching network adapter failed.',
1803                               instance=instance, exc_info=True)
1804                     raise exception.InterfaceDetachFailed(
1805                             instance_uuid=instance.uuid)
1806 
1807                 # The interface is gone so just log it as a warning.
1808                 LOG.warning('Detaching interface %(mac)s failed because '
1809                             'the device is no longer found on the guest.',
1810                             {'mac': mac}, instance=instance)
1811 
1812     def _create_snapshot_metadata(self, image_meta, instance,
1813                                   img_fmt, snp_name):
1814         metadata = {'is_public': False,
1815                     'status': 'active',
1816                     'name': snp_name,
1817                     'properties': {
1818                                    'kernel_id': instance.kernel_id,
1819                                    'image_location': 'snapshot',
1820                                    'image_state': 'available',
1821                                    'owner_id': instance.project_id,
1822                                    'ramdisk_id': instance.ramdisk_id,
1823                                    }
1824                     }
1825         if instance.os_type:
1826             metadata['properties']['os_type'] = instance.os_type
1827 
1828         # NOTE(vish): glance forces ami disk format to be ami
1829         if image_meta.disk_format == 'ami':
1830             metadata['disk_format'] = 'ami'
1831         else:
1832             metadata['disk_format'] = img_fmt
1833 
1834         if image_meta.obj_attr_is_set("container_format"):
1835             metadata['container_format'] = image_meta.container_format
1836         else:
1837             metadata['container_format'] = "bare"
1838 
1839         return metadata
1840 
1841     def snapshot(self, context, instance, image_id, update_task_state):
1842         """Create snapshot from a running VM instance.
1843 
1844         This command only works with qemu 0.14+
1845         """
1846         try:
1847             guest = self._host.get_guest(instance)
1848 
1849             # TODO(sahid): We are converting all calls from a
1850             # virDomain object to use nova.virt.libvirt.Guest.
1851             # We should be able to remove virt_dom at the end.
1852             virt_dom = guest._domain
1853         except exception.InstanceNotFound:
1854             raise exception.InstanceNotRunning(instance_id=instance.uuid)
1855 
1856         snapshot = self._image_api.get(context, image_id)
1857 
1858         # source_format is an on-disk format
1859         # source_type is a backend type
1860         disk_path, source_format = libvirt_utils.find_disk(guest)
1861         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
1862 
1863         # We won't have source_type for raw or qcow2 disks, because we can't
1864         # determine that from the path. We should have it from the libvirt
1865         # xml, though.
1866         if source_type is None:
1867             source_type = source_format
1868         # For lxc instances we won't have it either from libvirt xml
1869         # (because we just gave libvirt the mounted filesystem), or the path,
1870         # so source_type is still going to be None. In this case,
1871         # root_disk is going to default to CONF.libvirt.images_type
1872         # below, which is still safe.
1873 
1874         image_format = CONF.libvirt.snapshot_image_format or source_type
1875 
1876         # NOTE(bfilippov): save lvm and rbd as raw
1877         if image_format == 'lvm' or image_format == 'rbd':
1878             image_format = 'raw'
1879 
1880         metadata = self._create_snapshot_metadata(instance.image_meta,
1881                                                   instance,
1882                                                   image_format,
1883                                                   snapshot['name'])
1884 
1885         snapshot_name = uuidutils.generate_uuid(dashed=False)
1886 
1887         state = guest.get_power_state(self._host)
1888 
1889         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
1890         #               cold snapshots. Currently, checking for encryption is
1891         #               redundant because LVM supports only cold snapshots.
1892         #               It is necessary in case this situation changes in the
1893         #               future.
1894         if (self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)
1895                 and source_type not in ('lvm')
1896                 and not CONF.ephemeral_storage_encryption.enabled
1897                 and not CONF.workarounds.disable_libvirt_livesnapshot
1898                 # NOTE(rmk): We cannot perform live snapshots when a
1899                 # managedSave file is present, so we will use the cold/legacy
1900                 # method for instances which are shutdown or paused.
1901                 # NOTE(mriedem): Live snapshot doesn't work with paused
1902                 # instances on older versions of libvirt/qemu. We can likely
1903                 # remove the restriction on PAUSED once we require
1904                 # libvirt>=3.6.0 and qemu>=2.10 since that works with the
1905                 # Pike Ubuntu Cloud Archive testing in Queens.
1906                 and state not in (power_state.SHUTDOWN, power_state.PAUSED)):
1907             live_snapshot = True
1908             # Abort is an idempotent operation, so make sure any block
1909             # jobs which may have failed are ended. This operation also
1910             # confirms the running instance, as opposed to the system as a
1911             # whole, has a new enough version of the hypervisor (bug 1193146).
1912             try:
1913                 guest.get_block_device(disk_path).abort_job()
1914             except libvirt.libvirtError as ex:
1915                 error_code = ex.get_error_code()
1916                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
1917                     live_snapshot = False
1918                 else:
1919                     pass
1920         else:
1921             live_snapshot = False
1922 
1923         self._prepare_domain_for_snapshot(context, live_snapshot, state,
1924                                           instance)
1925 
1926         root_disk = self.image_backend.by_libvirt_path(
1927             instance, disk_path, image_type=source_type)
1928 
1929         if live_snapshot:
1930             LOG.info("Beginning live snapshot process", instance=instance)
1931         else:
1932             LOG.info("Beginning cold snapshot process", instance=instance)
1933 
1934         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
1935 
1936         try:
1937             update_task_state(task_state=task_states.IMAGE_UPLOADING,
1938                               expected_state=task_states.IMAGE_PENDING_UPLOAD)
1939             metadata['location'] = root_disk.direct_snapshot(
1940                 context, snapshot_name, image_format, image_id,
1941                 instance.image_ref)
1942             self._snapshot_domain(context, live_snapshot, virt_dom, state,
1943                                   instance)
1944             self._image_api.update(context, image_id, metadata,
1945                                    purge_props=False)
1946         except (NotImplementedError, exception.ImageUnacceptable,
1947                 exception.Forbidden) as e:
1948             if type(e) != NotImplementedError:
1949                 LOG.warning('Performing standard snapshot because direct '
1950                             'snapshot failed: %(error)s',
1951                             {'error': encodeutils.exception_to_unicode(e)})
1952             failed_snap = metadata.pop('location', None)
1953             if failed_snap:
1954                 failed_snap = {'url': str(failed_snap)}
1955             root_disk.cleanup_direct_snapshot(failed_snap,
1956                                                   also_destroy_volume=True,
1957                                                   ignore_errors=True)
1958             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
1959                               expected_state=task_states.IMAGE_UPLOADING)
1960 
1961             # TODO(nic): possibly abstract this out to the root_disk
1962             if source_type == 'rbd' and live_snapshot:
1963                 # Standard snapshot uses qemu-img convert from RBD which is
1964                 # not safe to run with live_snapshot.
1965                 live_snapshot = False
1966                 # Suspend the guest, so this is no longer a live snapshot
1967                 self._prepare_domain_for_snapshot(context, live_snapshot,
1968                                                   state, instance)
1969 
1970             snapshot_directory = CONF.libvirt.snapshots_directory
1971             fileutils.ensure_tree(snapshot_directory)
1972             with utils.tempdir(dir=snapshot_directory) as tmpdir:
1973                 try:
1974                     out_path = os.path.join(tmpdir, snapshot_name)
1975                     if live_snapshot:
1976                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
1977                         os.chmod(tmpdir, 0o701)
1978                         self._live_snapshot(context, instance, guest,
1979                                             disk_path, out_path, source_format,
1980                                             image_format, instance.image_meta)
1981                     else:
1982                         root_disk.snapshot_extract(out_path, image_format)
1983                     LOG.info("Snapshot extracted, beginning image upload",
1984                              instance=instance)
1985                 except libvirt.libvirtError as ex:
1986                     error_code = ex.get_error_code()
1987                     if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1988                         LOG.info('Instance %(instance_name)s disappeared '
1989                                  'while taking snapshot of it: [Error Code '
1990                                  '%(error_code)s] %(ex)s',
1991                                  {'instance_name': instance.name,
1992                                   'error_code': error_code,
1993                                   'ex': ex},
1994                                  instance=instance)
1995                         raise exception.InstanceNotFound(
1996                             instance_id=instance.uuid)
1997                     else:
1998                         raise
1999                 finally:
2000                     self._snapshot_domain(context, live_snapshot, virt_dom,
2001                                           state, instance)
2002 
2003                 # Upload that image to the image service
2004                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
2005                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
2006                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
2007                     self._image_api.update(context,
2008                                            image_id,
2009                                            metadata,
2010                                            image_file)
2011         except Exception:
2012             with excutils.save_and_reraise_exception():
2013                 LOG.exception(_("Failed to snapshot image"))
2014                 failed_snap = metadata.pop('location', None)
2015                 if failed_snap:
2016                     failed_snap = {'url': str(failed_snap)}
2017                 root_disk.cleanup_direct_snapshot(
2018                         failed_snap, also_destroy_volume=True,
2019                         ignore_errors=True)
2020 
2021         LOG.info("Snapshot image upload complete", instance=instance)
2022 
2023     def _prepare_domain_for_snapshot(self, context, live_snapshot, state,
2024                                      instance):
2025         # NOTE(dkang): managedSave does not work for LXC
2026         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
2027             if state == power_state.RUNNING or state == power_state.PAUSED:
2028                 self.suspend(context, instance)
2029 
2030     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
2031                          instance):
2032         guest = None
2033         # NOTE(dkang): because previous managedSave is not called
2034         #              for LXC, _create_domain must not be called.
2035         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
2036             if state == power_state.RUNNING:
2037                 guest = self._create_domain(domain=virt_dom)
2038             elif state == power_state.PAUSED:
2039                 guest = self._create_domain(domain=virt_dom, pause=True)
2040 
2041             if guest is not None:
2042                 self._attach_pci_devices(
2043                     guest, pci_manager.get_instance_pci_devs(instance))
2044                 self._attach_direct_passthrough_ports(
2045                     context, instance, guest)
2046 
2047     def _can_set_admin_password(self, image_meta):
2048 
2049         if CONF.libvirt.virt_type == 'parallels':
2050             if not self._host.has_min_version(
2051                    MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD):
2052                 raise exception.SetAdminPasswdNotSupported()
2053         elif CONF.libvirt.virt_type in ('kvm', 'qemu'):
2054             if not self._host.has_min_version(
2055                    MIN_LIBVIRT_SET_ADMIN_PASSWD):
2056                 raise exception.SetAdminPasswdNotSupported()
2057             if not image_meta.properties.get('hw_qemu_guest_agent', False):
2058                 raise exception.QemuGuestAgentNotEnabled()
2059         else:
2060             raise exception.SetAdminPasswdNotSupported()
2061 
2062     # TODO(melwitt): Combine this with the similar xenapi code at some point.
2063     def _save_instance_password_if_sshkey_present(self, instance, new_pass):
2064         sshkey = instance.key_data if 'key_data' in instance else None
2065         if sshkey and sshkey.startswith("ssh-rsa"):
2066             enc = crypto.ssh_encrypt_text(sshkey, new_pass)
2067             # NOTE(melwitt): The convert_password method doesn't actually do
2068             # anything with the context argument, so we can pass None.
2069             instance.system_metadata.update(
2070                 password.convert_password(None, base64.encode_as_text(enc)))
2071             instance.save()
2072 
2073     def set_admin_password(self, instance, new_pass):
2074         self._can_set_admin_password(instance.image_meta)
2075 
2076         guest = self._host.get_guest(instance)
2077         user = instance.image_meta.properties.get("os_admin_user")
2078         if not user:
2079             if instance.os_type == "windows":
2080                 user = "Administrator"
2081             else:
2082                 user = "root"
2083         try:
2084             guest.set_user_password(user, new_pass)
2085         except libvirt.libvirtError as ex:
2086             error_code = ex.get_error_code()
2087             if error_code == libvirt.VIR_ERR_AGENT_UNRESPONSIVE:
2088                 LOG.debug('Failed to set password: QEMU agent unresponsive',
2089                           instance_uuid=instance.uuid)
2090                 raise NotImplementedError()
2091 
2092             err_msg = encodeutils.exception_to_unicode(ex)
2093             msg = (_('Error from libvirt while set password for username '
2094                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
2095                    % {'user': user, 'error_code': error_code, 'ex': err_msg})
2096             raise exception.InternalError(msg)
2097         else:
2098             # Save the password in sysmeta so it may be retrieved from the
2099             # metadata service.
2100             self._save_instance_password_if_sshkey_present(instance, new_pass)
2101 
2102     def _can_quiesce(self, instance, image_meta):
2103         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
2104             raise exception.InstanceQuiesceNotSupported(
2105                 instance_id=instance.uuid)
2106 
2107         if not image_meta.properties.get('hw_qemu_guest_agent', False):
2108             raise exception.QemuGuestAgentNotEnabled()
2109 
2110     def _requires_quiesce(self, image_meta):
2111         return image_meta.properties.get('os_require_quiesce', False)
2112 
2113     def _set_quiesced(self, context, instance, image_meta, quiesced):
2114         self._can_quiesce(instance, image_meta)
2115         try:
2116             guest = self._host.get_guest(instance)
2117             if quiesced:
2118                 guest.freeze_filesystems()
2119             else:
2120                 guest.thaw_filesystems()
2121         except libvirt.libvirtError as ex:
2122             error_code = ex.get_error_code()
2123             err_msg = encodeutils.exception_to_unicode(ex)
2124             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
2125                      '[Error Code %(error_code)s] %(ex)s')
2126                    % {'instance_name': instance.name,
2127                       'error_code': error_code, 'ex': err_msg})
2128             raise exception.InternalError(msg)
2129 
2130     def quiesce(self, context, instance, image_meta):
2131         """Freeze the guest filesystems to prepare for snapshot.
2132 
2133         The qemu-guest-agent must be setup to execute fsfreeze.
2134         """
2135         self._set_quiesced(context, instance, image_meta, True)
2136 
2137     def unquiesce(self, context, instance, image_meta):
2138         """Thaw the guest filesystems after snapshot."""
2139         self._set_quiesced(context, instance, image_meta, False)
2140 
2141     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
2142                        source_format, image_format, image_meta):
2143         """Snapshot an instance without downtime."""
2144         dev = guest.get_block_device(disk_path)
2145 
2146         # Save a copy of the domain's persistent XML file
2147         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
2148 
2149         # Abort is an idempotent operation, so make sure any block
2150         # jobs which may have failed are ended.
2151         try:
2152             dev.abort_job()
2153         except Exception:
2154             pass
2155 
2156         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
2157         #             in QEMU 1.3. In order to do this, we need to create
2158         #             a destination image with the original backing file
2159         #             and matching size of the instance root disk.
2160         src_disk_size = libvirt_utils.get_disk_size(disk_path,
2161                                                     format=source_format)
2162         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
2163                                                         format=source_format,
2164                                                         basename=False)
2165         disk_delta = out_path + '.delta'
2166         libvirt_utils.create_cow_image(src_back_path, disk_delta,
2167                                        src_disk_size)
2168 
2169         quiesced = False
2170         try:
2171             self._set_quiesced(context, instance, image_meta, True)
2172             quiesced = True
2173         except exception.NovaException as err:
2174             if self._requires_quiesce(image_meta):
2175                 raise
2176             LOG.info('Skipping quiescing instance: %(reason)s.',
2177                      {'reason': err}, instance=instance)
2178 
2179         try:
2180             # NOTE (rmk): blockRebase cannot be executed on persistent
2181             #             domains, so we need to temporarily undefine it.
2182             #             If any part of this block fails, the domain is
2183             #             re-defined regardless.
2184             if guest.has_persistent_configuration():
2185                 support_uefi = self._has_uefi_support()
2186                 guest.delete_configuration(support_uefi)
2187 
2188             # NOTE (rmk): Establish a temporary mirror of our root disk and
2189             #             issue an abort once we have a complete copy.
2190             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
2191 
2192             while not dev.is_job_complete():
2193                 time.sleep(0.5)
2194 
2195             dev.abort_job()
2196             nova.privsep.path.chown(disk_delta, uid=os.getuid())
2197         finally:
2198             self._host.write_instance_config(xml)
2199             if quiesced:
2200                 self._set_quiesced(context, instance, image_meta, False)
2201 
2202         # Convert the delta (CoW) image with a backing file to a flat
2203         # image with no backing file.
2204         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
2205                                        out_path, image_format)
2206 
2207     def _volume_snapshot_update_status(self, context, snapshot_id, status):
2208         """Send a snapshot status update to Cinder.
2209 
2210         This method captures and logs exceptions that occur
2211         since callers cannot do anything useful with these exceptions.
2212 
2213         Operations on the Cinder side waiting for this will time out if
2214         a failure occurs sending the update.
2215 
2216         :param context: security context
2217         :param snapshot_id: id of snapshot being updated
2218         :param status: new status value
2219 
2220         """
2221 
2222         try:
2223             self._volume_api.update_snapshot_status(context,
2224                                                     snapshot_id,
2225                                                     status)
2226         except Exception:
2227             LOG.exception(_('Failed to send updated snapshot status '
2228                             'to volume service.'))
2229 
2230     def _volume_snapshot_create(self, context, instance, guest,
2231                                 volume_id, new_file):
2232         """Perform volume snapshot.
2233 
2234            :param guest: VM that volume is attached to
2235            :param volume_id: volume UUID to snapshot
2236            :param new_file: relative path to new qcow2 file present on share
2237 
2238         """
2239         xml = guest.get_xml_desc()
2240         xml_doc = etree.fromstring(xml)
2241 
2242         device_info = vconfig.LibvirtConfigGuest()
2243         device_info.parse_dom(xml_doc)
2244 
2245         disks_to_snap = []          # to be snapshotted by libvirt
2246         network_disks_to_snap = []  # network disks (netfs, etc.)
2247         disks_to_skip = []          # local disks not snapshotted
2248 
2249         for guest_disk in device_info.devices:
2250             if (guest_disk.root_name != 'disk'):
2251                 continue
2252 
2253             if (guest_disk.target_dev is None):
2254                 continue
2255 
2256             if (guest_disk.serial is None or guest_disk.serial != volume_id):
2257                 disks_to_skip.append(guest_disk.target_dev)
2258                 continue
2259 
2260             # disk is a Cinder volume with the correct volume_id
2261 
2262             disk_info = {
2263                 'dev': guest_disk.target_dev,
2264                 'serial': guest_disk.serial,
2265                 'current_file': guest_disk.source_path,
2266                 'source_protocol': guest_disk.source_protocol,
2267                 'source_name': guest_disk.source_name,
2268                 'source_hosts': guest_disk.source_hosts,
2269                 'source_ports': guest_disk.source_ports
2270             }
2271 
2272             # Determine path for new_file based on current path
2273             if disk_info['current_file'] is not None:
2274                 current_file = disk_info['current_file']
2275                 new_file_path = os.path.join(os.path.dirname(current_file),
2276                                              new_file)
2277                 disks_to_snap.append((current_file, new_file_path))
2278             # NOTE(mriedem): This used to include a check for gluster in
2279             # addition to netfs since they were added together. Support for
2280             # gluster was removed in the 16.0.0 Pike release. It is unclear,
2281             # however, if other volume drivers rely on the netfs disk source
2282             # protocol.
2283             elif disk_info['source_protocol'] == 'netfs':
2284                 network_disks_to_snap.append((disk_info, new_file))
2285 
2286         if not disks_to_snap and not network_disks_to_snap:
2287             msg = _('Found no disk to snapshot.')
2288             raise exception.InternalError(msg)
2289 
2290         snapshot = vconfig.LibvirtConfigGuestSnapshot()
2291 
2292         for current_name, new_filename in disks_to_snap:
2293             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2294             snap_disk.name = current_name
2295             snap_disk.source_path = new_filename
2296             snap_disk.source_type = 'file'
2297             snap_disk.snapshot = 'external'
2298             snap_disk.driver_name = 'qcow2'
2299 
2300             snapshot.add_disk(snap_disk)
2301 
2302         for disk_info, new_filename in network_disks_to_snap:
2303             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2304             snap_disk.name = disk_info['dev']
2305             snap_disk.source_type = 'network'
2306             snap_disk.source_protocol = disk_info['source_protocol']
2307             snap_disk.snapshot = 'external'
2308             snap_disk.source_path = new_filename
2309             old_dir = disk_info['source_name'].split('/')[0]
2310             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
2311             snap_disk.source_hosts = disk_info['source_hosts']
2312             snap_disk.source_ports = disk_info['source_ports']
2313 
2314             snapshot.add_disk(snap_disk)
2315 
2316         for dev in disks_to_skip:
2317             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2318             snap_disk.name = dev
2319             snap_disk.snapshot = 'no'
2320 
2321             snapshot.add_disk(snap_disk)
2322 
2323         snapshot_xml = snapshot.to_xml()
2324         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
2325 
2326         image_meta = instance.image_meta
2327         try:
2328             # Check to see if we can quiesce the guest before taking the
2329             # snapshot.
2330             self._can_quiesce(instance, image_meta)
2331             try:
2332                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2333                                reuse_ext=True, quiesce=True)
2334                 return
2335             except libvirt.libvirtError:
2336                 # If the image says that quiesce is required then we fail.
2337                 if self._requires_quiesce(image_meta):
2338                     raise
2339                 LOG.exception(_('Unable to create quiesced VM snapshot, '
2340                                 'attempting again with quiescing disabled.'),
2341                               instance=instance)
2342         except (exception.InstanceQuiesceNotSupported,
2343                 exception.QemuGuestAgentNotEnabled) as err:
2344             # If the image says that quiesce is required then we need to fail.
2345             if self._requires_quiesce(image_meta):
2346                 raise
2347             LOG.info('Skipping quiescing instance: %(reason)s.',
2348                      {'reason': err}, instance=instance)
2349 
2350         try:
2351             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2352                            reuse_ext=True, quiesce=False)
2353         except libvirt.libvirtError:
2354             LOG.exception(_('Unable to create VM snapshot, '
2355                             'failing volume_snapshot operation.'),
2356                           instance=instance)
2357 
2358             raise
2359 
2360     def _volume_refresh_connection_info(self, context, instance, volume_id):
2361         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
2362                   context, volume_id, instance.uuid)
2363 
2364         driver_bdm = driver_block_device.convert_volume(bdm)
2365         if driver_bdm:
2366             driver_bdm.refresh_connection_info(context, instance,
2367                                                self._volume_api, self)
2368 
2369     def volume_snapshot_create(self, context, instance, volume_id,
2370                                create_info):
2371         """Create snapshots of a Cinder volume via libvirt.
2372 
2373         :param instance: VM instance object reference
2374         :param volume_id: id of volume being snapshotted
2375         :param create_info: dict of information used to create snapshots
2376                      - snapshot_id : ID of snapshot
2377                      - type : qcow2 / <other>
2378                      - new_file : qcow2 file created by Cinder which
2379                      becomes the VM's active image after
2380                      the snapshot is complete
2381         """
2382 
2383         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
2384                   {'c_info': create_info}, instance=instance)
2385 
2386         try:
2387             guest = self._host.get_guest(instance)
2388         except exception.InstanceNotFound:
2389             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2390 
2391         if create_info['type'] != 'qcow2':
2392             msg = _('Unknown type: %s') % create_info['type']
2393             raise exception.InternalError(msg)
2394 
2395         snapshot_id = create_info.get('snapshot_id', None)
2396         if snapshot_id is None:
2397             msg = _('snapshot_id required in create_info')
2398             raise exception.InternalError(msg)
2399 
2400         try:
2401             self._volume_snapshot_create(context, instance, guest,
2402                                          volume_id, create_info['new_file'])
2403         except Exception:
2404             with excutils.save_and_reraise_exception():
2405                 LOG.exception(_('Error occurred during '
2406                                 'volume_snapshot_create, '
2407                                 'sending error status to Cinder.'),
2408                               instance=instance)
2409                 self._volume_snapshot_update_status(
2410                     context, snapshot_id, 'error')
2411 
2412         self._volume_snapshot_update_status(
2413             context, snapshot_id, 'creating')
2414 
2415         def _wait_for_snapshot():
2416             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
2417 
2418             if snapshot.get('status') != 'creating':
2419                 self._volume_refresh_connection_info(context, instance,
2420                                                      volume_id)
2421                 raise loopingcall.LoopingCallDone()
2422 
2423         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
2424         timer.start(interval=0.5).wait()
2425 
2426     @staticmethod
2427     def _rebase_with_qemu_img(guest, device, active_disk_object,
2428                               rebase_base):
2429         """Rebase a device tied to a guest using qemu-img.
2430 
2431         :param guest:the Guest which owns the device being rebased
2432         :type guest: nova.virt.libvirt.guest.Guest
2433         :param device: the guest block device to rebase
2434         :type device: nova.virt.libvirt.guest.BlockDevice
2435         :param active_disk_object: the guest block device to rebase
2436         :type active_disk_object: nova.virt.libvirt.config.\
2437                                     LibvirtConfigGuestDisk
2438         :param rebase_base: the new parent in the backing chain
2439         :type rebase_base: None or string
2440         """
2441 
2442         # It's unsure how well qemu-img handles network disks for
2443         # every protocol. So let's be safe.
2444         active_protocol = active_disk_object.source_protocol
2445         if active_protocol is not None:
2446             msg = _("Something went wrong when deleting a volume snapshot: "
2447                     "rebasing a %(protocol)s network disk using qemu-img "
2448                     "has not been fully tested") % {'protocol':
2449                     active_protocol}
2450             LOG.error(msg)
2451             raise exception.InternalError(msg)
2452 
2453         if rebase_base is None:
2454             # If backing_file is specified as "" (the empty string), then
2455             # the image is rebased onto no backing file (i.e. it will exist
2456             # independently of any backing file).
2457             backing_file = ""
2458             qemu_img_extra_arg = []
2459         else:
2460             # If the rebased image is going to have a backing file then
2461             # explicitly set the backing file format to avoid any security
2462             # concerns related to file format auto detection.
2463             backing_file = rebase_base
2464             b_file_fmt = images.qemu_img_info(backing_file).file_format
2465             qemu_img_extra_arg = ['-F', b_file_fmt]
2466 
2467         qemu_img_extra_arg.append(active_disk_object.source_path)
2468         utils.execute("qemu-img", "rebase", "-b", backing_file,
2469                       *qemu_img_extra_arg)
2470 
2471     def _volume_snapshot_delete(self, context, instance, volume_id,
2472                                 snapshot_id, delete_info=None):
2473         """Note:
2474             if file being merged into == active image:
2475                 do a blockRebase (pull) operation
2476             else:
2477                 do a blockCommit operation
2478             Files must be adjacent in snap chain.
2479 
2480         :param instance: instance object reference
2481         :param volume_id: volume UUID
2482         :param snapshot_id: snapshot UUID (unused currently)
2483         :param delete_info: {
2484             'type':              'qcow2',
2485             'file_to_merge':     'a.img',
2486             'merge_target_file': 'b.img' or None (if merging file_to_merge into
2487                                                   active image)
2488           }
2489         """
2490 
2491         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
2492                   instance=instance)
2493 
2494         if delete_info['type'] != 'qcow2':
2495             msg = _('Unknown delete_info type %s') % delete_info['type']
2496             raise exception.InternalError(msg)
2497 
2498         try:
2499             guest = self._host.get_guest(instance)
2500         except exception.InstanceNotFound:
2501             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2502 
2503         # Find dev name
2504         my_dev = None
2505         active_disk = None
2506 
2507         xml = guest.get_xml_desc()
2508         xml_doc = etree.fromstring(xml)
2509 
2510         device_info = vconfig.LibvirtConfigGuest()
2511         device_info.parse_dom(xml_doc)
2512 
2513         active_disk_object = None
2514 
2515         for guest_disk in device_info.devices:
2516             if (guest_disk.root_name != 'disk'):
2517                 continue
2518 
2519             if (guest_disk.target_dev is None or guest_disk.serial is None):
2520                 continue
2521 
2522             if guest_disk.serial == volume_id:
2523                 my_dev = guest_disk.target_dev
2524 
2525                 active_disk = guest_disk.source_path
2526                 active_protocol = guest_disk.source_protocol
2527                 active_disk_object = guest_disk
2528                 break
2529 
2530         if my_dev is None or (active_disk is None and active_protocol is None):
2531             LOG.debug('Domain XML: %s', xml, instance=instance)
2532             msg = (_('Disk with id: %s not found attached to instance.')
2533                    % volume_id)
2534             raise exception.InternalError(msg)
2535 
2536         LOG.debug("found device at %s", my_dev, instance=instance)
2537 
2538         def _get_snap_dev(filename, backing_store):
2539             if filename is None:
2540                 msg = _('filename cannot be None')
2541                 raise exception.InternalError(msg)
2542 
2543             # libgfapi delete
2544             LOG.debug("XML: %s", xml)
2545 
2546             LOG.debug("active disk object: %s", active_disk_object)
2547 
2548             # determine reference within backing store for desired image
2549             filename_to_merge = filename
2550             matched_name = None
2551             b = backing_store
2552             index = None
2553 
2554             current_filename = active_disk_object.source_name.split('/')[1]
2555             if current_filename == filename_to_merge:
2556                 return my_dev + '[0]'
2557 
2558             while b is not None:
2559                 source_filename = b.source_name.split('/')[1]
2560                 if source_filename == filename_to_merge:
2561                     LOG.debug('found match: %s', b.source_name)
2562                     matched_name = b.source_name
2563                     index = b.index
2564                     break
2565 
2566                 b = b.backing_store
2567 
2568             if matched_name is None:
2569                 msg = _('no match found for %s') % (filename_to_merge)
2570                 raise exception.InternalError(msg)
2571 
2572             LOG.debug('index of match (%s) is %s', b.source_name, index)
2573 
2574             my_snap_dev = '%s[%s]' % (my_dev, index)
2575             return my_snap_dev
2576 
2577         if delete_info['merge_target_file'] is None:
2578             # pull via blockRebase()
2579 
2580             # Merge the most recent snapshot into the active image
2581 
2582             rebase_disk = my_dev
2583             rebase_base = delete_info['file_to_merge']  # often None
2584             if (active_protocol is not None) and (rebase_base is not None):
2585                 rebase_base = _get_snap_dev(rebase_base,
2586                                             active_disk_object.backing_store)
2587 
2588             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2589             # and when available this flag _must_ be used to ensure backing
2590             # paths are maintained relative by qemu.
2591             #
2592             # If _RELATIVE flag not found, continue with old behaviour
2593             # (relative backing path seems to work for this case)
2594             try:
2595                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
2596                 relative = rebase_base is not None
2597             except AttributeError:
2598                 LOG.warning(
2599                     "Relative blockrebase support was not detected. "
2600                     "Continuing with old behaviour.")
2601                 relative = False
2602 
2603             LOG.debug(
2604                 'disk: %(disk)s, base: %(base)s, '
2605                 'bw: %(bw)s, relative: %(relative)s',
2606                 {'disk': rebase_disk,
2607                  'base': rebase_base,
2608                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
2609                  'relative': str(relative)}, instance=instance)
2610 
2611             dev = guest.get_block_device(rebase_disk)
2612             if guest.is_active():
2613                 result = dev.rebase(rebase_base, relative=relative)
2614                 if result == 0:
2615                     LOG.debug('blockRebase started successfully',
2616                               instance=instance)
2617 
2618                 while not dev.is_job_complete():
2619                     LOG.debug('waiting for blockRebase job completion',
2620                               instance=instance)
2621                     time.sleep(0.5)
2622 
2623             # If the guest is not running libvirt won't do a blockRebase.
2624             # In that case, let's ask qemu-img to rebase the disk.
2625             else:
2626                 LOG.debug('Guest is not running so doing a block rebase '
2627                           'using "qemu-img rebase"', instance=instance)
2628                 self._rebase_with_qemu_img(guest, dev, active_disk_object,
2629                                            rebase_base)
2630 
2631         else:
2632             # commit with blockCommit()
2633             my_snap_base = None
2634             my_snap_top = None
2635             commit_disk = my_dev
2636 
2637             if active_protocol is not None:
2638                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
2639                                              active_disk_object.backing_store)
2640                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
2641                                             active_disk_object.backing_store)
2642 
2643             commit_base = my_snap_base or delete_info['merge_target_file']
2644             commit_top = my_snap_top or delete_info['file_to_merge']
2645 
2646             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
2647                       'commit_base=%(commit_base)s '
2648                       'commit_top=%(commit_top)s ',
2649                       {'commit_disk': commit_disk,
2650                        'commit_base': commit_base,
2651                        'commit_top': commit_top}, instance=instance)
2652 
2653             dev = guest.get_block_device(commit_disk)
2654             result = dev.commit(commit_base, commit_top, relative=True)
2655 
2656             if result == 0:
2657                 LOG.debug('blockCommit started successfully',
2658                           instance=instance)
2659 
2660             while not dev.is_job_complete():
2661                 LOG.debug('waiting for blockCommit job completion',
2662                           instance=instance)
2663                 time.sleep(0.5)
2664 
2665     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
2666                                delete_info):
2667         try:
2668             self._volume_snapshot_delete(context, instance, volume_id,
2669                                          snapshot_id, delete_info=delete_info)
2670         except Exception:
2671             with excutils.save_and_reraise_exception():
2672                 LOG.exception(_('Error occurred during '
2673                                 'volume_snapshot_delete, '
2674                                 'sending error status to Cinder.'),
2675                               instance=instance)
2676                 self._volume_snapshot_update_status(
2677                     context, snapshot_id, 'error_deleting')
2678 
2679         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
2680         self._volume_refresh_connection_info(context, instance, volume_id)
2681 
2682     def reboot(self, context, instance, network_info, reboot_type,
2683                block_device_info=None, bad_volumes_callback=None):
2684         """Reboot a virtual machine, given an instance reference."""
2685         if reboot_type == 'SOFT':
2686             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
2687             try:
2688                 soft_reboot_success = self._soft_reboot(instance)
2689             except libvirt.libvirtError as e:
2690                 LOG.debug("Instance soft reboot failed: %s",
2691                           encodeutils.exception_to_unicode(e),
2692                           instance=instance)
2693                 soft_reboot_success = False
2694 
2695             if soft_reboot_success:
2696                 LOG.info("Instance soft rebooted successfully.",
2697                          instance=instance)
2698                 return
2699             else:
2700                 LOG.warning("Failed to soft reboot instance. "
2701                             "Trying hard reboot.",
2702                             instance=instance)
2703         return self._hard_reboot(context, instance, network_info,
2704                                  block_device_info)
2705 
2706     def _soft_reboot(self, instance):
2707         """Attempt to shutdown and restart the instance gracefully.
2708 
2709         We use shutdown and create here so we can return if the guest
2710         responded and actually rebooted. Note that this method only
2711         succeeds if the guest responds to acpi. Therefore we return
2712         success or failure so we can fall back to a hard reboot if
2713         necessary.
2714 
2715         :returns: True if the reboot succeeded
2716         """
2717         guest = self._host.get_guest(instance)
2718 
2719         state = guest.get_power_state(self._host)
2720         old_domid = guest.id
2721         # NOTE(vish): This check allows us to reboot an instance that
2722         #             is already shutdown.
2723         if state == power_state.RUNNING:
2724             guest.shutdown()
2725         # NOTE(vish): This actually could take slightly longer than the
2726         #             FLAG defines depending on how long the get_info
2727         #             call takes to return.
2728         self._prepare_pci_devices_for_use(
2729             pci_manager.get_instance_pci_devs(instance, 'all'))
2730         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
2731             guest = self._host.get_guest(instance)
2732 
2733             state = guest.get_power_state(self._host)
2734             new_domid = guest.id
2735 
2736             # NOTE(ivoks): By checking domain IDs, we make sure we are
2737             #              not recreating domain that's already running.
2738             if old_domid != new_domid:
2739                 if state in [power_state.SHUTDOWN,
2740                              power_state.CRASHED]:
2741                     LOG.info("Instance shutdown successfully.",
2742                              instance=instance)
2743                     self._create_domain(domain=guest._domain)
2744                     timer = loopingcall.FixedIntervalLoopingCall(
2745                         self._wait_for_running, instance)
2746                     timer.start(interval=0.5).wait()
2747                     return True
2748                 else:
2749                     LOG.info("Instance may have been rebooted during soft "
2750                              "reboot, so return now.", instance=instance)
2751                     return True
2752             greenthread.sleep(1)
2753         return False
2754 
2755     def _hard_reboot(self, context, instance, network_info,
2756                      block_device_info=None):
2757         """Reboot a virtual machine, given an instance reference.
2758 
2759         Performs a Libvirt reset (if supported) on the domain.
2760 
2761         If Libvirt reset is unavailable this method actually destroys and
2762         re-creates the domain to ensure the reboot happens, as the guest
2763         OS cannot ignore this action.
2764         """
2765         # NOTE(sbauza): Since we undefine the guest XML when destroying, we
2766         # need to remember the existing mdevs for reusing them.
2767         mdevs = self._get_all_assigned_mediated_devices(instance)
2768         mdevs = list(mdevs.keys())
2769         # NOTE(mdbooth): In addition to performing a hard reboot of the domain,
2770         # the hard reboot operation is relied upon by operators to be an
2771         # automated attempt to fix as many things as possible about a
2772         # non-functioning instance before resorting to manual intervention.
2773         # With this goal in mind, we tear down all the aspects of an instance
2774         # we can here without losing data. This allows us to re-initialise from
2775         # scratch, and hopefully fix, most aspects of a non-functioning guest.
2776         self.destroy(context, instance, network_info, destroy_disks=False,
2777                      block_device_info=block_device_info)
2778 
2779         # Convert the system metadata to image metadata
2780         # NOTE(mdbooth): This is a workaround for stateless Nova compute
2781         #                https://bugs.launchpad.net/nova/+bug/1349978
2782         instance_dir = libvirt_utils.get_instance_path(instance)
2783         fileutils.ensure_tree(instance_dir)
2784 
2785         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2786                                             instance,
2787                                             instance.image_meta,
2788                                             block_device_info)
2789         # NOTE(vish): This could generate the wrong device_format if we are
2790         #             using the raw backend and the images don't exist yet.
2791         #             The create_images_and_backing below doesn't properly
2792         #             regenerate raw backend images, however, so when it
2793         #             does we need to (re)generate the xml after the images
2794         #             are in place.
2795         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2796                                   instance.image_meta,
2797                                   block_device_info=block_device_info,
2798                                   mdevs=mdevs)
2799 
2800         # NOTE(mdbooth): context.auth_token will not be set when we call
2801         #                _hard_reboot from resume_state_on_host_boot()
2802         if context.auth_token is not None:
2803             # NOTE (rmk): Re-populate any missing backing files.
2804             config = vconfig.LibvirtConfigGuest()
2805             config.parse_str(xml)
2806             backing_disk_info = self._get_instance_disk_info_from_config(
2807                 config, block_device_info)
2808             self._create_images_and_backing(context, instance, instance_dir,
2809                                             backing_disk_info)
2810 
2811         # Initialize all the necessary networking, block devices and
2812         # start the instance.
2813         # NOTE(melwitt): Pass vifs_already_plugged=True here even though we've
2814         # unplugged vifs earlier. The behavior of neutron plug events depends
2815         # on which vif type we're using and we are working with a stale network
2816         # info cache here, so won't rely on waiting for neutron plug events.
2817         # vifs_already_plugged=True means "do not wait for neutron plug events"
2818         self._create_domain_and_network(context, xml, instance, network_info,
2819                                         block_device_info=block_device_info,
2820                                         vifs_already_plugged=True)
2821         self._prepare_pci_devices_for_use(
2822             pci_manager.get_instance_pci_devs(instance, 'all'))
2823 
2824         def _wait_for_reboot():
2825             """Called at an interval until the VM is running again."""
2826             state = self.get_info(instance).state
2827 
2828             if state == power_state.RUNNING:
2829                 LOG.info("Instance rebooted successfully.",
2830                          instance=instance)
2831                 raise loopingcall.LoopingCallDone()
2832 
2833         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
2834         timer.start(interval=0.5).wait()
2835 
2836     def pause(self, instance):
2837         """Pause VM instance."""
2838         self._host.get_guest(instance).pause()
2839 
2840     def unpause(self, instance):
2841         """Unpause paused VM instance."""
2842         guest = self._host.get_guest(instance)
2843         guest.resume()
2844         guest.sync_guest_time()
2845 
2846     def _clean_shutdown(self, instance, timeout, retry_interval):
2847         """Attempt to shutdown the instance gracefully.
2848 
2849         :param instance: The instance to be shutdown
2850         :param timeout: How long to wait in seconds for the instance to
2851                         shutdown
2852         :param retry_interval: How often in seconds to signal the instance
2853                                to shutdown while waiting
2854 
2855         :returns: True if the shutdown succeeded
2856         """
2857 
2858         # List of states that represent a shutdown instance
2859         SHUTDOWN_STATES = [power_state.SHUTDOWN,
2860                            power_state.CRASHED]
2861 
2862         try:
2863             guest = self._host.get_guest(instance)
2864         except exception.InstanceNotFound:
2865             # If the instance has gone then we don't need to
2866             # wait for it to shutdown
2867             return True
2868 
2869         state = guest.get_power_state(self._host)
2870         if state in SHUTDOWN_STATES:
2871             LOG.info("Instance already shutdown.", instance=instance)
2872             return True
2873 
2874         LOG.debug("Shutting down instance from state %s", state,
2875                   instance=instance)
2876         guest.shutdown()
2877         retry_countdown = retry_interval
2878 
2879         for sec in range(timeout):
2880 
2881             guest = self._host.get_guest(instance)
2882             state = guest.get_power_state(self._host)
2883 
2884             if state in SHUTDOWN_STATES:
2885                 LOG.info("Instance shutdown successfully after %d seconds.",
2886                          sec, instance=instance)
2887                 return True
2888 
2889             # Note(PhilD): We can't assume that the Guest was able to process
2890             #              any previous shutdown signal (for example it may
2891             #              have still been startingup, so within the overall
2892             #              timeout we re-trigger the shutdown every
2893             #              retry_interval
2894             if retry_countdown == 0:
2895                 retry_countdown = retry_interval
2896                 # Instance could shutdown at any time, in which case we
2897                 # will get an exception when we call shutdown
2898                 try:
2899                     LOG.debug("Instance in state %s after %d seconds - "
2900                               "resending shutdown", state, sec,
2901                               instance=instance)
2902                     guest.shutdown()
2903                 except libvirt.libvirtError:
2904                     # Assume this is because its now shutdown, so loop
2905                     # one more time to clean up.
2906                     LOG.debug("Ignoring libvirt exception from shutdown "
2907                               "request.", instance=instance)
2908                     continue
2909             else:
2910                 retry_countdown -= 1
2911 
2912             time.sleep(1)
2913 
2914         LOG.info("Instance failed to shutdown in %d seconds.",
2915                  timeout, instance=instance)
2916         return False
2917 
2918     def power_off(self, instance, timeout=0, retry_interval=0):
2919         """Power off the specified instance."""
2920         if timeout:
2921             self._clean_shutdown(instance, timeout, retry_interval)
2922         self._destroy(instance)
2923 
2924     def power_on(self, context, instance, network_info,
2925                  block_device_info=None):
2926         """Power on the specified instance."""
2927         # We use _hard_reboot here to ensure that all backing files,
2928         # network, and block device connections, etc. are established
2929         # and available before we attempt to start the instance.
2930         self._hard_reboot(context, instance, network_info, block_device_info)
2931 
2932     def trigger_crash_dump(self, instance):
2933 
2934         """Trigger crash dump by injecting an NMI to the specified instance."""
2935         try:
2936             self._host.get_guest(instance).inject_nmi()
2937         except libvirt.libvirtError as ex:
2938             error_code = ex.get_error_code()
2939 
2940             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
2941                 raise exception.TriggerCrashDumpNotSupported()
2942             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
2943                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
2944 
2945             LOG.exception(_('Error from libvirt while injecting an NMI to '
2946                             '%(instance_uuid)s: '
2947                             '[Error Code %(error_code)s] %(ex)s'),
2948                           {'instance_uuid': instance.uuid,
2949                            'error_code': error_code, 'ex': ex})
2950             raise
2951 
2952     def suspend(self, context, instance):
2953         """Suspend the specified instance."""
2954         guest = self._host.get_guest(instance)
2955 
2956         self._detach_pci_devices(guest,
2957             pci_manager.get_instance_pci_devs(instance))
2958         self._detach_direct_passthrough_ports(context, instance, guest)
2959         self._detach_mediated_devices(guest)
2960         guest.save_memory_state()
2961 
2962     def resume(self, context, instance, network_info, block_device_info=None):
2963         """resume the specified instance."""
2964         xml = self._get_existing_domain_xml(instance, network_info,
2965                                             block_device_info)
2966         guest = self._create_domain_and_network(context, xml, instance,
2967                            network_info, block_device_info=block_device_info,
2968                            vifs_already_plugged=True)
2969         self._attach_pci_devices(guest,
2970             pci_manager.get_instance_pci_devs(instance))
2971         self._attach_direct_passthrough_ports(
2972             context, instance, guest, network_info)
2973         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
2974                                                      instance)
2975         timer.start(interval=0.5).wait()
2976         guest.sync_guest_time()
2977 
2978     def resume_state_on_host_boot(self, context, instance, network_info,
2979                                   block_device_info=None):
2980         """resume guest state when a host is booted."""
2981         # Check if the instance is running already and avoid doing
2982         # anything if it is.
2983         try:
2984             guest = self._host.get_guest(instance)
2985             state = guest.get_power_state(self._host)
2986 
2987             ignored_states = (power_state.RUNNING,
2988                               power_state.SUSPENDED,
2989                               power_state.NOSTATE,
2990                               power_state.PAUSED)
2991 
2992             if state in ignored_states:
2993                 return
2994         except (exception.InternalError, exception.InstanceNotFound):
2995             pass
2996 
2997         # Instance is not up and could be in an unknown state.
2998         # Be as absolute as possible about getting it back into
2999         # a known and running state.
3000         self._hard_reboot(context, instance, network_info, block_device_info)
3001 
3002     def rescue(self, context, instance, network_info, image_meta,
3003                rescue_password):
3004         """Loads a VM using rescue images.
3005 
3006         A rescue is normally performed when something goes wrong with the
3007         primary images and data needs to be corrected/recovered. Rescuing
3008         should not edit or over-ride the original image, only allow for
3009         data recovery.
3010 
3011         """
3012         instance_dir = libvirt_utils.get_instance_path(instance)
3013         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
3014         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3015         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
3016 
3017         rescue_image_id = None
3018         if image_meta.obj_attr_is_set("id"):
3019             rescue_image_id = image_meta.id
3020 
3021         rescue_images = {
3022             'image_id': (rescue_image_id or
3023                         CONF.libvirt.rescue_image_id or instance.image_ref),
3024             'kernel_id': (CONF.libvirt.rescue_kernel_id or
3025                           instance.kernel_id),
3026             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
3027                            instance.ramdisk_id),
3028         }
3029         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3030                                             instance,
3031                                             image_meta,
3032                                             rescue=True)
3033         injection_info = InjectionInfo(network_info=network_info,
3034                                        admin_pass=rescue_password,
3035                                        files=None)
3036         gen_confdrive = functools.partial(self._create_configdrive,
3037                                           context, instance, injection_info,
3038                                           rescue=True)
3039         self._create_image(context, instance, disk_info['mapping'],
3040                            injection_info=injection_info, suffix='.rescue',
3041                            disk_images=rescue_images)
3042         xml = self._get_guest_xml(context, instance, network_info, disk_info,
3043                                   image_meta, rescue=rescue_images)
3044         self._destroy(instance)
3045         self._create_domain(xml, post_xml_callback=gen_confdrive)
3046 
3047     def unrescue(self, instance, network_info):
3048         """Reboot the VM which is being rescued back into primary images.
3049         """
3050         instance_dir = libvirt_utils.get_instance_path(instance)
3051         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3052         xml = libvirt_utils.load_file(unrescue_xml_path)
3053         guest = self._host.get_guest(instance)
3054 
3055         # TODO(sahid): We are converting all calls from a
3056         # virDomain object to use nova.virt.libvirt.Guest.
3057         # We should be able to remove virt_dom at the end.
3058         virt_dom = guest._domain
3059         self._destroy(instance)
3060         self._create_domain(xml, virt_dom)
3061         os.unlink(unrescue_xml_path)
3062         rescue_files = os.path.join(instance_dir, "*.rescue")
3063         for rescue_file in glob.iglob(rescue_files):
3064             if os.path.isdir(rescue_file):
3065                 shutil.rmtree(rescue_file)
3066             else:
3067                 os.unlink(rescue_file)
3068         # cleanup rescue volume
3069         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
3070                                 if lvmdisk.endswith('.rescue')])
3071         if CONF.libvirt.images_type == 'rbd':
3072             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
3073                                       disk.endswith('.rescue'))
3074             LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
3075 
3076     def poll_rebooting_instances(self, timeout, instances):
3077         pass
3078 
3079     # NOTE(ilyaalekseyev): Implementation like in multinics
3080     # for xenapi(tr3buchet)
3081     def spawn(self, context, instance, image_meta, injected_files,
3082               admin_password, allocations, network_info=None,
3083               block_device_info=None):
3084         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3085                                             instance,
3086                                             image_meta,
3087                                             block_device_info)
3088         injection_info = InjectionInfo(network_info=network_info,
3089                                        files=injected_files,
3090                                        admin_pass=admin_password)
3091         gen_confdrive = functools.partial(self._create_configdrive,
3092                                           context, instance,
3093                                           injection_info)
3094         self._create_image(context, instance, disk_info['mapping'],
3095                            injection_info=injection_info,
3096                            block_device_info=block_device_info)
3097 
3098         # Required by Quobyte CI
3099         self._ensure_console_log_for_instance(instance)
3100 
3101         # Does the guest need to be assigned some vGPU mediated devices ?
3102         mdevs = self._allocate_mdevs(allocations)
3103 
3104         xml = self._get_guest_xml(context, instance, network_info,
3105                                   disk_info, image_meta,
3106                                   block_device_info=block_device_info,
3107                                   mdevs=mdevs)
3108         self._create_domain_and_network(
3109             context, xml, instance, network_info,
3110             block_device_info=block_device_info,
3111             post_xml_callback=gen_confdrive,
3112             destroy_disks_on_failure=True)
3113         LOG.debug("Instance is running", instance=instance)
3114 
3115         def _wait_for_boot():
3116             """Called at an interval until the VM is running."""
3117             state = self.get_info(instance).state
3118 
3119             if state == power_state.RUNNING:
3120                 LOG.info("Instance spawned successfully.", instance=instance)
3121                 raise loopingcall.LoopingCallDone()
3122 
3123         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
3124         timer.start(interval=0.5).wait()
3125 
3126     def _get_console_output_file(self, instance, console_log):
3127         bytes_to_read = MAX_CONSOLE_BYTES
3128         log_data = b""  # The last N read bytes
3129         i = 0  # in case there is a log rotation (like "virtlogd")
3130         path = console_log
3131 
3132         while bytes_to_read > 0 and os.path.exists(path):
3133             read_log_data, remaining = nova.privsep.path.last_bytes(
3134                                         path, bytes_to_read)
3135             # We need the log file content in chronological order,
3136             # that's why we *prepend* the log data.
3137             log_data = read_log_data + log_data
3138 
3139             # Prep to read the next file in the chain
3140             bytes_to_read -= len(read_log_data)
3141             path = console_log + "." + str(i)
3142             i += 1
3143 
3144             if remaining > 0:
3145                 LOG.info('Truncated console log returned, '
3146                          '%d bytes ignored', remaining, instance=instance)
3147         return log_data
3148 
3149     def get_console_output(self, context, instance):
3150         guest = self._host.get_guest(instance)
3151 
3152         xml = guest.get_xml_desc()
3153         tree = etree.fromstring(xml)
3154 
3155         # If the guest has a console logging to a file prefer to use that
3156         file_consoles = tree.findall("./devices/console[@type='file']")
3157         if file_consoles:
3158             for file_console in file_consoles:
3159                 source_node = file_console.find('./source')
3160                 if source_node is None:
3161                     continue
3162                 path = source_node.get("path")
3163                 if not path:
3164                     continue
3165 
3166                 if not os.path.exists(path):
3167                     LOG.info('Instance is configured with a file console, '
3168                              'but the backing file is not (yet?) present',
3169                              instance=instance)
3170                     return ""
3171 
3172                 return self._get_console_output_file(instance, path)
3173 
3174         # Try 'pty' types
3175         pty_consoles = tree.findall("./devices/console[@type='pty']")
3176         if pty_consoles:
3177             for pty_console in pty_consoles:
3178                 source_node = pty_console.find('./source')
3179                 if source_node is None:
3180                     continue
3181                 pty = source_node.get("path")
3182                 if not pty:
3183                     continue
3184                 break
3185             else:
3186                 raise exception.ConsoleNotAvailable()
3187         else:
3188             raise exception.ConsoleNotAvailable()
3189 
3190         console_log = self._get_console_log_path(instance)
3191         data = nova.privsep.libvirt.readpty(pty)
3192 
3193         # NOTE(markus_z): The virt_types kvm and qemu are the only ones
3194         # which create a dedicated file device for the console logging.
3195         # Other virt_types like xen, lxc, uml, parallels depend on the
3196         # flush of that pty device into the "console.log" file to ensure
3197         # that a series of "get_console_output" calls return the complete
3198         # content even after rebooting a guest.
3199         nova.privsep.path.writefile(console_log, 'a+', data)
3200         return self._get_console_output_file(instance, console_log)
3201 
3202     def get_host_ip_addr(self):
3203         ips = compute_utils.get_machine_ips()
3204         if CONF.my_ip not in ips:
3205             LOG.warning('my_ip address (%(my_ip)s) was not found on '
3206                         'any of the interfaces: %(ifaces)s',
3207                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
3208         return CONF.my_ip
3209 
3210     def get_vnc_console(self, context, instance):
3211         def get_vnc_port_for_instance(instance_name):
3212             guest = self._host.get_guest(instance)
3213 
3214             xml = guest.get_xml_desc()
3215             xml_dom = etree.fromstring(xml)
3216 
3217             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
3218             if graphic is not None:
3219                 return graphic.get('port')
3220             # NOTE(rmk): We had VNC consoles enabled but the instance in
3221             # question is not actually listening for connections.
3222             raise exception.ConsoleTypeUnavailable(console_type='vnc')
3223 
3224         port = get_vnc_port_for_instance(instance.name)
3225         host = CONF.vnc.server_proxyclient_address
3226 
3227         return ctype.ConsoleVNC(host=host, port=port)
3228 
3229     def get_spice_console(self, context, instance):
3230         def get_spice_ports_for_instance(instance_name):
3231             guest = self._host.get_guest(instance)
3232 
3233             xml = guest.get_xml_desc()
3234             xml_dom = etree.fromstring(xml)
3235 
3236             graphic = xml_dom.find("./devices/graphics[@type='spice']")
3237             if graphic is not None:
3238                 return (graphic.get('port'), graphic.get('tlsPort'))
3239             # NOTE(rmk): We had Spice consoles enabled but the instance in
3240             # question is not actually listening for connections.
3241             raise exception.ConsoleTypeUnavailable(console_type='spice')
3242 
3243         ports = get_spice_ports_for_instance(instance.name)
3244         host = CONF.spice.server_proxyclient_address
3245 
3246         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
3247 
3248     def get_serial_console(self, context, instance):
3249         guest = self._host.get_guest(instance)
3250         for hostname, port in self._get_serial_ports_from_guest(
3251                 guest, mode='bind'):
3252             return ctype.ConsoleSerial(host=hostname, port=port)
3253         raise exception.ConsoleTypeUnavailable(console_type='serial')
3254 
3255     @staticmethod
3256     def _create_ephemeral(target, ephemeral_size,
3257                           fs_label, os_type, is_block_dev=False,
3258                           context=None, specified_fs=None,
3259                           vm_mode=None):
3260         if not is_block_dev:
3261             if (CONF.libvirt.virt_type == "parallels" and
3262                     vm_mode == fields.VMMode.EXE):
3263 
3264                 libvirt_utils.create_ploop_image('expanded', target,
3265                                                  '%dG' % ephemeral_size,
3266                                                  specified_fs)
3267                 return
3268             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
3269 
3270         # Run as root only for block devices.
3271         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
3272                       specified_fs=specified_fs)
3273 
3274     @staticmethod
3275     def _create_swap(target, swap_mb, context=None):
3276         """Create a swap file of specified size."""
3277         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
3278         nova.privsep.fs.unprivileged_mkfs('swap', target)
3279 
3280     @staticmethod
3281     def _get_console_log_path(instance):
3282         return os.path.join(libvirt_utils.get_instance_path(instance),
3283                             'console.log')
3284 
3285     def _ensure_console_log_for_instance(self, instance):
3286         # NOTE(mdbooth): Although libvirt will create this file for us
3287         # automatically when it starts, it will initially create it with
3288         # root ownership and then chown it depending on the configuration of
3289         # the domain it is launching. Quobyte CI explicitly disables the
3290         # chown by setting dynamic_ownership=0 in libvirt's config.
3291         # Consequently when the domain starts it is unable to write to its
3292         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
3293         #
3294         # To work around this, we create the file manually before starting
3295         # the domain so it has the same ownership as Nova. This works
3296         # for Quobyte CI because it is also configured to run qemu as the same
3297         # user as the Nova service. Installations which don't set
3298         # dynamic_ownership=0 are not affected because libvirt will always
3299         # correctly configure permissions regardless of initial ownership.
3300         #
3301         # Setting dynamic_ownership=0 is dubious and potentially broken in
3302         # more ways than console.log (see comment #22 on the above bug), so
3303         # Future Maintainer who finds this code problematic should check to see
3304         # if we still support it.
3305         console_file = self._get_console_log_path(instance)
3306         LOG.debug('Ensure instance console log exists: %s', console_file,
3307                   instance=instance)
3308         try:
3309             libvirt_utils.file_open(console_file, 'a').close()
3310         # NOTE(sfinucan): We can safely ignore permission issues here and
3311         # assume that it is libvirt that has taken ownership of this file.
3312         except IOError as ex:
3313             if ex.errno != errno.EACCES:
3314                 raise
3315             LOG.debug('Console file already exists: %s.', console_file)
3316 
3317     @staticmethod
3318     def _get_disk_config_image_type():
3319         # TODO(mikal): there is a bug here if images_type has
3320         # changed since creation of the instance, but I am pretty
3321         # sure that this bug already exists.
3322         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
3323 
3324     @staticmethod
3325     def _is_booted_from_volume(block_device_info):
3326         """Determines whether the VM is booting from volume
3327 
3328         Determines whether the block device info indicates that the VM
3329         is booting from a volume.
3330         """
3331         block_device_mapping = driver.block_device_info_get_mapping(
3332             block_device_info)
3333         return bool(block_device.get_root_bdm(block_device_mapping))
3334 
3335     def _inject_data(self, disk, instance, injection_info):
3336         """Injects data in a disk image
3337 
3338         Helper used for injecting data in a disk image file system.
3339 
3340         :param disk: The disk we're injecting into (an Image object)
3341         :param instance: The instance we're injecting into
3342         :param injection_info: Injection info
3343         """
3344         # Handles the partition need to be used.
3345         LOG.debug('Checking root disk injection %s',
3346                   str(injection_info), instance=instance)
3347         target_partition = None
3348         if not instance.kernel_id:
3349             target_partition = CONF.libvirt.inject_partition
3350             if target_partition == 0:
3351                 target_partition = None
3352         if CONF.libvirt.virt_type == 'lxc':
3353             target_partition = None
3354 
3355         # Handles the key injection.
3356         if CONF.libvirt.inject_key and instance.get('key_data'):
3357             key = str(instance.key_data)
3358         else:
3359             key = None
3360 
3361         # Handles the admin password injection.
3362         if not CONF.libvirt.inject_password:
3363             admin_pass = None
3364         else:
3365             admin_pass = injection_info.admin_pass
3366 
3367         # Handles the network injection.
3368         net = netutils.get_injected_network_template(
3369             injection_info.network_info,
3370             libvirt_virt_type=CONF.libvirt.virt_type)
3371 
3372         # Handles the metadata injection
3373         metadata = instance.get('metadata')
3374 
3375         if any((key, net, metadata, admin_pass, injection_info.files)):
3376             LOG.debug('Injecting %s', str(injection_info),
3377                       instance=instance)
3378             img_id = instance.image_ref
3379             try:
3380                 disk_api.inject_data(disk.get_model(self._conn),
3381                                      key, net, metadata, admin_pass,
3382                                      injection_info.files,
3383                                      partition=target_partition,
3384                                      mandatory=('files',))
3385             except Exception as e:
3386                 with excutils.save_and_reraise_exception():
3387                     LOG.error('Error injecting data into image '
3388                               '%(img_id)s (%(e)s)',
3389                               {'img_id': img_id, 'e': e},
3390                               instance=instance)
3391 
3392     # NOTE(sileht): many callers of this method assume that this
3393     # method doesn't fail if an image already exists but instead
3394     # think that it will be reused (ie: (live)-migration/resize)
3395     def _create_image(self, context, instance,
3396                       disk_mapping, injection_info=None, suffix='',
3397                       disk_images=None, block_device_info=None,
3398                       fallback_from_host=None,
3399                       ignore_bdi_for_swap=False):
3400         booted_from_volume = self._is_booted_from_volume(block_device_info)
3401 
3402         def image(fname, image_type=CONF.libvirt.images_type):
3403             return self.image_backend.by_name(instance,
3404                                               fname + suffix, image_type)
3405 
3406         def raw(fname):
3407             return image(fname, image_type='raw')
3408 
3409         # ensure directories exist and are writable
3410         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
3411 
3412         LOG.info('Creating image', instance=instance)
3413 
3414         inst_type = instance.get_flavor()
3415         swap_mb = 0
3416         if 'disk.swap' in disk_mapping:
3417             mapping = disk_mapping['disk.swap']
3418 
3419             if ignore_bdi_for_swap:
3420                 # This is a workaround to support legacy swap resizing,
3421                 # which does not touch swap size specified in bdm,
3422                 # but works with flavor specified size only.
3423                 # In this case we follow the legacy logic and ignore block
3424                 # device info completely.
3425                 # NOTE(ft): This workaround must be removed when a correct
3426                 # implementation of resize operation changing sizes in bdms is
3427                 # developed. Also at that stage we probably may get rid of
3428                 # the direct usage of flavor swap size here,
3429                 # leaving the work with bdm only.
3430                 swap_mb = inst_type['swap']
3431             else:
3432                 swap = driver.block_device_info_get_swap(block_device_info)
3433                 if driver.swap_is_usable(swap):
3434                     swap_mb = swap['swap_size']
3435                 elif (inst_type['swap'] > 0 and
3436                       not block_device.volume_in_mapping(
3437                         mapping['dev'], block_device_info)):
3438                     swap_mb = inst_type['swap']
3439 
3440             if swap_mb > 0:
3441                 if (CONF.libvirt.virt_type == "parallels" and
3442                         instance.vm_mode == fields.VMMode.EXE):
3443                     msg = _("Swap disk is not supported "
3444                             "for Virtuozzo container")
3445                     raise exception.Invalid(msg)
3446 
3447         if not disk_images:
3448             disk_images = {'image_id': instance.image_ref,
3449                            'kernel_id': instance.kernel_id,
3450                            'ramdisk_id': instance.ramdisk_id}
3451 
3452         if disk_images['kernel_id']:
3453             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
3454             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
3455                                 context=context,
3456                                 filename=fname,
3457                                 image_id=disk_images['kernel_id'])
3458             if disk_images['ramdisk_id']:
3459                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
3460                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
3461                                      context=context,
3462                                      filename=fname,
3463                                      image_id=disk_images['ramdisk_id'])
3464 
3465         if CONF.libvirt.virt_type == 'uml':
3466             # PONDERING(mikal): can I assume that root is UID zero in every
3467             # OS? Probably not.
3468             uid = pwd.getpwnam('root').pw_uid
3469             nova.privsep.path.chown(image('disk').path, uid=uid)
3470 
3471         self._create_and_inject_local_root(context, instance,
3472                                            booted_from_volume, suffix,
3473                                            disk_images, injection_info,
3474                                            fallback_from_host)
3475 
3476         # Lookup the filesystem type if required
3477         os_type_with_default = disk_api.get_fs_type_for_os_type(
3478             instance.os_type)
3479         # Generate a file extension based on the file system
3480         # type and the mkfs commands configured if any
3481         file_extension = disk_api.get_file_extension_for_os_type(
3482                                                           os_type_with_default)
3483 
3484         vm_mode = fields.VMMode.get_from_instance(instance)
3485         ephemeral_gb = instance.flavor.ephemeral_gb
3486         if 'disk.local' in disk_mapping:
3487             disk_image = image('disk.local')
3488             fn = functools.partial(self._create_ephemeral,
3489                                    fs_label='ephemeral0',
3490                                    os_type=instance.os_type,
3491                                    is_block_dev=disk_image.is_block_dev,
3492                                    vm_mode=vm_mode)
3493             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
3494             size = ephemeral_gb * units.Gi
3495             disk_image.cache(fetch_func=fn,
3496                              context=context,
3497                              filename=fname,
3498                              size=size,
3499                              ephemeral_size=ephemeral_gb)
3500 
3501         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
3502                 block_device_info)):
3503             disk_image = image(blockinfo.get_eph_disk(idx))
3504 
3505             specified_fs = eph.get('guest_format')
3506             if specified_fs and not self.is_supported_fs_format(specified_fs):
3507                 msg = _("%s format is not supported") % specified_fs
3508                 raise exception.InvalidBDMFormat(details=msg)
3509 
3510             fn = functools.partial(self._create_ephemeral,
3511                                    fs_label='ephemeral%d' % idx,
3512                                    os_type=instance.os_type,
3513                                    is_block_dev=disk_image.is_block_dev,
3514                                    vm_mode=vm_mode)
3515             size = eph['size'] * units.Gi
3516             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
3517             disk_image.cache(fetch_func=fn,
3518                              context=context,
3519                              filename=fname,
3520                              size=size,
3521                              ephemeral_size=eph['size'],
3522                              specified_fs=specified_fs)
3523 
3524         if swap_mb > 0:
3525             size = swap_mb * units.Mi
3526             image('disk.swap').cache(fetch_func=self._create_swap,
3527                                      context=context,
3528                                      filename="swap_%s" % swap_mb,
3529                                      size=size,
3530                                      swap_mb=swap_mb)
3531 
3532     def _create_and_inject_local_root(self, context, instance,
3533                                       booted_from_volume, suffix, disk_images,
3534                                       injection_info, fallback_from_host):
3535         # File injection only if needed
3536         need_inject = (not configdrive.required_by(instance) and
3537                        injection_info is not None and
3538                        CONF.libvirt.inject_partition != -2)
3539 
3540         # NOTE(ndipanov): Even if disk_mapping was passed in, which
3541         # currently happens only on rescue - we still don't want to
3542         # create a base image.
3543         if not booted_from_volume:
3544             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
3545             size = instance.flavor.root_gb * units.Gi
3546 
3547             if size == 0 or suffix == '.rescue':
3548                 size = None
3549 
3550             backend = self.image_backend.by_name(instance, 'disk' + suffix,
3551                                                  CONF.libvirt.images_type)
3552             if instance.task_state == task_states.RESIZE_FINISH:
3553                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
3554             if backend.SUPPORTS_CLONE:
3555                 def clone_fallback_to_fetch(*args, **kwargs):
3556                     try:
3557                         backend.clone(context, disk_images['image_id'])
3558                     except exception.ImageUnacceptable:
3559                         libvirt_utils.fetch_image(*args, **kwargs)
3560                 fetch_func = clone_fallback_to_fetch
3561             else:
3562                 fetch_func = libvirt_utils.fetch_image
3563             self._try_fetch_image_cache(backend, fetch_func, context,
3564                                         root_fname, disk_images['image_id'],
3565                                         instance, size, fallback_from_host)
3566 
3567             if need_inject:
3568                 self._inject_data(backend, instance, injection_info)
3569 
3570         elif need_inject:
3571             LOG.warning('File injection into a boot from volume '
3572                         'instance is not supported', instance=instance)
3573 
3574     def _create_configdrive(self, context, instance, injection_info,
3575                             rescue=False):
3576         # As this method being called right after the definition of a
3577         # domain, but before its actual launch, device metadata will be built
3578         # and saved in the instance for it to be used by the config drive and
3579         # the metadata service.
3580         instance.device_metadata = self._build_device_metadata(context,
3581                                                                instance)
3582         if configdrive.required_by(instance):
3583             LOG.info('Using config drive', instance=instance)
3584 
3585             name = 'disk.config'
3586             if rescue:
3587                 name += '.rescue'
3588 
3589             config_disk = self.image_backend.by_name(
3590                 instance, name, self._get_disk_config_image_type())
3591 
3592             # Don't overwrite an existing config drive
3593             if not config_disk.exists():
3594                 extra_md = {}
3595                 if injection_info.admin_pass:
3596                     extra_md['admin_pass'] = injection_info.admin_pass
3597 
3598                 inst_md = instance_metadata.InstanceMetadata(
3599                     instance, content=injection_info.files, extra_md=extra_md,
3600                     network_info=injection_info.network_info,
3601                     request_context=context)
3602 
3603                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
3604                 with cdb:
3605                     # NOTE(mdbooth): We're hardcoding here the path of the
3606                     # config disk when using the flat backend. This isn't
3607                     # good, but it's required because we need a local path we
3608                     # know we can write to in case we're subsequently
3609                     # importing into rbd. This will be cleaned up when we
3610                     # replace this with a call to create_from_func, but that
3611                     # can't happen until we've updated the backends and we
3612                     # teach them not to cache config disks. This isn't
3613                     # possible while we're still using cache() under the hood.
3614                     config_disk_local_path = os.path.join(
3615                         libvirt_utils.get_instance_path(instance), name)
3616                     LOG.info('Creating config drive at %(path)s',
3617                              {'path': config_disk_local_path},
3618                              instance=instance)
3619 
3620                     try:
3621                         cdb.make_drive(config_disk_local_path)
3622                     except processutils.ProcessExecutionError as e:
3623                         with excutils.save_and_reraise_exception():
3624                             LOG.error('Creating config drive failed with '
3625                                       'error: %s', e, instance=instance)
3626 
3627                 try:
3628                     config_disk.import_file(
3629                         instance, config_disk_local_path, name)
3630                 finally:
3631                     # NOTE(mikal): if the config drive was imported into RBD,
3632                     # then we no longer need the local copy
3633                     if CONF.libvirt.images_type == 'rbd':
3634                         LOG.info('Deleting local config drive %(path)s '
3635                                  'because it was imported into RBD.',
3636                                  {'path': config_disk_local_path},
3637                                  instance=instance)
3638                         os.unlink(config_disk_local_path)
3639 
3640     def _prepare_pci_devices_for_use(self, pci_devices):
3641         # kvm , qemu support managed mode
3642         # In managed mode, the configured device will be automatically
3643         # detached from the host OS drivers when the guest is started,
3644         # and then re-attached when the guest shuts down.
3645         if CONF.libvirt.virt_type != 'xen':
3646             # we do manual detach only for xen
3647             return
3648         try:
3649             for dev in pci_devices:
3650                 libvirt_dev_addr = dev['hypervisor_name']
3651                 libvirt_dev = \
3652                         self._host.device_lookup_by_name(libvirt_dev_addr)
3653                 # Note(yjiang5) Spelling for 'dettach' is correct, see
3654                 # http://libvirt.org/html/libvirt-libvirt.html.
3655                 libvirt_dev.dettach()
3656 
3657             # Note(yjiang5): A reset of one PCI device may impact other
3658             # devices on the same bus, thus we need two separated loops
3659             # to detach and then reset it.
3660             for dev in pci_devices:
3661                 libvirt_dev_addr = dev['hypervisor_name']
3662                 libvirt_dev = \
3663                         self._host.device_lookup_by_name(libvirt_dev_addr)
3664                 libvirt_dev.reset()
3665 
3666         except libvirt.libvirtError as exc:
3667             raise exception.PciDevicePrepareFailed(id=dev['id'],
3668                                                    instance_uuid=
3669                                                    dev['instance_uuid'],
3670                                                    reason=six.text_type(exc))
3671 
3672     def _detach_pci_devices(self, guest, pci_devs):
3673         try:
3674             for dev in pci_devs:
3675                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
3676                 # after detachDeviceFlags returned, we should check the dom to
3677                 # ensure the detaching is finished
3678                 xml = guest.get_xml_desc()
3679                 xml_doc = etree.fromstring(xml)
3680                 guest_config = vconfig.LibvirtConfigGuest()
3681                 guest_config.parse_dom(xml_doc)
3682 
3683                 for hdev in [d for d in guest_config.devices
3684                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
3685                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
3686                     dbsf = pci_utils.parse_address(dev.address)
3687                     if [int(x, 16) for x in hdbsf] ==\
3688                             [int(x, 16) for x in dbsf]:
3689                         raise exception.PciDeviceDetachFailed(reason=
3690                                                               "timeout",
3691                                                               dev=dev)
3692 
3693         except libvirt.libvirtError as ex:
3694             error_code = ex.get_error_code()
3695             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
3696                 LOG.warning("Instance disappeared while detaching "
3697                             "a PCI device from it.")
3698             else:
3699                 raise
3700 
3701     def _attach_pci_devices(self, guest, pci_devs):
3702         try:
3703             for dev in pci_devs:
3704                 guest.attach_device(self._get_guest_pci_device(dev))
3705 
3706         except libvirt.libvirtError:
3707             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
3708                       {'dev': pci_devs, 'dom': guest.id})
3709             raise
3710 
3711     @staticmethod
3712     def _has_direct_passthrough_port(network_info):
3713         for vif in network_info:
3714             if (vif['vnic_type'] in
3715                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3716                 return True
3717         return False
3718 
3719     def _attach_direct_passthrough_ports(
3720         self, context, instance, guest, network_info=None):
3721         if network_info is None:
3722             network_info = instance.info_cache.network_info
3723         if network_info is None:
3724             return
3725 
3726         if self._has_direct_passthrough_port(network_info):
3727             for vif in network_info:
3728                 if (vif['vnic_type'] in
3729                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3730                     cfg = self.vif_driver.get_config(instance,
3731                                                      vif,
3732                                                      instance.image_meta,
3733                                                      instance.flavor,
3734                                                      CONF.libvirt.virt_type,
3735                                                      self._host)
3736                     LOG.debug('Attaching direct passthrough port %(port)s '
3737                               'to %(dom)s', {'port': vif, 'dom': guest.id},
3738                               instance=instance)
3739                     guest.attach_device(cfg)
3740 
3741     def _detach_direct_passthrough_ports(self, context, instance, guest):
3742         network_info = instance.info_cache.network_info
3743         if network_info is None:
3744             return
3745 
3746         if self._has_direct_passthrough_port(network_info):
3747             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
3748             # pci request per direct passthrough port. Therefore we can trust
3749             # that pci_slot value in the vif is correct.
3750             direct_passthrough_pci_addresses = [
3751                 vif['profile']['pci_slot']
3752                 for vif in network_info
3753                 if (vif['vnic_type'] in
3754                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
3755                     vif['profile'].get('pci_slot') is not None)
3756             ]
3757 
3758             # use detach_pci_devices to avoid failure in case of
3759             # multiple guest direct passthrough ports with the same MAC
3760             # (protection use-case, ports are on different physical
3761             # interfaces)
3762             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
3763             direct_passthrough_pci_addresses = (
3764                 [pci_dev for pci_dev in pci_devs
3765                  if pci_dev.address in direct_passthrough_pci_addresses])
3766             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
3767 
3768     def _set_host_enabled(self, enabled,
3769                           disable_reason=DISABLE_REASON_UNDEFINED):
3770         """Enables / Disables the compute service on this host.
3771 
3772            This doesn't override non-automatic disablement with an automatic
3773            setting; thereby permitting operators to keep otherwise
3774            healthy hosts out of rotation.
3775         """
3776 
3777         status_name = {True: 'disabled',
3778                        False: 'enabled'}
3779 
3780         disable_service = not enabled
3781 
3782         ctx = nova_context.get_admin_context()
3783         try:
3784             service = objects.Service.get_by_compute_host(ctx, CONF.host)
3785 
3786             if service.disabled != disable_service:
3787                 # Note(jang): this is a quick fix to stop operator-
3788                 # disabled compute hosts from re-enabling themselves
3789                 # automatically. We prefix any automatic reason code
3790                 # with a fixed string. We only re-enable a host
3791                 # automatically if we find that string in place.
3792                 # This should probably be replaced with a separate flag.
3793                 if not service.disabled or (
3794                         service.disabled_reason and
3795                         service.disabled_reason.startswith(DISABLE_PREFIX)):
3796                     service.disabled = disable_service
3797                     service.disabled_reason = (
3798                        DISABLE_PREFIX + disable_reason
3799                        if disable_service and disable_reason else
3800                            DISABLE_REASON_UNDEFINED)
3801                     service.save()
3802                     LOG.debug('Updating compute service status to %s',
3803                               status_name[disable_service])
3804                 else:
3805                     LOG.debug('Not overriding manual compute service '
3806                               'status with: %s',
3807                               status_name[disable_service])
3808         except exception.ComputeHostNotFound:
3809             LOG.warning('Cannot update service status on host "%s" '
3810                         'since it is not registered.', CONF.host)
3811         except Exception:
3812             LOG.warning('Cannot update service status on host "%s" '
3813                         'due to an unexpected exception.', CONF.host,
3814                         exc_info=True)
3815 
3816         if enabled:
3817             mount.get_manager().host_up(self._host)
3818         else:
3819             mount.get_manager().host_down()
3820 
3821     def _get_guest_cpu_model_config(self):
3822         mode = CONF.libvirt.cpu_mode
3823         model = CONF.libvirt.cpu_model
3824 
3825         if (CONF.libvirt.virt_type == "kvm" or
3826             CONF.libvirt.virt_type == "qemu"):
3827             if mode is None:
3828                 caps = self._host.get_capabilities()
3829                 # AArch64 lacks 'host-model' support because neither libvirt
3830                 # nor QEMU are able to tell what the host CPU model exactly is.
3831                 # And there is no CPU description code for ARM(64) at this
3832                 # point.
3833 
3834                 # Also worth noting: 'host-passthrough' mode will completely
3835                 # break live migration, *unless* all the Compute nodes (running
3836                 # libvirtd) have *identical* CPUs.
3837                 if caps.host.cpu.arch == fields.Architecture.AARCH64:
3838                     mode = "host-passthrough"
3839                     LOG.info('CPU mode "host-passthrough" was chosen. Live '
3840                              'migration can break unless all compute nodes '
3841                              'have identical cpus. AArch64 does not support '
3842                              'other modes.')
3843                 else:
3844                     mode = "host-model"
3845             if mode == "none":
3846                 return vconfig.LibvirtConfigGuestCPU()
3847         else:
3848             if mode is None or mode == "none":
3849                 return None
3850 
3851         if ((CONF.libvirt.virt_type != "kvm" and
3852              CONF.libvirt.virt_type != "qemu")):
3853             msg = _("Config requested an explicit CPU model, but "
3854                     "the current libvirt hypervisor '%s' does not "
3855                     "support selecting CPU models") % CONF.libvirt.virt_type
3856             raise exception.Invalid(msg)
3857 
3858         if mode == "custom" and model is None:
3859             msg = _("Config requested a custom CPU model, but no "
3860                     "model name was provided")
3861             raise exception.Invalid(msg)
3862         elif mode != "custom" and model is not None:
3863             msg = _("A CPU model name should not be set when a "
3864                     "host CPU model is requested")
3865             raise exception.Invalid(msg)
3866 
3867         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen",
3868                   {'mode': mode, 'model': (model or "")})
3869 
3870         cpu = vconfig.LibvirtConfigGuestCPU()
3871         cpu.mode = mode
3872         cpu.model = model
3873 
3874         return cpu
3875 
3876     def _get_guest_cpu_config(self, flavor, image_meta,
3877                               guest_cpu_numa_config, instance_numa_topology):
3878         cpu = self._get_guest_cpu_model_config()
3879 
3880         if cpu is None:
3881             return None
3882 
3883         topology = hardware.get_best_cpu_topology(
3884                 flavor, image_meta, numa_topology=instance_numa_topology)
3885 
3886         cpu.sockets = topology.sockets
3887         cpu.cores = topology.cores
3888         cpu.threads = topology.threads
3889         cpu.numa = guest_cpu_numa_config
3890 
3891         return cpu
3892 
3893     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
3894                                image_type=None):
3895         disk_unit = None
3896         disk = self.image_backend.by_name(instance, name, image_type)
3897         if (name == 'disk.config' and image_type == 'rbd' and
3898                 not disk.exists()):
3899             # This is likely an older config drive that has not been migrated
3900             # to rbd yet. Try to fall back on 'flat' image type.
3901             # TODO(melwitt): Add online migration of some sort so we can
3902             # remove this fall back once we know all config drives are in rbd.
3903             # NOTE(vladikr): make sure that the flat image exist, otherwise
3904             # the image will be created after the domain definition.
3905             flat_disk = self.image_backend.by_name(instance, name, 'flat')
3906             if flat_disk.exists():
3907                 disk = flat_disk
3908                 LOG.debug('Config drive not found in RBD, falling back to the '
3909                           'instance directory', instance=instance)
3910         disk_info = disk_mapping[name]
3911         if 'unit' in disk_mapping:
3912             disk_unit = disk_mapping['unit']
3913             disk_mapping['unit'] += 1  # Increments for the next disk added
3914         conf = disk.libvirt_info(disk_info['bus'],
3915                                  disk_info['dev'],
3916                                  disk_info['type'],
3917                                  self.disk_cachemode,
3918                                  inst_type['extra_specs'],
3919                                  self._host.get_version(),
3920                                  disk_unit=disk_unit)
3921         return conf
3922 
3923     def _get_guest_fs_config(self, instance, name, image_type=None):
3924         disk = self.image_backend.by_name(instance, name, image_type)
3925         return disk.libvirt_fs_info("/", "ploop")
3926 
3927     def _get_guest_storage_config(self, context, instance, image_meta,
3928                                   disk_info,
3929                                   rescue, block_device_info,
3930                                   inst_type, os_type):
3931         devices = []
3932         disk_mapping = disk_info['mapping']
3933 
3934         block_device_mapping = driver.block_device_info_get_mapping(
3935             block_device_info)
3936         mount_rootfs = CONF.libvirt.virt_type == "lxc"
3937         scsi_controller = self._get_scsi_controller(image_meta)
3938 
3939         if scsi_controller and scsi_controller.model == 'virtio-scsi':
3940             # The virtio-scsi can handle up to 256 devices but the
3941             # optional element "address" must be defined to describe
3942             # where the device is placed on the controller (see:
3943             # LibvirtConfigGuestDeviceAddressDrive).
3944             #
3945             # Note about why it's added in disk_mapping: It's not
3946             # possible to pass an 'int' by reference in Python, so we
3947             # use disk_mapping as container to keep reference of the
3948             # unit added and be able to increment it for each disk
3949             # added.
3950             disk_mapping['unit'] = 0
3951 
3952         def _get_ephemeral_devices():
3953             eph_devices = []
3954             for idx, eph in enumerate(
3955                 driver.block_device_info_get_ephemerals(
3956                     block_device_info)):
3957                 diskeph = self._get_guest_disk_config(
3958                     instance,
3959                     blockinfo.get_eph_disk(idx),
3960                     disk_mapping, inst_type)
3961                 eph_devices.append(diskeph)
3962             return eph_devices
3963 
3964         if mount_rootfs:
3965             fs = vconfig.LibvirtConfigGuestFilesys()
3966             fs.source_type = "mount"
3967             fs.source_dir = os.path.join(
3968                 libvirt_utils.get_instance_path(instance), 'rootfs')
3969             devices.append(fs)
3970         elif (os_type == fields.VMMode.EXE and
3971               CONF.libvirt.virt_type == "parallels"):
3972             if rescue:
3973                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
3974                 devices.append(fsrescue)
3975 
3976                 fsos = self._get_guest_fs_config(instance, "disk")
3977                 fsos.target_dir = "/mnt/rescue"
3978                 devices.append(fsos)
3979             else:
3980                 if 'disk' in disk_mapping:
3981                     fs = self._get_guest_fs_config(instance, "disk")
3982                     devices.append(fs)
3983                 devices = devices + _get_ephemeral_devices()
3984         else:
3985 
3986             if rescue:
3987                 diskrescue = self._get_guest_disk_config(instance,
3988                                                          'disk.rescue',
3989                                                          disk_mapping,
3990                                                          inst_type)
3991                 devices.append(diskrescue)
3992 
3993                 diskos = self._get_guest_disk_config(instance,
3994                                                      'disk',
3995                                                      disk_mapping,
3996                                                      inst_type)
3997                 devices.append(diskos)
3998             else:
3999                 if 'disk' in disk_mapping:
4000                     diskos = self._get_guest_disk_config(instance,
4001                                                          'disk',
4002                                                          disk_mapping,
4003                                                          inst_type)
4004                     devices.append(diskos)
4005 
4006                 if 'disk.local' in disk_mapping:
4007                     disklocal = self._get_guest_disk_config(instance,
4008                                                             'disk.local',
4009                                                             disk_mapping,
4010                                                             inst_type)
4011                     devices.append(disklocal)
4012                     instance.default_ephemeral_device = (
4013                         block_device.prepend_dev(disklocal.target_dev))
4014 
4015                 devices = devices + _get_ephemeral_devices()
4016 
4017                 if 'disk.swap' in disk_mapping:
4018                     diskswap = self._get_guest_disk_config(instance,
4019                                                            'disk.swap',
4020                                                            disk_mapping,
4021                                                            inst_type)
4022                     devices.append(diskswap)
4023                     instance.default_swap_device = (
4024                         block_device.prepend_dev(diskswap.target_dev))
4025 
4026             config_name = 'disk.config.rescue' if rescue else 'disk.config'
4027             if config_name in disk_mapping:
4028                 diskconfig = self._get_guest_disk_config(
4029                     instance, config_name, disk_mapping, inst_type,
4030                     self._get_disk_config_image_type())
4031                 devices.append(diskconfig)
4032 
4033         for vol in block_device.get_bdms_to_connect(block_device_mapping,
4034                                                    mount_rootfs):
4035             connection_info = vol['connection_info']
4036             vol_dev = block_device.prepend_dev(vol['mount_device'])
4037             info = disk_mapping[vol_dev]
4038             self._connect_volume(context, connection_info, instance)
4039             if scsi_controller and scsi_controller.model == 'virtio-scsi':
4040                 info['unit'] = disk_mapping['unit']
4041                 disk_mapping['unit'] += 1
4042             cfg = self._get_volume_config(connection_info, info)
4043             devices.append(cfg)
4044             vol['connection_info'] = connection_info
4045             vol.save()
4046 
4047         for d in devices:
4048             self._set_cache_mode(d)
4049 
4050         if scsi_controller:
4051             devices.append(scsi_controller)
4052 
4053         return devices
4054 
4055     @staticmethod
4056     def _get_scsi_controller(image_meta):
4057         """Return scsi controller or None based on image meta"""
4058         if image_meta.properties.get('hw_scsi_model'):
4059             hw_scsi_model = image_meta.properties.hw_scsi_model
4060             scsi_controller = vconfig.LibvirtConfigGuestController()
4061             scsi_controller.type = 'scsi'
4062             scsi_controller.model = hw_scsi_model
4063             scsi_controller.index = 0
4064             return scsi_controller
4065 
4066     def _get_host_sysinfo_serial_hardware(self):
4067         """Get a UUID from the host hardware
4068 
4069         Get a UUID for the host hardware reported by libvirt.
4070         This is typically from the SMBIOS data, unless it has
4071         been overridden in /etc/libvirt/libvirtd.conf
4072         """
4073         caps = self._host.get_capabilities()
4074         return caps.host.uuid
4075 
4076     def _get_host_sysinfo_serial_os(self):
4077         """Get a UUID from the host operating system
4078 
4079         Get a UUID for the host operating system. Modern Linux
4080         distros based on systemd provide a /etc/machine-id
4081         file containing a UUID. This is also provided inside
4082         systemd based containers and can be provided by other
4083         init systems too, since it is just a plain text file.
4084         """
4085         if not os.path.exists("/etc/machine-id"):
4086             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
4087             raise exception.InternalError(msg)
4088 
4089         with open("/etc/machine-id") as f:
4090             # We want to have '-' in the right place
4091             # so we parse & reformat the value
4092             lines = f.read().split()
4093             if not lines:
4094                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
4095                 raise exception.InternalError(msg)
4096 
4097             return str(uuid.UUID(lines[0]))
4098 
4099     def _get_host_sysinfo_serial_auto(self):
4100         if os.path.exists("/etc/machine-id"):
4101             return self._get_host_sysinfo_serial_os()
4102         else:
4103             return self._get_host_sysinfo_serial_hardware()
4104 
4105     def _get_guest_config_sysinfo(self, instance):
4106         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
4107 
4108         sysinfo.system_manufacturer = version.vendor_string()
4109         sysinfo.system_product = version.product_string()
4110         sysinfo.system_version = version.version_string_with_package()
4111 
4112         sysinfo.system_serial = self._sysinfo_serial_func()
4113         sysinfo.system_uuid = instance.uuid
4114 
4115         sysinfo.system_family = "Virtual Machine"
4116 
4117         return sysinfo
4118 
4119     def _get_guest_pci_device(self, pci_device):
4120 
4121         dbsf = pci_utils.parse_address(pci_device.address)
4122         dev = vconfig.LibvirtConfigGuestHostdevPCI()
4123         dev.domain, dev.bus, dev.slot, dev.function = dbsf
4124 
4125         # only kvm support managed mode
4126         if CONF.libvirt.virt_type in ('xen', 'parallels',):
4127             dev.managed = 'no'
4128         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
4129             dev.managed = 'yes'
4130 
4131         return dev
4132 
4133     def _get_guest_config_meta(self, instance):
4134         """Get metadata config for guest."""
4135 
4136         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
4137         meta.package = version.version_string_with_package()
4138         meta.name = instance.display_name
4139         meta.creationTime = time.time()
4140 
4141         if instance.image_ref not in ("", None):
4142             meta.roottype = "image"
4143             meta.rootid = instance.image_ref
4144 
4145         system_meta = instance.system_metadata
4146         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
4147         ometa.userid = instance.user_id
4148         ometa.username = system_meta.get('owner_user_name', 'N/A')
4149         ometa.projectid = instance.project_id
4150         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
4151         meta.owner = ometa
4152 
4153         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
4154         flavor = instance.flavor
4155         fmeta.name = flavor.name
4156         fmeta.memory = flavor.memory_mb
4157         fmeta.vcpus = flavor.vcpus
4158         fmeta.ephemeral = flavor.ephemeral_gb
4159         fmeta.disk = flavor.root_gb
4160         fmeta.swap = flavor.swap
4161 
4162         meta.flavor = fmeta
4163 
4164         return meta
4165 
4166     def _machine_type_mappings(self):
4167         mappings = {}
4168         for mapping in CONF.libvirt.hw_machine_type:
4169             host_arch, _, machine_type = mapping.partition('=')
4170             mappings[host_arch] = machine_type
4171         return mappings
4172 
4173     def _get_machine_type(self, image_meta, caps):
4174         # The underlying machine type can be set as an image attribute,
4175         # or otherwise based on some architecture specific defaults
4176 
4177         mach_type = None
4178 
4179         if image_meta.properties.get('hw_machine_type') is not None:
4180             mach_type = image_meta.properties.hw_machine_type
4181         else:
4182             # For ARM systems we will default to vexpress-a15 for armv7
4183             # and virt for aarch64
4184             if caps.host.cpu.arch == fields.Architecture.ARMV7:
4185                 mach_type = "vexpress-a15"
4186 
4187             if caps.host.cpu.arch == fields.Architecture.AARCH64:
4188                 mach_type = "virt"
4189 
4190             if caps.host.cpu.arch in (fields.Architecture.S390,
4191                                       fields.Architecture.S390X):
4192                 mach_type = 's390-ccw-virtio'
4193 
4194             # If set in the config, use that as the default.
4195             if CONF.libvirt.hw_machine_type:
4196                 mappings = self._machine_type_mappings()
4197                 mach_type = mappings.get(caps.host.cpu.arch)
4198 
4199         return mach_type
4200 
4201     @staticmethod
4202     def _create_idmaps(klass, map_strings):
4203         idmaps = []
4204         if len(map_strings) > 5:
4205             map_strings = map_strings[0:5]
4206             LOG.warning("Too many id maps, only included first five.")
4207         for map_string in map_strings:
4208             try:
4209                 idmap = klass()
4210                 values = [int(i) for i in map_string.split(":")]
4211                 idmap.start = values[0]
4212                 idmap.target = values[1]
4213                 idmap.count = values[2]
4214                 idmaps.append(idmap)
4215             except (ValueError, IndexError):
4216                 LOG.warning("Invalid value for id mapping %s", map_string)
4217         return idmaps
4218 
4219     def _get_guest_idmaps(self):
4220         id_maps = []
4221         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
4222             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
4223                                            CONF.libvirt.uid_maps)
4224             id_maps.extend(uid_maps)
4225         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
4226             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
4227                                            CONF.libvirt.gid_maps)
4228             id_maps.extend(gid_maps)
4229         return id_maps
4230 
4231     def _update_guest_cputune(self, guest, flavor, virt_type):
4232         is_able = self._host.is_cpu_control_policy_capable()
4233 
4234         cputuning = ['shares', 'period', 'quota']
4235         wants_cputune = any([k for k in cputuning
4236             if "quota:cpu_" + k in flavor.extra_specs.keys()])
4237 
4238         if wants_cputune and not is_able:
4239             raise exception.UnsupportedHostCPUControlPolicy()
4240 
4241         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
4242             return
4243 
4244         if guest.cputune is None:
4245             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
4246             # Setting the default cpu.shares value to be a value
4247             # dependent on the number of vcpus
4248         guest.cputune.shares = 1024 * guest.vcpus
4249 
4250         for name in cputuning:
4251             key = "quota:cpu_" + name
4252             if key in flavor.extra_specs:
4253                 setattr(guest.cputune, name,
4254                         int(flavor.extra_specs[key]))
4255 
4256     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
4257                                            wants_hugepages):
4258         if instance_numa_topology:
4259             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
4260             for instance_cell in instance_numa_topology.cells:
4261                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
4262                 guest_cell.id = instance_cell.id
4263                 guest_cell.cpus = instance_cell.cpuset
4264                 guest_cell.memory = instance_cell.memory * units.Ki
4265 
4266                 # The vhost-user network backend requires file backed
4267                 # guest memory (ie huge pages) to be marked as shared
4268                 # access, not private, so an external process can read
4269                 # and write the pages.
4270                 #
4271                 # You can't change the shared vs private flag for an
4272                 # already running guest, and since we can't predict what
4273                 # types of NIC may be hotplugged, we have no choice but
4274                 # to unconditionally turn on the shared flag. This has
4275                 # no real negative functional effect on the guest, so
4276                 # is a reasonable approach to take
4277                 if wants_hugepages:
4278                     guest_cell.memAccess = "shared"
4279                 guest_cpu_numa.cells.append(guest_cell)
4280             return guest_cpu_numa
4281 
4282     def _has_cpu_policy_support(self):
4283         for ver in BAD_LIBVIRT_CPU_POLICY_VERSIONS:
4284             if self._host.has_version(ver):
4285                 ver_ = self._version_to_string(ver)
4286                 raise exception.CPUPinningNotSupported(reason=_(
4287                     'Invalid libvirt version %(version)s') % {'version': ver_})
4288         return True
4289 
4290     def _wants_hugepages(self, host_topology, instance_topology):
4291         """Determine if the guest / host topology implies the
4292            use of huge pages for guest RAM backing
4293         """
4294 
4295         if host_topology is None or instance_topology is None:
4296             return False
4297 
4298         avail_pagesize = [page.size_kb
4299                           for page in host_topology.cells[0].mempages]
4300         avail_pagesize.sort()
4301         # Remove smallest page size as that's not classed as a largepage
4302         avail_pagesize = avail_pagesize[1:]
4303 
4304         # See if we have page size set
4305         for cell in instance_topology.cells:
4306             if (cell.pagesize is not None and
4307                 cell.pagesize in avail_pagesize):
4308                 return True
4309 
4310         return False
4311 
4312     def _get_vcpu_realtime_scheduler(self, vcpus_rt):
4313         """Returns the config object of LibvirtConfigGuestCPUTuneVCPUSched.
4314         Prepares realtime config for the guest.
4315         """
4316         vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
4317         vcpusched.vcpus = vcpus_rt
4318         vcpusched.scheduler = "fifo"
4319         vcpusched.priority = CONF.libvirt.realtime_scheduler_priority
4320         return vcpusched
4321 
4322     def _get_cell_pairs(self, guest_cpu_numa_config, host_topology):
4323         """Returns the lists of pairs(tuple) of an instance cell and
4324         corresponding host cell:
4325             [(LibvirtConfigGuestCPUNUMACell, NUMACell), ...]
4326         """
4327         cell_pairs = []
4328         for guest_config_cell in guest_cpu_numa_config.cells:
4329             for host_cell in host_topology.cells:
4330                 if guest_config_cell.id == host_cell.id:
4331                     cell_pairs.append((guest_config_cell, host_cell))
4332         return cell_pairs
4333 
4334     def _get_numa_memnode(self, guest_node_id, host_cell_id):
4335         """Returns the config object of LibvirtConfigGuestNUMATuneMemNode.
4336         Prepares numa memory node config for the guest.
4337         """
4338         node = vconfig.LibvirtConfigGuestNUMATuneMemNode()
4339         node.cellid = guest_node_id
4340         node.nodeset = [host_cell_id]
4341         node.mode = "strict"
4342         return node
4343 
4344     def _get_guest_numa_config(self, instance_numa_topology, flavor,
4345                                allowed_cpus=None, image_meta=None):
4346         """Returns the config objects for the guest NUMA specs.
4347 
4348         Determines the CPUs that the guest can be pinned to if the guest
4349         specifies a cell topology and the host supports it. Constructs the
4350         libvirt XML config object representing the NUMA topology selected
4351         for the guest. Returns a tuple of:
4352 
4353             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
4354 
4355         With the following caveats:
4356 
4357             a) If there is no specified guest NUMA topology, then
4358                all tuple elements except cpu_set shall be None. cpu_set
4359                will be populated with the chosen CPUs that the guest
4360                allowed CPUs fit within, which could be the supplied
4361                allowed_cpus value if the host doesn't support NUMA
4362                topologies.
4363 
4364             b) If there is a specified guest NUMA topology, then
4365                cpu_set will be None and guest_cpu_numa will be the
4366                LibvirtConfigGuestCPUNUMA object representing the guest's
4367                NUMA topology. If the host supports NUMA, then guest_cpu_tune
4368                will contain a LibvirtConfigGuestCPUTune object representing
4369                the optimized chosen cells that match the host capabilities
4370                with the instance's requested topology. If the host does
4371                not support NUMA, then guest_cpu_tune and guest_numa_tune
4372                will be None.
4373         """
4374 
4375         if (not self._has_numa_support() and
4376                 instance_numa_topology is not None):
4377             # We should not get here, since we should have avoided
4378             # reporting NUMA topology from _get_host_numa_topology
4379             # in the first place. Just in case of a scheduler
4380             # mess up though, raise an exception
4381             raise exception.NUMATopologyUnsupported()
4382 
4383         topology = self._get_host_numa_topology()
4384 
4385         # We have instance NUMA so translate it to the config class
4386         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
4387                 instance_numa_topology,
4388                 self._wants_hugepages(topology, instance_numa_topology))
4389 
4390         if not guest_cpu_numa_config:
4391             # No NUMA topology defined for instance - let the host kernel deal
4392             # with the NUMA effects.
4393             # TODO(ndipanov): Attempt to spread the instance
4394             # across NUMA nodes and expose the topology to the
4395             # instance as an optimisation
4396             return GuestNumaConfig(allowed_cpus, None, None, None)
4397         else:
4398             if topology:
4399                 # Now get the CpuTune configuration from the numa_topology
4400                 guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
4401                 emupcpus = []
4402 
4403                 # Init NUMATune configuration
4404                 guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
4405                 guest_numa_tune.memory = (
4406                     vconfig.LibvirtConfigGuestNUMATuneMemory())
4407                 guest_numa_tune.memnodes = []
4408 
4409                 emulator_threads_isolated = (
4410                     instance_numa_topology.emulator_threads_isolated)
4411 
4412                 # Set realtime scheduler for CPUTune
4413                 vcpus_rt = set([])
4414                 wants_realtime = hardware.is_realtime_enabled(flavor)
4415                 if wants_realtime:
4416                     if not self._host.has_min_version(
4417                             MIN_LIBVIRT_REALTIME_VERSION):
4418                         raise exception.RealtimePolicyNotSupported()
4419                     vcpus_rt = hardware.vcpus_realtime_topology(
4420                         flavor, image_meta)
4421                     vcpusched = self._get_vcpu_realtime_scheduler(vcpus_rt)
4422                     guest_cpu_tune.vcpusched.append(vcpusched)
4423 
4424                 # TODO(sahid): Defining domain topology should be
4425                 # refactored.
4426                 cell_pairs = self._get_cell_pairs(guest_cpu_numa_config,
4427                                                   topology)
4428                 for guest_node_id, (guest_config_cell, host_cell) in enumerate(
4429                         cell_pairs):
4430                     guest_numa_tune.memnodes.append(
4431                         self._get_numa_memnode(guest_node_id, host_cell.id))
4432                     guest_numa_tune.memory.nodeset.append(host_cell.id)
4433 
4434                     object_numa_cell = instance_numa_topology.cells[
4435                                                                 guest_node_id]
4436 
4437                     for cpu in guest_config_cell.cpus:
4438                         pin_cpuset = (
4439                             vconfig.LibvirtConfigGuestCPUTuneVCPUPin())
4440                         pin_cpuset.id = cpu
4441                         # If there is pinning information in the cell
4442                         # we pin to individual CPUs, otherwise we float
4443                         # over the whole host NUMA node
4444 
4445                         if (object_numa_cell.cpu_pinning and
4446                                 self._has_cpu_policy_support()):
4447                             pcpu = object_numa_cell.cpu_pinning[cpu]
4448                             pin_cpuset.cpuset = set([pcpu])
4449                         else:
4450                             pin_cpuset.cpuset = host_cell.cpuset
4451                         if emulator_threads_isolated:
4452                             emupcpus.extend(
4453                                 object_numa_cell.cpuset_reserved)
4454                         elif not wants_realtime or cpu not in vcpus_rt:
4455                             # - If realtime IS NOT enabled, the
4456                             #   emulator threads are allowed to float
4457                             #   across all the pCPUs associated with
4458                             #   the guest vCPUs ("not wants_realtime"
4459                             #   is true, so we add all pcpus)
4460                             # - If realtime IS enabled, then at least
4461                             #   1 vCPU is required to be set aside for
4462                             #   non-realtime usage. The emulator
4463                             #   threads are allowed to float acros the
4464                             #   pCPUs that are associated with the
4465                             #   non-realtime VCPUs (the "cpu not in
4466                             #   vcpu_rt" check deals with this
4467                             #   filtering)
4468                             emupcpus.extend(pin_cpuset.cpuset)
4469                         guest_cpu_tune.vcpupin.append(pin_cpuset)
4470 
4471                 # TODO(berrange) When the guest has >1 NUMA node, it will
4472                 # span multiple host NUMA nodes. By pinning emulator threads
4473                 # to the union of all nodes, we guarantee there will be
4474                 # cross-node memory access by the emulator threads when
4475                 # responding to guest I/O operations. The only way to avoid
4476                 # this would be to pin emulator threads to a single node and
4477                 # tell the guest OS to only do I/O from one of its virtual
4478                 # NUMA nodes. This is not even remotely practical.
4479                 #
4480                 # The long term solution is to make use of a new QEMU feature
4481                 # called "I/O Threads" which will let us configure an explicit
4482                 # I/O thread for each guest vCPU or guest NUMA node. It is
4483                 # still TBD how to make use of this feature though, especially
4484                 # how to associate IO threads with guest devices to eliminate
4485                 # cross NUMA node traffic. This is an area of investigation
4486                 # for QEMU community devs.
4487                 emulatorpin = vconfig.LibvirtConfigGuestCPUTuneEmulatorPin()
4488                 emulatorpin.cpuset = set(emupcpus)
4489                 guest_cpu_tune.emulatorpin = emulatorpin
4490                 # Sort the vcpupin list per vCPU id for human-friendlier XML
4491                 guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
4492 
4493                 # normalize cell.id
4494                 for i, (cell, memnode) in enumerate(
4495                                             zip(guest_cpu_numa_config.cells,
4496                                                 guest_numa_tune.memnodes)):
4497                     cell.id = i
4498                     memnode.cellid = i
4499 
4500                 return GuestNumaConfig(None, guest_cpu_tune,
4501                                        guest_cpu_numa_config,
4502                                        guest_numa_tune)
4503             else:
4504                 return GuestNumaConfig(allowed_cpus, None,
4505                                        guest_cpu_numa_config, None)
4506 
4507     def _get_guest_os_type(self, virt_type):
4508         """Returns the guest OS type based on virt type."""
4509         if virt_type == "lxc":
4510             ret = fields.VMMode.EXE
4511         elif virt_type == "uml":
4512             ret = fields.VMMode.UML
4513         elif virt_type == "xen":
4514             ret = fields.VMMode.XEN
4515         else:
4516             ret = fields.VMMode.HVM
4517         return ret
4518 
4519     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
4520                               root_device_name):
4521         if rescue.get('kernel_id'):
4522             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
4523             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4524             if virt_type == "qemu":
4525                 guest.os_cmdline += " no_timer_check"
4526         if rescue.get('ramdisk_id'):
4527             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
4528 
4529     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
4530                                 root_device_name, image_meta):
4531         guest.os_kernel = os.path.join(inst_path, "kernel")
4532         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4533         if virt_type == "qemu":
4534             guest.os_cmdline += " no_timer_check"
4535         if instance.ramdisk_id:
4536             guest.os_initrd = os.path.join(inst_path, "ramdisk")
4537         # we only support os_command_line with images with an explicit
4538         # kernel set and don't want to break nova if there's an
4539         # os_command_line property without a specified kernel_id param
4540         if image_meta.properties.get("os_command_line"):
4541             guest.os_cmdline = image_meta.properties.os_command_line
4542 
4543     def _set_clock(self, guest, os_type, image_meta, virt_type):
4544         # NOTE(mikal): Microsoft Windows expects the clock to be in
4545         # "localtime". If the clock is set to UTC, then you can use a
4546         # registry key to let windows know, but Microsoft says this is
4547         # buggy in http://support.microsoft.com/kb/2687252
4548         clk = vconfig.LibvirtConfigGuestClock()
4549         if os_type == 'windows':
4550             LOG.info('Configuring timezone for windows instance to localtime')
4551             clk.offset = 'localtime'
4552         else:
4553             clk.offset = 'utc'
4554         guest.set_clock(clk)
4555 
4556         if virt_type == "kvm":
4557             self._set_kvm_timers(clk, os_type, image_meta)
4558 
4559     def _set_kvm_timers(self, clk, os_type, image_meta):
4560         # TODO(berrange) One day this should be per-guest
4561         # OS type configurable
4562         tmpit = vconfig.LibvirtConfigGuestTimer()
4563         tmpit.name = "pit"
4564         tmpit.tickpolicy = "delay"
4565 
4566         tmrtc = vconfig.LibvirtConfigGuestTimer()
4567         tmrtc.name = "rtc"
4568         tmrtc.tickpolicy = "catchup"
4569 
4570         clk.add_timer(tmpit)
4571         clk.add_timer(tmrtc)
4572 
4573         guestarch = libvirt_utils.get_arch(image_meta)
4574         if guestarch in (fields.Architecture.I686,
4575                          fields.Architecture.X86_64):
4576             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
4577             # qemu -no-hpet is not supported on non-x86 targets.
4578             tmhpet = vconfig.LibvirtConfigGuestTimer()
4579             tmhpet.name = "hpet"
4580             tmhpet.present = False
4581             clk.add_timer(tmhpet)
4582 
4583         # Provide Windows guests with the paravirtualized hyperv timer source.
4584         # This is the windows equiv of kvm-clock, allowing Windows
4585         # guests to accurately keep time.
4586         if os_type == 'windows':
4587             tmhyperv = vconfig.LibvirtConfigGuestTimer()
4588             tmhyperv.name = "hypervclock"
4589             tmhyperv.present = True
4590             clk.add_timer(tmhyperv)
4591 
4592     def _set_features(self, guest, os_type, caps, virt_type, image_meta):
4593         if virt_type == "xen":
4594             # PAE only makes sense in X86
4595             if caps.host.cpu.arch in (fields.Architecture.I686,
4596                                       fields.Architecture.X86_64):
4597                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
4598 
4599         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
4600                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
4601             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
4602             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
4603 
4604         if (virt_type in ("qemu", "kvm") and
4605                 os_type == 'windows'):
4606             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
4607             hv.relaxed = True
4608 
4609             hv.spinlocks = True
4610             # Increase spinlock retries - value recommended by
4611             # KVM maintainers who certify Windows guests
4612             # with Microsoft
4613             hv.spinlock_retries = 8191
4614             hv.vapic = True
4615             guest.features.append(hv)
4616 
4617         if (virt_type in ("qemu", "kvm") and
4618                 image_meta.properties.get('img_hide_hypervisor_id')):
4619             guest.features.append(vconfig.LibvirtConfigGuestFeatureKvmHidden())
4620 
4621     def _check_number_of_serial_console(self, num_ports):
4622         virt_type = CONF.libvirt.virt_type
4623         if (virt_type in ("kvm", "qemu") and
4624             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
4625             raise exception.SerialPortNumberLimitExceeded(
4626                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
4627 
4628     def _add_video_driver(self, guest, image_meta, flavor):
4629         VALID_VIDEO_DEVICES = ("vga", "cirrus", "vmvga",
4630                                "xen", "qxl", "virtio")
4631         video = vconfig.LibvirtConfigGuestVideo()
4632         # NOTE(ldbragst): The following logic sets the video.type
4633         # depending on supported defaults given the architecture,
4634         # virtualization type, and features. The video.type attribute can
4635         # be overridden by the user with image_meta.properties, which
4636         # is carried out in the next if statement below this one.
4637         guestarch = libvirt_utils.get_arch(image_meta)
4638         if guest.os_type == fields.VMMode.XEN:
4639             video.type = 'xen'
4640         elif CONF.libvirt.virt_type == 'parallels':
4641             video.type = 'vga'
4642         elif guestarch in (fields.Architecture.PPC,
4643                            fields.Architecture.PPC64,
4644                            fields.Architecture.PPC64LE):
4645             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
4646             # so use 'vga' instead when running on Power hardware.
4647             video.type = 'vga'
4648         elif guestarch in (fields.Architecture.AARCH64):
4649             # NOTE(kevinz): Only virtio device type is supported by AARCH64
4650             # so use 'virtio' instead when running on AArch64 hardware.
4651             video.type = 'virtio'
4652         elif CONF.spice.enabled:
4653             video.type = 'qxl'
4654         if image_meta.properties.get('hw_video_model'):
4655             video.type = image_meta.properties.hw_video_model
4656             if (video.type not in VALID_VIDEO_DEVICES):
4657                 raise exception.InvalidVideoMode(model=video.type)
4658 
4659         # Set video memory, only if the flavor's limit is set
4660         video_ram = image_meta.properties.get('hw_video_ram', 0)
4661         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
4662         if video_ram > max_vram:
4663             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
4664                                                  max_vram=max_vram)
4665         if max_vram and video_ram:
4666             video.vram = video_ram * units.Mi / units.Ki
4667         guest.add_device(video)
4668 
4669     def _add_qga_device(self, guest, instance):
4670         qga = vconfig.LibvirtConfigGuestChannel()
4671         qga.type = "unix"
4672         qga.target_name = "org.qemu.guest_agent.0"
4673         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
4674                           ("org.qemu.guest_agent.0", instance.name))
4675         guest.add_device(qga)
4676 
4677     def _add_rng_device(self, guest, flavor):
4678         rng_device = vconfig.LibvirtConfigGuestRng()
4679         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
4680         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
4681         if rate_bytes:
4682             rng_device.rate_bytes = int(rate_bytes)
4683             rng_device.rate_period = int(period)
4684         rng_path = CONF.libvirt.rng_dev_path
4685         if (rng_path and not os.path.exists(rng_path)):
4686             raise exception.RngDeviceNotExist(path=rng_path)
4687         rng_device.backend = rng_path
4688         guest.add_device(rng_device)
4689 
4690     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
4691         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
4692         if image_meta.properties.get('hw_qemu_guest_agent', False):
4693             LOG.debug("Qemu guest agent is enabled through image "
4694                       "metadata", instance=instance)
4695             self._add_qga_device(guest, instance)
4696         rng_is_virtio = image_meta.properties.get('hw_rng_model') == 'virtio'
4697         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
4698         rng_allowed = strutils.bool_from_string(rng_allowed_str)
4699         if rng_is_virtio and rng_allowed:
4700             self._add_rng_device(guest, flavor)
4701 
4702     def _get_guest_memory_backing_config(
4703             self, inst_topology, numatune, flavor):
4704         wantsmempages = False
4705         if inst_topology:
4706             for cell in inst_topology.cells:
4707                 if cell.pagesize:
4708                     wantsmempages = True
4709                     break
4710 
4711         wantsrealtime = hardware.is_realtime_enabled(flavor)
4712 
4713         membacking = None
4714         if wantsmempages:
4715             pages = self._get_memory_backing_hugepages_support(
4716                 inst_topology, numatune)
4717             if pages:
4718                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4719                 membacking.hugepages = pages
4720         if wantsrealtime:
4721             if not membacking:
4722                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4723             membacking.locked = True
4724             membacking.sharedpages = False
4725 
4726         return membacking
4727 
4728     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
4729         if not self._has_numa_support():
4730             # We should not get here, since we should have avoided
4731             # reporting NUMA topology from _get_host_numa_topology
4732             # in the first place. Just in case of a scheduler
4733             # mess up though, raise an exception
4734             raise exception.MemoryPagesUnsupported()
4735 
4736         host_topology = self._get_host_numa_topology()
4737 
4738         if host_topology is None:
4739             # As above, we should not get here but just in case...
4740             raise exception.MemoryPagesUnsupported()
4741 
4742         # Currently libvirt does not support the smallest
4743         # pagesize set as a backend memory.
4744         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
4745         avail_pagesize = [page.size_kb
4746                           for page in host_topology.cells[0].mempages]
4747         avail_pagesize.sort()
4748         smallest = avail_pagesize[0]
4749 
4750         pages = []
4751         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
4752             if inst_cell.pagesize and inst_cell.pagesize > smallest:
4753                 for memnode in numatune.memnodes:
4754                     if guest_cellid == memnode.cellid:
4755                         page = (
4756                             vconfig.LibvirtConfigGuestMemoryBackingPage())
4757                         page.nodeset = [guest_cellid]
4758                         page.size_kb = inst_cell.pagesize
4759                         pages.append(page)
4760                         break  # Quit early...
4761         return pages
4762 
4763     def _get_flavor(self, ctxt, instance, flavor):
4764         if flavor is not None:
4765             return flavor
4766         return instance.flavor
4767 
4768     def _has_uefi_support(self):
4769         # This means that the host can support uefi booting for guests
4770         supported_archs = [fields.Architecture.X86_64,
4771                            fields.Architecture.AARCH64]
4772         caps = self._host.get_capabilities()
4773         return ((caps.host.cpu.arch in supported_archs) and
4774                 os.path.exists(DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]))
4775 
4776     def _get_supported_perf_events(self):
4777 
4778         if (len(CONF.libvirt.enabled_perf_events) == 0 or
4779              not self._host.has_min_version(MIN_LIBVIRT_PERF_VERSION)):
4780             return []
4781 
4782         supported_events = []
4783         host_cpu_info = self._get_cpu_info()
4784         for event in CONF.libvirt.enabled_perf_events:
4785             if self._supported_perf_event(event, host_cpu_info['features']):
4786                 supported_events.append(event)
4787         return supported_events
4788 
4789     def _supported_perf_event(self, event, cpu_features):
4790 
4791         libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
4792 
4793         if not hasattr(libvirt, libvirt_perf_event_name):
4794             LOG.warning("Libvirt doesn't support event type %s.", event)
4795             return False
4796 
4797         if (event in PERF_EVENTS_CPU_FLAG_MAPPING
4798             and PERF_EVENTS_CPU_FLAG_MAPPING[event] not in cpu_features):
4799             LOG.warning("Host does not support event type %s.", event)
4800             return False
4801 
4802         return True
4803 
4804     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
4805                                       image_meta, flavor, root_device_name):
4806         if virt_type == "xen":
4807             if guest.os_type == fields.VMMode.HVM:
4808                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
4809             else:
4810                 guest.os_cmdline = CONSOLE
4811         elif virt_type in ("kvm", "qemu"):
4812             if caps.host.cpu.arch in (fields.Architecture.I686,
4813                                       fields.Architecture.X86_64):
4814                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
4815                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
4816             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
4817             if caps.host.cpu.arch == fields.Architecture.AARCH64:
4818                 if not hw_firmware_type:
4819                     hw_firmware_type = fields.FirmwareType.UEFI
4820             if hw_firmware_type == fields.FirmwareType.UEFI:
4821                 if self._has_uefi_support():
4822                     global uefi_logged
4823                     if not uefi_logged:
4824                         LOG.warning("uefi support is without some kind of "
4825                                     "functional testing and therefore "
4826                                     "considered experimental.")
4827                         uefi_logged = True
4828                     guest.os_loader = DEFAULT_UEFI_LOADER_PATH[
4829                         caps.host.cpu.arch]
4830                     guest.os_loader_type = "pflash"
4831                 else:
4832                     raise exception.UEFINotSupported()
4833             guest.os_mach_type = self._get_machine_type(image_meta, caps)
4834             if image_meta.properties.get('hw_boot_menu') is None:
4835                 guest.os_bootmenu = strutils.bool_from_string(
4836                     flavor.extra_specs.get('hw:boot_menu', 'no'))
4837             else:
4838                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
4839 
4840         elif virt_type == "lxc":
4841             guest.os_init_path = "/sbin/init"
4842             guest.os_cmdline = CONSOLE
4843         elif virt_type == "uml":
4844             guest.os_kernel = "/usr/bin/linux"
4845             guest.os_root = root_device_name
4846         elif virt_type == "parallels":
4847             if guest.os_type == fields.VMMode.EXE:
4848                 guest.os_init_path = "/sbin/init"
4849 
4850     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
4851                     instance, inst_path, image_meta, disk_info):
4852         if rescue:
4853             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
4854                                        root_device_name)
4855         elif instance.kernel_id:
4856             self._set_guest_for_inst_kernel(instance, guest, inst_path,
4857                                             virt_type, root_device_name,
4858                                             image_meta)
4859         else:
4860             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
4861 
4862     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
4863                          image_meta):
4864         # NOTE(markus_z): Beware! Below are so many conditionals that it is
4865         # easy to lose track. Use this chart to figure out your case:
4866         #
4867         # case | is serial | has       | is qemu | resulting
4868         #      | enabled?  | virtlogd? | or kvm? | devices
4869         # --------------------------------------------------
4870         #    1 |        no |        no |     no  | pty*
4871         #    2 |        no |        no |     yes | file + pty
4872         #    3 |        no |       yes |      no | see case 1
4873         #    4 |        no |       yes |     yes | pty with logd
4874         #    5 |       yes |        no |      no | see case 1
4875         #    6 |       yes |        no |     yes | tcp + pty
4876         #    7 |       yes |       yes |      no | see case 1
4877         #    8 |       yes |       yes |     yes | tcp with logd
4878         #    * exception: virt_type "parallels" doesn't create a device
4879         if virt_type == 'parallels':
4880             pass
4881         elif virt_type not in ("qemu", "kvm"):
4882             log_path = self._get_console_log_path(instance)
4883             self._create_pty_device(guest_cfg,
4884                                     vconfig.LibvirtConfigGuestConsole,
4885                                     log_path=log_path)
4886         elif (virt_type in ("qemu", "kvm") and
4887                   self._is_s390x_guest(image_meta)):
4888             self._create_consoles_s390x(guest_cfg, instance,
4889                                         flavor, image_meta)
4890         elif virt_type in ("qemu", "kvm"):
4891             self._create_consoles_qemu_kvm(guest_cfg, instance,
4892                                         flavor, image_meta)
4893 
4894     def _is_s390x_guest(self, image_meta):
4895         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
4896         return libvirt_utils.get_arch(image_meta) in s390x_archs
4897 
4898     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
4899                                   image_meta):
4900         char_dev_cls = vconfig.LibvirtConfigGuestSerial
4901         log_path = self._get_console_log_path(instance)
4902         if CONF.serial_console.enabled:
4903             if not self._serial_ports_already_defined(instance):
4904                 num_ports = hardware.get_number_of_serial_ports(flavor,
4905                                                                 image_meta)
4906                 self._check_number_of_serial_console(num_ports)
4907                 self._create_serial_consoles(guest_cfg, num_ports,
4908                                              char_dev_cls, log_path)
4909         else:
4910             self._create_file_device(guest_cfg, instance, char_dev_cls)
4911         self._create_pty_device(guest_cfg, char_dev_cls, log_path=log_path)
4912 
4913     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
4914         char_dev_cls = vconfig.LibvirtConfigGuestConsole
4915         log_path = self._get_console_log_path(instance)
4916         if CONF.serial_console.enabled:
4917             if not self._serial_ports_already_defined(instance):
4918                 num_ports = hardware.get_number_of_serial_ports(flavor,
4919                                                                 image_meta)
4920                 self._create_serial_consoles(guest_cfg, num_ports,
4921                                              char_dev_cls, log_path)
4922         else:
4923             self._create_file_device(guest_cfg, instance, char_dev_cls,
4924                                      "sclplm")
4925         self._create_pty_device(guest_cfg, char_dev_cls, "sclp", log_path)
4926 
4927     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
4928                            log_path=None):
4929         def _create_base_dev():
4930             consolepty = char_dev_cls()
4931             consolepty.target_type = target_type
4932             consolepty.type = "pty"
4933             return consolepty
4934 
4935         def _create_logd_dev():
4936             consolepty = _create_base_dev()
4937             log = vconfig.LibvirtConfigGuestCharDeviceLog()
4938             log.file = log_path
4939             consolepty.log = log
4940             return consolepty
4941 
4942         if CONF.serial_console.enabled:
4943             if self._is_virtlogd_available():
4944                 return
4945             else:
4946                 # NOTE(markus_z): You may wonder why this is necessary and
4947                 # so do I. I'm certain that this is *not* needed in any
4948                 # real use case. It is, however, useful if you want to
4949                 # pypass the Nova API and use "virsh console <guest>" on
4950                 # an hypervisor, as this CLI command doesn't work with TCP
4951                 # devices (like the serial console is).
4952                 #     https://bugzilla.redhat.com/show_bug.cgi?id=781467
4953                 # Pypassing the Nova API however is a thing we don't want.
4954                 # Future changes should remove this and fix the unit tests
4955                 # which ask for the existence.
4956                 guest_cfg.add_device(_create_base_dev())
4957         else:
4958             if self._is_virtlogd_available():
4959                 guest_cfg.add_device(_create_logd_dev())
4960             else:
4961                 guest_cfg.add_device(_create_base_dev())
4962 
4963     def _create_file_device(self, guest_cfg, instance, char_dev_cls,
4964                             target_type=None):
4965         if self._is_virtlogd_available():
4966             return
4967 
4968         consolelog = char_dev_cls()
4969         consolelog.target_type = target_type
4970         consolelog.type = "file"
4971         consolelog.source_path = self._get_console_log_path(instance)
4972         guest_cfg.add_device(consolelog)
4973 
4974     def _serial_ports_already_defined(self, instance):
4975         try:
4976             guest = self._host.get_guest(instance)
4977             if list(self._get_serial_ports_from_guest(guest)):
4978                 # Serial port are already configured for instance that
4979                 # means we are in a context of migration.
4980                 return True
4981         except exception.InstanceNotFound:
4982             LOG.debug(
4983                 "Instance does not exist yet on libvirt, we can "
4984                 "safely pass on looking for already defined serial "
4985                 "ports in its domain XML", instance=instance)
4986         return False
4987 
4988     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
4989                                 log_path):
4990         for port in six.moves.range(num_ports):
4991             console = char_dev_cls()
4992             console.port = port
4993             console.type = "tcp"
4994             console.listen_host = CONF.serial_console.proxyclient_address
4995             listen_port = serial_console.acquire_port(console.listen_host)
4996             console.listen_port = listen_port
4997             # NOTE: only the first serial console gets the boot messages,
4998             # that's why we attach the logd subdevice only to that.
4999             if port == 0 and self._is_virtlogd_available():
5000                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
5001                 log.file = log_path
5002                 console.log = log
5003             guest_cfg.add_device(console)
5004 
5005     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
5006         """Update VirtCPUModel object according to libvirt CPU config.
5007 
5008         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
5009                            instance's virtual cpu configuration.
5010         :param:vcpu_model: VirtCPUModel object. A new object will be created
5011                            if None.
5012 
5013         :return: Updated VirtCPUModel object, or None if cpu_config is None
5014 
5015         """
5016 
5017         if not cpu_config:
5018             return
5019         if not vcpu_model:
5020             vcpu_model = objects.VirtCPUModel()
5021 
5022         vcpu_model.arch = cpu_config.arch
5023         vcpu_model.vendor = cpu_config.vendor
5024         vcpu_model.model = cpu_config.model
5025         vcpu_model.mode = cpu_config.mode
5026         vcpu_model.match = cpu_config.match
5027 
5028         if cpu_config.sockets:
5029             vcpu_model.topology = objects.VirtCPUTopology(
5030                 sockets=cpu_config.sockets,
5031                 cores=cpu_config.cores,
5032                 threads=cpu_config.threads)
5033         else:
5034             vcpu_model.topology = None
5035 
5036         features = [objects.VirtCPUFeature(
5037             name=f.name,
5038             policy=f.policy) for f in cpu_config.features]
5039         vcpu_model.features = features
5040 
5041         return vcpu_model
5042 
5043     def _vcpu_model_to_cpu_config(self, vcpu_model):
5044         """Create libvirt CPU config according to VirtCPUModel object.
5045 
5046         :param:vcpu_model: VirtCPUModel object.
5047 
5048         :return: vconfig.LibvirtConfigGuestCPU.
5049 
5050         """
5051 
5052         cpu_config = vconfig.LibvirtConfigGuestCPU()
5053         cpu_config.arch = vcpu_model.arch
5054         cpu_config.model = vcpu_model.model
5055         cpu_config.mode = vcpu_model.mode
5056         cpu_config.match = vcpu_model.match
5057         cpu_config.vendor = vcpu_model.vendor
5058         if vcpu_model.topology:
5059             cpu_config.sockets = vcpu_model.topology.sockets
5060             cpu_config.cores = vcpu_model.topology.cores
5061             cpu_config.threads = vcpu_model.topology.threads
5062         if vcpu_model.features:
5063             for f in vcpu_model.features:
5064                 xf = vconfig.LibvirtConfigGuestCPUFeature()
5065                 xf.name = f.name
5066                 xf.policy = f.policy
5067                 cpu_config.features.add(xf)
5068         return cpu_config
5069 
5070     def _guest_add_pcie_root_ports(self, guest):
5071         """Add PCI Express root ports.
5072 
5073         PCI Express machine can have as many PCIe devices as it has
5074         pcie-root-port controllers (slots in virtual motherboard).
5075 
5076         If we want to have more PCIe slots for hotplug then we need to create
5077         whole PCIe structure (libvirt limitation).
5078         """
5079 
5080         pcieroot = vconfig.LibvirtConfigGuestPCIeRootController()
5081         guest.add_device(pcieroot)
5082 
5083         for x in range(0, CONF.libvirt.num_pcie_ports):
5084             pcierootport = vconfig.LibvirtConfigGuestPCIeRootPortController()
5085             guest.add_device(pcierootport)
5086 
5087     def _guest_add_usb_host_keyboard(self, guest):
5088         """Add USB Host controller and keyboard for graphical console use.
5089 
5090         Add USB keyboard as PS/2 support may not be present on non-x86
5091         architectures.
5092         """
5093         keyboard = vconfig.LibvirtConfigGuestInput()
5094         keyboard.type = "keyboard"
5095         keyboard.bus = "usb"
5096         guest.add_device(keyboard)
5097 
5098         usbhost = vconfig.LibvirtConfigGuestUSBHostController()
5099         usbhost.index = 0
5100         guest.add_device(usbhost)
5101 
5102     def _get_guest_config(self, instance, network_info, image_meta,
5103                           disk_info, rescue=None, block_device_info=None,
5104                           context=None, mdevs=None):
5105         """Get config data for parameters.
5106 
5107         :param rescue: optional dictionary that should contain the key
5108             'ramdisk_id' if a ramdisk is needed for the rescue image and
5109             'kernel_id' if a kernel is needed for the rescue image.
5110 
5111         :param mdevs: optional list of mediated devices to assign to the guest.
5112         """
5113         flavor = instance.flavor
5114         inst_path = libvirt_utils.get_instance_path(instance)
5115         disk_mapping = disk_info['mapping']
5116 
5117         virt_type = CONF.libvirt.virt_type
5118         guest = vconfig.LibvirtConfigGuest()
5119         guest.virt_type = virt_type
5120         guest.name = instance.name
5121         guest.uuid = instance.uuid
5122         # We are using default unit for memory: KiB
5123         guest.memory = flavor.memory_mb * units.Ki
5124         guest.vcpus = flavor.vcpus
5125         allowed_cpus = hardware.get_vcpu_pin_set()
5126 
5127         guest_numa_config = self._get_guest_numa_config(
5128             instance.numa_topology, flavor, allowed_cpus, image_meta)
5129 
5130         guest.cpuset = guest_numa_config.cpuset
5131         guest.cputune = guest_numa_config.cputune
5132         guest.numatune = guest_numa_config.numatune
5133 
5134         guest.membacking = self._get_guest_memory_backing_config(
5135             instance.numa_topology,
5136             guest_numa_config.numatune,
5137             flavor)
5138 
5139         guest.metadata.append(self._get_guest_config_meta(instance))
5140         guest.idmaps = self._get_guest_idmaps()
5141 
5142         for event in self._supported_perf_events:
5143             guest.add_perf_event(event)
5144 
5145         self._update_guest_cputune(guest, flavor, virt_type)
5146 
5147         guest.cpu = self._get_guest_cpu_config(
5148             flavor, image_meta, guest_numa_config.numaconfig,
5149             instance.numa_topology)
5150 
5151         # Notes(yjiang5): we always sync the instance's vcpu model with
5152         # the corresponding config file.
5153         instance.vcpu_model = self._cpu_config_to_vcpu_model(
5154             guest.cpu, instance.vcpu_model)
5155 
5156         if 'root' in disk_mapping:
5157             root_device_name = block_device.prepend_dev(
5158                 disk_mapping['root']['dev'])
5159         else:
5160             root_device_name = None
5161 
5162         if root_device_name:
5163             # NOTE(yamahata):
5164             # for nova.api.ec2.cloud.CloudController.get_metadata()
5165             instance.root_device_name = root_device_name
5166 
5167         guest.os_type = (fields.VMMode.get_from_instance(instance) or
5168                 self._get_guest_os_type(virt_type))
5169         caps = self._host.get_capabilities()
5170 
5171         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
5172                                            image_meta, flavor,
5173                                            root_device_name)
5174         if virt_type not in ('lxc', 'uml'):
5175             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
5176                     instance, inst_path, image_meta, disk_info)
5177 
5178         self._set_features(guest, instance.os_type, caps, virt_type,
5179                            image_meta)
5180         self._set_clock(guest, instance.os_type, image_meta, virt_type)
5181 
5182         storage_configs = self._get_guest_storage_config(context,
5183                 instance, image_meta, disk_info, rescue, block_device_info,
5184                 flavor, guest.os_type)
5185         for config in storage_configs:
5186             guest.add_device(config)
5187 
5188         for vif in network_info:
5189             config = self.vif_driver.get_config(
5190                 instance, vif, image_meta,
5191                 flavor, virt_type, self._host)
5192             guest.add_device(config)
5193 
5194         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
5195 
5196         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
5197         if pointer:
5198             guest.add_device(pointer)
5199 
5200         self._guest_add_spice_channel(guest)
5201 
5202         if self._guest_add_video_device(guest):
5203             self._add_video_driver(guest, image_meta, flavor)
5204 
5205             # We want video == we want graphical console. Some architectures
5206             # do not have input devices attached in default configuration.
5207             # Let then add USB Host controller and USB keyboard.
5208             # x86(-64) and ppc64 have usb host controller and keyboard
5209             # s390x does not support USB
5210             if caps.host.cpu.arch == fields.Architecture.AARCH64:
5211                 self._guest_add_usb_host_keyboard(guest)
5212 
5213         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
5214         if virt_type in ('qemu', 'kvm'):
5215             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
5216 
5217         # Add PCIe root port controllers for PCI Express machines
5218         # but only if their amount is configured
5219         if (CONF.libvirt.num_pcie_ports and
5220                 ((caps.host.cpu.arch == fields.Architecture.AARCH64 and
5221                 guest.os_mach_type.startswith('virt')) or
5222                 (caps.host.cpu.arch == fields.Architecture.X86_64 and
5223                 guest.os_mach_type is not None and
5224                 'q35' in guest.os_mach_type))):
5225             self._guest_add_pcie_root_ports(guest)
5226 
5227         self._guest_add_pci_devices(guest, instance)
5228 
5229         self._guest_add_watchdog_action(guest, flavor, image_meta)
5230 
5231         self._guest_add_memory_balloon(guest)
5232 
5233         if mdevs:
5234             self._guest_add_mdevs(guest, mdevs)
5235 
5236         return guest
5237 
5238     def _guest_add_mdevs(self, guest, chosen_mdevs):
5239         for chosen_mdev in chosen_mdevs:
5240             mdev = vconfig.LibvirtConfigGuestHostdevMDEV()
5241             mdev.uuid = chosen_mdev
5242             guest.add_device(mdev)
5243 
5244     @staticmethod
5245     def _guest_add_spice_channel(guest):
5246         if (CONF.spice.enabled and CONF.spice.agent_enabled
5247                 and guest.virt_type not in ('lxc', 'uml', 'xen')):
5248             channel = vconfig.LibvirtConfigGuestChannel()
5249             channel.type = 'spicevmc'
5250             channel.target_name = "com.redhat.spice.0"
5251             guest.add_device(channel)
5252 
5253     @staticmethod
5254     def _guest_add_memory_balloon(guest):
5255         virt_type = guest.virt_type
5256         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
5257         if (virt_type in ('xen', 'qemu', 'kvm') and
5258                     CONF.libvirt.mem_stats_period_seconds > 0):
5259             balloon = vconfig.LibvirtConfigMemoryBalloon()
5260             if virt_type in ('qemu', 'kvm'):
5261                 balloon.model = 'virtio'
5262             else:
5263                 balloon.model = 'xen'
5264             balloon.period = CONF.libvirt.mem_stats_period_seconds
5265             guest.add_device(balloon)
5266 
5267     @staticmethod
5268     def _guest_add_watchdog_action(guest, flavor, image_meta):
5269         # image meta takes precedence over flavor extra specs; disable the
5270         # watchdog action by default
5271         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action')
5272                            or 'disabled')
5273         watchdog_action = image_meta.properties.get('hw_watchdog_action',
5274                                                     watchdog_action)
5275         # NB(sross): currently only actually supported by KVM/QEmu
5276         if watchdog_action != 'disabled':
5277             if watchdog_action in fields.WatchdogAction.ALL:
5278                 bark = vconfig.LibvirtConfigGuestWatchdog()
5279                 bark.action = watchdog_action
5280                 guest.add_device(bark)
5281             else:
5282                 raise exception.InvalidWatchdogAction(action=watchdog_action)
5283 
5284     def _guest_add_pci_devices(self, guest, instance):
5285         virt_type = guest.virt_type
5286         if virt_type in ('xen', 'qemu', 'kvm'):
5287             # Get all generic PCI devices (non-SR-IOV).
5288             for pci_dev in pci_manager.get_instance_pci_devs(instance):
5289                 guest.add_device(self._get_guest_pci_device(pci_dev))
5290         else:
5291             # PCI devices is only supported for hypervisors
5292             #  'xen', 'qemu' and 'kvm'.
5293             if pci_manager.get_instance_pci_devs(instance, 'all'):
5294                 raise exception.PciDeviceUnsupportedHypervisor(type=virt_type)
5295 
5296     @staticmethod
5297     def _guest_add_video_device(guest):
5298         # NB some versions of libvirt support both SPICE and VNC
5299         # at the same time. We're not trying to second guess which
5300         # those versions are. We'll just let libvirt report the
5301         # errors appropriately if the user enables both.
5302         add_video_driver = False
5303         if CONF.vnc.enabled and guest.virt_type not in ('lxc', 'uml'):
5304             graphics = vconfig.LibvirtConfigGuestGraphics()
5305             graphics.type = "vnc"
5306             if CONF.vnc.keymap:
5307                 graphics.keymap = CONF.vnc.keymap
5308             graphics.listen = CONF.vnc.server_listen
5309             guest.add_device(graphics)
5310             add_video_driver = True
5311         if CONF.spice.enabled and guest.virt_type not in ('lxc', 'uml', 'xen'):
5312             graphics = vconfig.LibvirtConfigGuestGraphics()
5313             graphics.type = "spice"
5314             if CONF.spice.keymap:
5315                 graphics.keymap = CONF.spice.keymap
5316             graphics.listen = CONF.spice.server_listen
5317             guest.add_device(graphics)
5318             add_video_driver = True
5319         return add_video_driver
5320 
5321     def _get_guest_pointer_model(self, os_type, image_meta):
5322         pointer_model = image_meta.properties.get(
5323             'hw_pointer_model', CONF.pointer_model)
5324         if pointer_model is None and CONF.libvirt.use_usb_tablet:
5325             # TODO(sahid): We set pointer_model to keep compatibility
5326             # until the next release O*. It means operators can continue
5327             # to use the deprecated option "use_usb_tablet" or set a
5328             # specific device to use
5329             pointer_model = "usbtablet"
5330             LOG.warning('The option "use_usb_tablet" has been '
5331                         'deprecated for Newton in favor of the more '
5332                         'generic "pointer_model". Please update '
5333                         'nova.conf to address this change.')
5334 
5335         if pointer_model == "usbtablet":
5336             # We want a tablet if VNC is enabled, or SPICE is enabled and
5337             # the SPICE agent is disabled. If the SPICE agent is enabled
5338             # it provides a paravirt mouse which drastically reduces
5339             # overhead (by eliminating USB polling).
5340             if CONF.vnc.enabled or (
5341                     CONF.spice.enabled and not CONF.spice.agent_enabled):
5342                 return self._get_guest_usb_tablet(os_type)
5343             else:
5344                 if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5345                     # For backward compatibility We don't want to break
5346                     # process of booting an instance if host is configured
5347                     # to use USB tablet without VNC or SPICE and SPICE
5348                     # agent disable.
5349                     LOG.warning('USB tablet requested for guests by host '
5350                                 'configuration. In order to accept this '
5351                                 'request VNC should be enabled or SPICE '
5352                                 'and SPICE agent disabled on host.')
5353                 else:
5354                     raise exception.UnsupportedPointerModelRequested(
5355                         model="usbtablet")
5356 
5357     def _get_guest_usb_tablet(self, os_type):
5358         tablet = None
5359         if os_type == fields.VMMode.HVM:
5360             tablet = vconfig.LibvirtConfigGuestInput()
5361             tablet.type = "tablet"
5362             tablet.bus = "usb"
5363         else:
5364             if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5365                 # For backward compatibility We don't want to break
5366                 # process of booting an instance if virtual machine mode
5367                 # is not configured as HVM.
5368                 LOG.warning('USB tablet requested for guests by host '
5369                             'configuration. In order to accept this '
5370                             'request the machine mode should be '
5371                             'configured as HVM.')
5372             else:
5373                 raise exception.UnsupportedPointerModelRequested(
5374                     model="usbtablet")
5375         return tablet
5376 
5377     def _get_guest_xml(self, context, instance, network_info, disk_info,
5378                        image_meta, rescue=None,
5379                        block_device_info=None,
5380                        mdevs=None):
5381         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
5382         # this ahead of time so that we don't acquire it while also
5383         # holding the logging lock.
5384         network_info_str = str(network_info)
5385         msg = ('Start _get_guest_xml '
5386                'network_info=%(network_info)s '
5387                'disk_info=%(disk_info)s '
5388                'image_meta=%(image_meta)s rescue=%(rescue)s '
5389                'block_device_info=%(block_device_info)s' %
5390                {'network_info': network_info_str, 'disk_info': disk_info,
5391                 'image_meta': image_meta, 'rescue': rescue,
5392                 'block_device_info': block_device_info})
5393         # NOTE(mriedem): block_device_info can contain auth_password so we
5394         # need to sanitize the password in the message.
5395         LOG.debug(strutils.mask_password(msg), instance=instance)
5396         conf = self._get_guest_config(instance, network_info, image_meta,
5397                                       disk_info, rescue, block_device_info,
5398                                       context, mdevs)
5399         xml = conf.to_xml()
5400 
5401         LOG.debug('End _get_guest_xml xml=%(xml)s',
5402                   {'xml': xml}, instance=instance)
5403         return xml
5404 
5405     def get_info(self, instance):
5406         """Retrieve information from libvirt for a specific instance.
5407 
5408         If a libvirt error is encountered during lookup, we might raise a
5409         NotFound exception or Error exception depending on how severe the
5410         libvirt error is.
5411 
5412         :param instance: nova.objects.instance.Instance object
5413         :returns: An InstanceInfo object
5414         """
5415         guest = self._host.get_guest(instance)
5416         # Kind of ugly but we need to pass host to get_info as for a
5417         # workaround, see libvirt/compat.py
5418         return guest.get_info(self._host)
5419 
5420     def _create_domain_setup_lxc(self, context, instance, image_meta,
5421                                  block_device_info):
5422         inst_path = libvirt_utils.get_instance_path(instance)
5423         block_device_mapping = driver.block_device_info_get_mapping(
5424             block_device_info)
5425         root_disk = block_device.get_root_bdm(block_device_mapping)
5426         if root_disk:
5427             self._connect_volume(context, root_disk['connection_info'],
5428                                  instance)
5429             disk_path = root_disk['connection_info']['data']['device_path']
5430 
5431             # NOTE(apmelton) - Even though the instance is being booted from a
5432             # cinder volume, it is still presented as a local block device.
5433             # LocalBlockImage is used here to indicate that the instance's
5434             # disk is backed by a local block device.
5435             image_model = imgmodel.LocalBlockImage(disk_path)
5436         else:
5437             root_disk = self.image_backend.by_name(instance, 'disk')
5438             image_model = root_disk.get_model(self._conn)
5439 
5440         container_dir = os.path.join(inst_path, 'rootfs')
5441         fileutils.ensure_tree(container_dir)
5442         rootfs_dev = disk_api.setup_container(image_model,
5443                                               container_dir=container_dir)
5444 
5445         try:
5446             # Save rootfs device to disconnect it when deleting the instance
5447             if rootfs_dev:
5448                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
5449             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
5450                 id_maps = self._get_guest_idmaps()
5451                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
5452         except Exception:
5453             with excutils.save_and_reraise_exception():
5454                 self._create_domain_cleanup_lxc(instance)
5455 
5456     def _create_domain_cleanup_lxc(self, instance):
5457         inst_path = libvirt_utils.get_instance_path(instance)
5458         container_dir = os.path.join(inst_path, 'rootfs')
5459 
5460         try:
5461             state = self.get_info(instance).state
5462         except exception.InstanceNotFound:
5463             # The domain may not be present if the instance failed to start
5464             state = None
5465 
5466         if state == power_state.RUNNING:
5467             # NOTE(uni): Now the container is running with its own private
5468             # mount namespace and so there is no need to keep the container
5469             # rootfs mounted in the host namespace
5470             LOG.debug('Attempting to unmount container filesystem: %s',
5471                       container_dir, instance=instance)
5472             disk_api.clean_lxc_namespace(container_dir=container_dir)
5473         else:
5474             disk_api.teardown_container(container_dir=container_dir)
5475 
5476     @contextlib.contextmanager
5477     def _lxc_disk_handler(self, context, instance, image_meta,
5478                           block_device_info):
5479         """Context manager to handle the pre and post instance boot,
5480            LXC specific disk operations.
5481 
5482            An image or a volume path will be prepared and setup to be
5483            used by the container, prior to starting it.
5484            The disk will be disconnected and unmounted if a container has
5485            failed to start.
5486         """
5487 
5488         if CONF.libvirt.virt_type != 'lxc':
5489             yield
5490             return
5491 
5492         self._create_domain_setup_lxc(context, instance, image_meta,
5493                                       block_device_info)
5494 
5495         try:
5496             yield
5497         finally:
5498             self._create_domain_cleanup_lxc(instance)
5499 
5500     # TODO(sahid): Consider renaming this to _create_guest.
5501     def _create_domain(self, xml=None, domain=None,
5502                        power_on=True, pause=False, post_xml_callback=None):
5503         """Create a domain.
5504 
5505         Either domain or xml must be passed in. If both are passed, then
5506         the domain definition is overwritten from the xml.
5507 
5508         :returns guest.Guest: Guest just created
5509         """
5510         if xml:
5511             guest = libvirt_guest.Guest.create(xml, self._host)
5512             if post_xml_callback is not None:
5513                 post_xml_callback()
5514         else:
5515             guest = libvirt_guest.Guest(domain)
5516 
5517         if power_on or pause:
5518             guest.launch(pause=pause)
5519 
5520         if not utils.is_neutron():
5521             guest.enable_hairpin()
5522 
5523         return guest
5524 
5525     def _neutron_failed_callback(self, event_name, instance):
5526         LOG.error('Neutron Reported failure on event '
5527                   '%(event)s for instance %(uuid)s',
5528                   {'event': event_name, 'uuid': instance.uuid},
5529                   instance=instance)
5530         if CONF.vif_plugging_is_fatal:
5531             raise exception.VirtualInterfaceCreateException()
5532 
5533     def _get_neutron_events(self, network_info):
5534         # NOTE(danms): We need to collect any VIFs that are currently
5535         # down that we expect a down->up event for. Anything that is
5536         # already up will not undergo that transition, and for
5537         # anything that might be stale (cache-wise) assume it's
5538         # already up so we don't block on it.
5539         return [('network-vif-plugged', vif['id'])
5540                 for vif in network_info if vif.get('active', True) is False]
5541 
5542     def _cleanup_failed_start(self, context, instance, network_info,
5543                               block_device_info, guest, destroy_disks):
5544         try:
5545             if guest and guest.is_active():
5546                 guest.poweroff()
5547         finally:
5548             self.cleanup(context, instance, network_info=network_info,
5549                          block_device_info=block_device_info,
5550                          destroy_disks=destroy_disks)
5551 
5552     def _create_domain_and_network(self, context, xml, instance, network_info,
5553                                    block_device_info=None, power_on=True,
5554                                    vifs_already_plugged=False,
5555                                    post_xml_callback=None,
5556                                    destroy_disks_on_failure=False):
5557 
5558         """Do required network setup and create domain."""
5559         timeout = CONF.vif_plugging_timeout
5560         if (self._conn_supports_start_paused and
5561             utils.is_neutron() and not
5562             vifs_already_plugged and power_on and timeout):
5563             events = self._get_neutron_events(network_info)
5564         else:
5565             events = []
5566 
5567         pause = bool(events)
5568         guest = None
5569         try:
5570             with self.virtapi.wait_for_instance_event(
5571                     instance, events, deadline=timeout,
5572                     error_callback=self._neutron_failed_callback):
5573                 self.plug_vifs(instance, network_info)
5574                 self.firewall_driver.setup_basic_filtering(instance,
5575                                                            network_info)
5576                 self.firewall_driver.prepare_instance_filter(instance,
5577                                                              network_info)
5578                 with self._lxc_disk_handler(context, instance,
5579                                             instance.image_meta,
5580                                             block_device_info):
5581                     guest = self._create_domain(
5582                         xml, pause=pause, power_on=power_on,
5583                         post_xml_callback=post_xml_callback)
5584 
5585                 self.firewall_driver.apply_instance_filter(instance,
5586                                                            network_info)
5587         except exception.VirtualInterfaceCreateException:
5588             # Neutron reported failure and we didn't swallow it, so
5589             # bail here
5590             with excutils.save_and_reraise_exception():
5591                 self._cleanup_failed_start(context, instance, network_info,
5592                                            block_device_info, guest,
5593                                            destroy_disks_on_failure)
5594         except eventlet.timeout.Timeout:
5595             # We never heard from Neutron
5596             LOG.warning('Timeout waiting for %(events)s for '
5597                         'instance with vm_state %(vm_state)s and '
5598                         'task_state %(task_state)s.',
5599                         {'events': events,
5600                          'vm_state': instance.vm_state,
5601                          'task_state': instance.task_state},
5602                         instance=instance)
5603             if CONF.vif_plugging_is_fatal:
5604                 self._cleanup_failed_start(context, instance, network_info,
5605                                            block_device_info, guest,
5606                                            destroy_disks_on_failure)
5607                 raise exception.VirtualInterfaceCreateException()
5608         except Exception:
5609             # Any other error, be sure to clean up
5610             LOG.error('Failed to start libvirt guest', instance=instance)
5611             with excutils.save_and_reraise_exception():
5612                 self._cleanup_failed_start(context, instance, network_info,
5613                                            block_device_info, guest,
5614                                            destroy_disks_on_failure)
5615 
5616         # Resume only if domain has been paused
5617         if pause:
5618             guest.resume()
5619         return guest
5620 
5621     def _get_vcpu_total(self):
5622         """Get available vcpu number of physical computer.
5623 
5624         :returns: the number of cpu core instances can be used.
5625 
5626         """
5627         try:
5628             total_pcpus = self._host.get_cpu_count()
5629         except libvirt.libvirtError:
5630             LOG.warning("Cannot get the number of cpu, because this "
5631                         "function is not implemented for this platform. ")
5632             return 0
5633 
5634         if not CONF.vcpu_pin_set:
5635             return total_pcpus
5636 
5637         available_ids = hardware.get_vcpu_pin_set()
5638         # We get the list of online CPUs on the host and see if the requested
5639         # set falls under these. If not, we retain the old behavior.
5640         online_pcpus = None
5641         try:
5642             online_pcpus = self._host.get_online_cpus()
5643         except libvirt.libvirtError as ex:
5644             error_code = ex.get_error_code()
5645             err_msg = encodeutils.exception_to_unicode(ex)
5646             LOG.warning(
5647                 "Couldn't retrieve the online CPUs due to a Libvirt "
5648                 "error: %(error)s with error code: %(error_code)s",
5649                 {'error': err_msg, 'error_code': error_code})
5650         if online_pcpus:
5651             if not (available_ids <= online_pcpus):
5652                 msg = (_("Invalid vcpu_pin_set config, one or more of the "
5653                          "specified cpuset is not online. Online cpuset(s): "
5654                          "%(online)s, requested cpuset(s): %(req)s"),
5655                        {'online': sorted(online_pcpus),
5656                         'req': sorted(available_ids)})
5657                 raise exception.Invalid(msg)
5658         elif sorted(available_ids)[-1] >= total_pcpus:
5659             raise exception.Invalid(_("Invalid vcpu_pin_set config, "
5660                                       "out of hypervisor cpu range."))
5661         return len(available_ids)
5662 
5663     @staticmethod
5664     def _get_local_gb_info():
5665         """Get local storage info of the compute node in GB.
5666 
5667         :returns: A dict containing:
5668              :total: How big the overall usable filesystem is (in gigabytes)
5669              :free: How much space is free (in gigabytes)
5670              :used: How much space is used (in gigabytes)
5671         """
5672 
5673         if CONF.libvirt.images_type == 'lvm':
5674             info = lvm.get_volume_group_info(
5675                                CONF.libvirt.images_volume_group)
5676         elif CONF.libvirt.images_type == 'rbd':
5677             info = LibvirtDriver._get_rbd_driver().get_pool_info()
5678         else:
5679             info = libvirt_utils.get_fs_info(CONF.instances_path)
5680 
5681         for (k, v) in info.items():
5682             info[k] = v / units.Gi
5683 
5684         return info
5685 
5686     def _get_vcpu_used(self):
5687         """Get vcpu usage number of physical computer.
5688 
5689         :returns: The total number of vcpu(s) that are currently being used.
5690 
5691         """
5692 
5693         total = 0
5694 
5695         # Not all libvirt drivers will support the get_vcpus_info()
5696         #
5697         # For example, LXC does not have a concept of vCPUs, while
5698         # QEMU (TCG) traditionally handles all vCPUs in a single
5699         # thread. So both will report an exception when the vcpus()
5700         # API call is made. In such a case we should report the
5701         # guest as having 1 vCPU, since that lets us still do
5702         # CPU over commit calculations that apply as the total
5703         # guest count scales.
5704         #
5705         # It is also possible that we might see an exception if
5706         # the guest is just in middle of shutting down. Technically
5707         # we should report 0 for vCPU usage in this case, but we
5708         # we can't reliably distinguish the vcpu not supported
5709         # case from the just shutting down case. Thus we don't know
5710         # whether to report 1 or 0 for vCPU count.
5711         #
5712         # Under-reporting vCPUs is bad because it could conceivably
5713         # let the scheduler place too many guests on the host. Over-
5714         # reporting vCPUs is not a problem as it'll auto-correct on
5715         # the next refresh of usage data.
5716         #
5717         # Thus when getting an exception we always report 1 as the
5718         # vCPU count, as the least worst value.
5719         for guest in self._host.list_guests():
5720             try:
5721                 vcpus = guest.get_vcpus_info()
5722                 total += len(list(vcpus))
5723             except libvirt.libvirtError:
5724                 total += 1
5725             # NOTE(gtt116): give other tasks a chance.
5726             greenthread.sleep(0)
5727         return total
5728 
5729     def _get_supported_vgpu_types(self):
5730         if not CONF.devices.enabled_vgpu_types:
5731             return []
5732         # TODO(sbauza): Move this check up to compute_manager.init_host
5733         if len(CONF.devices.enabled_vgpu_types) > 1:
5734             LOG.warning('libvirt only supports one GPU type per compute node,'
5735                         ' only first type will be used.')
5736         requested_types = CONF.devices.enabled_vgpu_types[:1]
5737         return requested_types
5738 
5739     def _get_vgpu_total(self):
5740         """Returns the number of total available vGPUs for any GPU type that is
5741         enabled with the enabled_vgpu_types CONF option.
5742         """
5743         requested_types = self._get_supported_vgpu_types()
5744         # Bail out early if operator doesn't care about providing vGPUs
5745         if not requested_types:
5746             return 0
5747         # Filter how many available mdevs we can create for all the supported
5748         # types.
5749         mdev_capable_devices = self._get_mdev_capable_devices(requested_types)
5750         vgpus = 0
5751         for dev in mdev_capable_devices:
5752             for _type in dev['types']:
5753                 vgpus += dev['types'][_type]['availableInstances']
5754         # Count the already created (but possibly not assigned to a guest)
5755         # mdevs for all the supported types
5756         mediated_devices = self._get_mediated_devices(requested_types)
5757         vgpus += len(mediated_devices)
5758         return vgpus
5759 
5760     def _get_instance_capabilities(self):
5761         """Get hypervisor instance capabilities
5762 
5763         Returns a list of tuples that describe instances the
5764         hypervisor is capable of hosting.  Each tuple consists
5765         of the triplet (arch, hypervisor_type, vm_mode).
5766 
5767         Supported hypervisor_type is filtered by virt_type,
5768         a parameter set by operators via `nova.conf`.
5769 
5770         :returns: List of tuples describing instance capabilities
5771         """
5772         caps = self._host.get_capabilities()
5773         instance_caps = list()
5774         for g in caps.guests:
5775             for dt in g.domtype:
5776                 if dt != CONF.libvirt.virt_type:
5777                     continue
5778                 instance_cap = (
5779                     fields.Architecture.canonicalize(g.arch),
5780                     fields.HVType.canonicalize(dt),
5781                     fields.VMMode.canonicalize(g.ostype))
5782                 instance_caps.append(instance_cap)
5783 
5784         return instance_caps
5785 
5786     def _get_cpu_info(self):
5787         """Get cpuinfo information.
5788 
5789         Obtains cpu feature from virConnect.getCapabilities.
5790 
5791         :return: see above description
5792 
5793         """
5794 
5795         caps = self._host.get_capabilities()
5796         cpu_info = dict()
5797 
5798         cpu_info['arch'] = caps.host.cpu.arch
5799         cpu_info['model'] = caps.host.cpu.model
5800         cpu_info['vendor'] = caps.host.cpu.vendor
5801 
5802         topology = dict()
5803         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
5804         topology['sockets'] = caps.host.cpu.sockets
5805         topology['cores'] = caps.host.cpu.cores
5806         topology['threads'] = caps.host.cpu.threads
5807         cpu_info['topology'] = topology
5808 
5809         features = set()
5810         for f in caps.host.cpu.features:
5811             features.add(f.name)
5812         cpu_info['features'] = features
5813         return cpu_info
5814 
5815     def _get_pcinet_info(self, vf_address):
5816         """Returns a dict of NET device."""
5817         devname = pci_utils.get_net_name_by_vf_pci_address(vf_address)
5818         if not devname:
5819             return
5820 
5821         virtdev = self._host.device_lookup_by_name(devname)
5822         xmlstr = virtdev.XMLDesc(0)
5823         cfgdev = vconfig.LibvirtConfigNodeDevice()
5824         cfgdev.parse_str(xmlstr)
5825         return {'name': cfgdev.name,
5826                 'capabilities': cfgdev.pci_capability.features}
5827 
5828     def _get_pcidev_info(self, devname):
5829         """Returns a dict of PCI device."""
5830 
5831         def _get_device_type(cfgdev, pci_address):
5832             """Get a PCI device's device type.
5833 
5834             An assignable PCI device can be a normal PCI device,
5835             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
5836             Function (VF). Only normal PCI devices or SR-IOV VFs
5837             are assignable, while SR-IOV PFs are always owned by
5838             hypervisor.
5839             """
5840             for fun_cap in cfgdev.pci_capability.fun_capability:
5841                 if fun_cap.type == 'virt_functions':
5842                     return {
5843                         'dev_type': fields.PciDeviceType.SRIOV_PF,
5844                     }
5845                 if (fun_cap.type == 'phys_function' and
5846                     len(fun_cap.device_addrs) != 0):
5847                     phys_address = "%04x:%02x:%02x.%01x" % (
5848                         fun_cap.device_addrs[0][0],
5849                         fun_cap.device_addrs[0][1],
5850                         fun_cap.device_addrs[0][2],
5851                         fun_cap.device_addrs[0][3])
5852                     return {
5853                         'dev_type': fields.PciDeviceType.SRIOV_VF,
5854                         'parent_addr': phys_address,
5855                     }
5856 
5857             # Note(moshele): libvirt < 1.3 reported virt_functions capability
5858             # only when VFs are enabled. The check below is a workaround
5859             # to get the correct report regardless of whether or not any
5860             # VFs are enabled for the device.
5861             if not self._host.has_min_version(
5862                 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION):
5863                 is_physical_function = pci_utils.is_physical_function(
5864                     *pci_utils.get_pci_address_fields(pci_address))
5865                 if is_physical_function:
5866                     return {'dev_type': fields.PciDeviceType.SRIOV_PF}
5867 
5868             return {'dev_type': fields.PciDeviceType.STANDARD}
5869 
5870         def _get_device_capabilities(device, address):
5871             """Get PCI VF device's additional capabilities.
5872 
5873             If a PCI device is a virtual function, this function reads the PCI
5874             parent's network capabilities (must be always a NIC device) and
5875             appends this information to the device's dictionary.
5876             """
5877             if device.get('dev_type') == fields.PciDeviceType.SRIOV_VF:
5878                 pcinet_info = self._get_pcinet_info(address)
5879                 if pcinet_info:
5880                     return {'capabilities':
5881                                 {'network': pcinet_info.get('capabilities')}}
5882             return {}
5883 
5884         virtdev = self._host.device_lookup_by_name(devname)
5885         xmlstr = virtdev.XMLDesc(0)
5886         cfgdev = vconfig.LibvirtConfigNodeDevice()
5887         cfgdev.parse_str(xmlstr)
5888 
5889         address = "%04x:%02x:%02x.%1x" % (
5890             cfgdev.pci_capability.domain,
5891             cfgdev.pci_capability.bus,
5892             cfgdev.pci_capability.slot,
5893             cfgdev.pci_capability.function)
5894 
5895         device = {
5896             "dev_id": cfgdev.name,
5897             "address": address,
5898             "product_id": "%04x" % cfgdev.pci_capability.product_id,
5899             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
5900             }
5901 
5902         device["numa_node"] = cfgdev.pci_capability.numa_node
5903 
5904         # requirement by DataBase Model
5905         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
5906         device.update(_get_device_type(cfgdev, address))
5907         device.update(_get_device_capabilities(device, address))
5908         return device
5909 
5910     def _get_pci_passthrough_devices(self):
5911         """Get host PCI devices information.
5912 
5913         Obtains pci devices information from libvirt, and returns
5914         as a JSON string.
5915 
5916         Each device information is a dictionary, with mandatory keys
5917         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
5918         'label' and other optional device specific information.
5919 
5920         Refer to the objects/pci_device.py for more idea of these keys.
5921 
5922         :returns: a JSON string containing a list of the assignable PCI
5923                   devices information
5924         """
5925         # Bail early if we know we can't support `listDevices` to avoid
5926         # repeated warnings within a periodic task
5927         if not getattr(self, '_list_devices_supported', True):
5928             return jsonutils.dumps([])
5929 
5930         try:
5931             dev_names = self._host.list_pci_devices() or []
5932         except libvirt.libvirtError as ex:
5933             error_code = ex.get_error_code()
5934             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
5935                 self._list_devices_supported = False
5936                 LOG.warning("URI %(uri)s does not support "
5937                             "listDevices: %(error)s",
5938                             {'uri': self._uri(),
5939                              'error': encodeutils.exception_to_unicode(ex)})
5940                 return jsonutils.dumps([])
5941             else:
5942                 raise
5943 
5944         pci_info = []
5945         for name in dev_names:
5946             pci_info.append(self._get_pcidev_info(name))
5947 
5948         return jsonutils.dumps(pci_info)
5949 
5950     def _get_mdev_capabilities_for_dev(self, devname, types=None):
5951         """Returns a dict of MDEV capable device with the ID as first key
5952         and then a list of supported types, each of them being a dict.
5953 
5954         :param types: Only return those specific types.
5955         """
5956         virtdev = self._host.device_lookup_by_name(devname)
5957         xmlstr = virtdev.XMLDesc(0)
5958         cfgdev = vconfig.LibvirtConfigNodeDevice()
5959         cfgdev.parse_str(xmlstr)
5960 
5961         device = {
5962             "dev_id": cfgdev.name,
5963             "types": {},
5964         }
5965         for mdev_cap in cfgdev.pci_capability.mdev_capability:
5966             for cap in mdev_cap.mdev_types:
5967                 if not types or cap['type'] in types:
5968                     device["types"].update({cap['type']: {
5969                         'availableInstances': cap['availableInstances'],
5970                         'name': cap['name'],
5971                         'deviceAPI': cap['deviceAPI']}})
5972         return device
5973 
5974     def _get_mdev_capable_devices(self, types=None):
5975         """Get host devices supporting mdev types.
5976 
5977         Obtain devices information from libvirt and returns a list of
5978         dictionaries.
5979 
5980         :param types: Filter only devices supporting those types.
5981         """
5982         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
5983             return []
5984         dev_names = self._host.list_mdev_capable_devices() or []
5985         mdev_capable_devices = []
5986         for name in dev_names:
5987             device = self._get_mdev_capabilities_for_dev(name, types)
5988             if not device["types"]:
5989                 continue
5990             mdev_capable_devices.append(device)
5991         return mdev_capable_devices
5992 
5993     def _get_mediated_device_information(self, devname):
5994         """Returns a dict of a mediated device."""
5995         virtdev = self._host.device_lookup_by_name(devname)
5996         xmlstr = virtdev.XMLDesc(0)
5997         cfgdev = vconfig.LibvirtConfigNodeDevice()
5998         cfgdev.parse_str(xmlstr)
5999 
6000         device = {
6001             "dev_id": cfgdev.name,
6002             # name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
6003             "uuid": str(uuid.UUID(cfgdev.name[5:].replace('_', '-'))),
6004             "type": cfgdev.mdev_information.type,
6005             "iommu_group": cfgdev.mdev_information.iommu_group,
6006         }
6007         return device
6008 
6009     def _get_mediated_devices(self, types=None):
6010         """Get host mediated devices.
6011 
6012         Obtain devices information from libvirt and returns a list of
6013         dictionaries.
6014 
6015         :param types: Filter only devices supporting those types.
6016         """
6017         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
6018             return []
6019         dev_names = self._host.list_mediated_devices() or []
6020         mediated_devices = []
6021         for name in dev_names:
6022             device = self._get_mediated_device_information(name)
6023             if not types or device["type"] in types:
6024                 mediated_devices.append(device)
6025         return mediated_devices
6026 
6027     def _get_all_assigned_mediated_devices(self, instance=None):
6028         """Lookup all instances from the host and return all the mediated
6029         devices that are assigned to a guest.
6030 
6031         :param instance: Only return mediated devices for that instance.
6032 
6033         :returns: A dictionary of keys being mediated device UUIDs and their
6034                   respective values the instance UUID of the guest using it.
6035         """
6036         allocated_mdevs = {}
6037         if instance:
6038             guest = self._host.get_guest(instance)
6039             guests = [guest]
6040         else:
6041             guests = self._host.list_guests(only_running=False)
6042         for guest in guests:
6043             cfg = guest.get_config()
6044             for device in cfg.devices:
6045                 if isinstance(device, vconfig.LibvirtConfigGuestHostdevMDEV):
6046                     allocated_mdevs[device.uuid] = guest.uuid
6047         return allocated_mdevs
6048 
6049     @staticmethod
6050     def _vgpu_allocations(allocations):
6051         """Filtering only the VGPU allocations from a list of allocations.
6052 
6053         :param allocations: Information about resources allocated to the
6054                             instance via placement, of the form returned by
6055                             SchedulerReportClient.get_allocations_for_consumer.
6056         """
6057         if not allocations:
6058             # If no allocations, there is no vGPU request.
6059             return {}
6060         RC_VGPU = rc_fields.ResourceClass.VGPU
6061         vgpu_allocations = {}
6062         for rp in allocations:
6063             res = allocations[rp]['resources']
6064             if RC_VGPU in res and res[RC_VGPU] > 0:
6065                 vgpu_allocations[rp] = {'resources': {RC_VGPU: res[RC_VGPU]}}
6066         return vgpu_allocations
6067 
6068     def _get_existing_mdevs_not_assigned(self, requested_types=None):
6069         """Returns the already created mediated devices that are not assigned
6070         to a guest yet.
6071 
6072         :param requested_types: Filter out the result for only mediated devices
6073                                 having those types.
6074         """
6075         allocated_mdevs = self._get_all_assigned_mediated_devices()
6076         mdevs = self._get_mediated_devices(requested_types)
6077         available_mdevs = set([mdev["uuid"]
6078                                for mdev in mdevs]) - set(allocated_mdevs)
6079         return available_mdevs
6080 
6081     def _create_new_mediated_device(self, requested_types, uuid=None):
6082         """Find a physical device that can support a new mediated device and
6083         create it.
6084 
6085         :param requested_types: Filter only capable devices supporting those
6086                                 types.
6087         :param uuid: The possible mdev UUID we want to create again
6088 
6089         :returns: the newly created mdev UUID or None if not possible
6090         """
6091         # Try to see if we can still create a new mediated device
6092         devices = self._get_mdev_capable_devices(requested_types)
6093         for device in devices:
6094             # For the moment, the libvirt driver only supports one
6095             # type per host
6096             # TODO(sbauza): Once we support more than one type, make
6097             # sure we look at the flavor/trait for the asked type.
6098             asked_type = requested_types[0]
6099             if device['types'][asked_type]['availableInstances'] > 0:
6100                 # That physical GPU has enough room for a new mdev
6101                 dev_name = device['dev_id']
6102                 # We need the PCI address, not the libvirt name
6103                 # The libvirt name is like 'pci_0000_84_00_0'
6104                 pci_addr = "{}:{}:{}.{}".format(*dev_name[4:].split('_'))
6105                 chosen_mdev = nova.privsep.libvirt.create_mdev(pci_addr,
6106                                                                asked_type,
6107                                                                uuid=uuid)
6108                 return chosen_mdev
6109 
6110     @utils.synchronized(VGPU_RESOURCE_SEMAPHORE)
6111     def _allocate_mdevs(self, allocations):
6112         """Returns a list of mediated device UUIDs corresponding to available
6113         resources we can assign to the guest(s) corresponding to the allocation
6114         requests passed as argument.
6115 
6116         That method can either find an existing but unassigned mediated device
6117         it can allocate, or create a new mediated device from a capable
6118         physical device if the latter has enough left capacity.
6119 
6120         :param allocations: Information about resources allocated to the
6121                             instance via placement, of the form returned by
6122                             SchedulerReportClient.get_allocations_for_consumer.
6123                             That code is supporting Placement API version 1.12
6124         """
6125         vgpu_allocations = self._vgpu_allocations(allocations)
6126         if not vgpu_allocations:
6127             return
6128         # TODO(sbauza): Once we have nested resource providers, find which one
6129         # is having the related allocation for the specific VGPU type.
6130         # For the moment, we should only have one allocation for
6131         # ResourceProvider.
6132         # TODO(sbauza): Iterate over all the allocations once we have
6133         # nested Resource Providers. For the moment, just take the first.
6134         if len(vgpu_allocations) > 1:
6135             LOG.warning('More than one allocation was passed over to libvirt '
6136                         'while at the moment libvirt only supports one. Only '
6137                         'the first allocation will be looked up.')
6138         alloc = six.next(six.itervalues(vgpu_allocations))
6139         vgpus_asked = alloc['resources'][rc_fields.ResourceClass.VGPU]
6140 
6141         requested_types = self._get_supported_vgpu_types()
6142         # Which mediated devices are created but not assigned to a guest ?
6143         mdevs_available = self._get_existing_mdevs_not_assigned(
6144             requested_types)
6145 
6146         chosen_mdevs = []
6147         for c in six.moves.range(vgpus_asked):
6148             chosen_mdev = None
6149             if mdevs_available:
6150                 # Take the first available mdev
6151                 chosen_mdev = mdevs_available.pop()
6152             else:
6153                 chosen_mdev = self._create_new_mediated_device(requested_types)
6154             if not chosen_mdev:
6155                 # If we can't find devices having available VGPUs, just raise
6156                 raise exception.ComputeResourcesUnavailable(
6157                     reason='vGPU resource is not available')
6158             else:
6159                 chosen_mdevs.append(chosen_mdev)
6160         return chosen_mdevs
6161 
6162     def _detach_mediated_devices(self, guest):
6163         mdevs = guest.get_all_devices(
6164             devtype=vconfig.LibvirtConfigGuestHostdevMDEV)
6165         for mdev_cfg in mdevs:
6166             try:
6167                 guest.detach_device(mdev_cfg, live=True)
6168             except libvirt.libvirtError as ex:
6169                 error_code = ex.get_error_code()
6170                 # NOTE(sbauza): There is a pending issue with libvirt that
6171                 # doesn't allow to hot-unplug mediated devices. Let's
6172                 # short-circuit the suspend action and set the instance back
6173                 # to ACTIVE.
6174                 # TODO(sbauza): Once libvirt supports this, amend the resume()
6175                 # operation to support reallocating mediated devices.
6176                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
6177                     reason = _("Suspend is not supported for instances having "
6178                                "attached vGPUs.")
6179                     raise exception.InstanceFaultRollback(
6180                         exception.InstanceSuspendFailure(reason=reason))
6181                 else:
6182                     raise
6183 
6184     def _has_numa_support(self):
6185         # This means that the host can support LibvirtConfigGuestNUMATune
6186         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
6187         for ver in BAD_LIBVIRT_NUMA_VERSIONS:
6188             if self._host.has_version(ver):
6189                 if not getattr(self, '_bad_libvirt_numa_version_warn', False):
6190                     LOG.warning('You are running with libvirt version %s '
6191                                 'which is known to have broken NUMA support. '
6192                                 'Consider patching or updating libvirt on '
6193                                 'this host if you need NUMA support.',
6194                                 self._version_to_string(ver))
6195                     self._bad_libvirt_numa_version_warn = True
6196                 return False
6197 
6198         caps = self._host.get_capabilities()
6199 
6200         if (caps.host.cpu.arch in (fields.Architecture.I686,
6201                                    fields.Architecture.X86_64,
6202                                    fields.Architecture.AARCH64) and
6203                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
6204             return True
6205         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
6206                                      fields.Architecture.PPC64LE) and
6207                 self._host.has_min_version(MIN_LIBVIRT_NUMA_VERSION_PPC,
6208                                            hv_type=host.HV_DRIVER_QEMU)):
6209             return True
6210 
6211         return False
6212 
6213     def _get_host_numa_topology(self):
6214         if not self._has_numa_support():
6215             return
6216 
6217         caps = self._host.get_capabilities()
6218         topology = caps.host.topology
6219 
6220         if topology is None or not topology.cells:
6221             return
6222 
6223         cells = []
6224         allowed_cpus = hardware.get_vcpu_pin_set()
6225         online_cpus = self._host.get_online_cpus()
6226         if allowed_cpus:
6227             allowed_cpus &= online_cpus
6228         else:
6229             allowed_cpus = online_cpus
6230 
6231         def _get_reserved_memory_for_cell(self, cell_id, page_size):
6232             cell = self._reserved_hugepages.get(cell_id, {})
6233             return cell.get(page_size, 0)
6234 
6235         for cell in topology.cells:
6236             cpuset = set(cpu.id for cpu in cell.cpus)
6237             siblings = sorted(map(set,
6238                                   set(tuple(cpu.siblings)
6239                                         if cpu.siblings else ()
6240                                       for cpu in cell.cpus)
6241                                   ))
6242             cpuset &= allowed_cpus
6243             siblings = [sib & allowed_cpus for sib in siblings]
6244             # Filter out empty sibling sets that may be left
6245             siblings = [sib for sib in siblings if len(sib) > 0]
6246 
6247             mempages = [
6248                 objects.NUMAPagesTopology(
6249                     size_kb=pages.size,
6250                     total=pages.total,
6251                     used=0,
6252                     reserved=_get_reserved_memory_for_cell(
6253                         self, cell.id, pages.size))
6254                 for pages in cell.mempages]
6255 
6256             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
6257                                     memory=cell.memory / units.Ki,
6258                                     cpu_usage=0, memory_usage=0,
6259                                     siblings=siblings,
6260                                     pinned_cpus=set([]),
6261                                     mempages=mempages)
6262             cells.append(cell)
6263 
6264         return objects.NUMATopology(cells=cells)
6265 
6266     def get_all_volume_usage(self, context, compute_host_bdms):
6267         """Return usage info for volumes attached to vms on
6268            a given host.
6269         """
6270         vol_usage = []
6271 
6272         for instance_bdms in compute_host_bdms:
6273             instance = instance_bdms['instance']
6274 
6275             for bdm in instance_bdms['instance_bdms']:
6276                 mountpoint = bdm['device_name']
6277                 if mountpoint.startswith('/dev/'):
6278                     mountpoint = mountpoint[5:]
6279                 volume_id = bdm['volume_id']
6280 
6281                 LOG.debug("Trying to get stats for the volume %s",
6282                           volume_id, instance=instance)
6283                 vol_stats = self.block_stats(instance, mountpoint)
6284 
6285                 if vol_stats:
6286                     stats = dict(volume=volume_id,
6287                                  instance=instance,
6288                                  rd_req=vol_stats[0],
6289                                  rd_bytes=vol_stats[1],
6290                                  wr_req=vol_stats[2],
6291                                  wr_bytes=vol_stats[3])
6292                     LOG.debug(
6293                         "Got volume usage stats for the volume=%(volume)s,"
6294                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
6295                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
6296                         stats, instance=instance)
6297                     vol_usage.append(stats)
6298 
6299         return vol_usage
6300 
6301     def block_stats(self, instance, disk_id):
6302         """Note that this function takes an instance name."""
6303         try:
6304             guest = self._host.get_guest(instance)
6305 
6306             # TODO(sahid): We are converting all calls from a
6307             # virDomain object to use nova.virt.libvirt.Guest.
6308             # We should be able to remove domain at the end.
6309             domain = guest._domain
6310             return domain.blockStats(disk_id)
6311         except libvirt.libvirtError as e:
6312             errcode = e.get_error_code()
6313             LOG.info('Getting block stats failed, device might have '
6314                      'been detached. Instance=%(instance_name)s '
6315                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
6316                      {'instance_name': instance.name, 'disk': disk_id,
6317                       'errcode': errcode, 'e': e},
6318                      instance=instance)
6319         except exception.InstanceNotFound:
6320             LOG.info('Could not find domain in libvirt for instance %s. '
6321                      'Cannot get block stats for device', instance.name,
6322                      instance=instance)
6323 
6324     def get_console_pool_info(self, console_type):
6325         # TODO(mdragon): console proxy should be implemented for libvirt,
6326         #                in case someone wants to use it with kvm or
6327         #                such. For now return fake data.
6328         return {'address': '127.0.0.1',
6329                 'username': 'fakeuser',
6330                 'password': 'fakepassword'}
6331 
6332     def refresh_security_group_rules(self, security_group_id):
6333         self.firewall_driver.refresh_security_group_rules(security_group_id)
6334 
6335     def refresh_instance_security_rules(self, instance):
6336         self.firewall_driver.refresh_instance_security_rules(instance)
6337 
6338     def get_inventory(self, nodename):
6339         """Return a dict, keyed by resource class, of inventory information for
6340         the supplied node.
6341         """
6342         disk_gb = int(self._get_local_gb_info()['total'])
6343         memory_mb = int(self._host.get_memory_mb_total())
6344         vcpus = self._get_vcpu_total()
6345 
6346         # NOTE(sbauza): For the moment, the libvirt driver only supports
6347         # providing the total number of virtual GPUs for a single GPU type. If
6348         # you have multiple physical GPUs, each of them providing multiple GPU
6349         # types, libvirt will return the total sum of virtual GPUs
6350         # corresponding to the single type passed in enabled_vgpu_types
6351         # configuration option. Eg. if you have 2 pGPUs supporting 'nvidia-35',
6352         # each of them having 16 available instances, the total here will be
6353         # 32.
6354         # If one of the 2 pGPUs doesn't support 'nvidia-35', it won't be used.
6355         # TODO(sbauza): Use ProviderTree and traits to make a better world.
6356         vgpus = self._get_vgpu_total()
6357 
6358         # NOTE(jaypipes): We leave some fields like allocation_ratio and
6359         # reserved out of the returned dicts here because, for now at least,
6360         # the RT injects those values into the inventory dict based on the
6361         # compute_nodes record values.
6362         result = {
6363             rc_fields.ResourceClass.VCPU: {
6364                 'total': vcpus,
6365                 'min_unit': 1,
6366                 'max_unit': vcpus,
6367                 'step_size': 1,
6368             },
6369             rc_fields.ResourceClass.MEMORY_MB: {
6370                 'total': memory_mb,
6371                 'min_unit': 1,
6372                 'max_unit': memory_mb,
6373                 'step_size': 1,
6374             },
6375             rc_fields.ResourceClass.DISK_GB: {
6376                 'total': disk_gb,
6377                 'min_unit': 1,
6378                 'max_unit': disk_gb,
6379                 'step_size': 1,
6380             },
6381         }
6382 
6383         if vgpus > 0:
6384             # Only provide VGPU resource classes if the driver supports it.
6385             result[rc_fields.ResourceClass.VGPU] = {
6386                 'total': vgpus,
6387                 'min_unit': 1,
6388                 'max_unit': vgpus,
6389                 'step_size': 1,
6390                 }
6391 
6392         return result
6393 
6394     def get_available_resource(self, nodename):
6395         """Retrieve resource information.
6396 
6397         This method is called when nova-compute launches, and
6398         as part of a periodic task that records the results in the DB.
6399 
6400         :param nodename: unused in this driver
6401         :returns: dictionary containing resource info
6402         """
6403 
6404         disk_info_dict = self._get_local_gb_info()
6405         data = {}
6406 
6407         # NOTE(dprince): calling capabilities before getVersion works around
6408         # an initialization issue with some versions of Libvirt (1.0.5.5).
6409         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
6410         # See: https://bugs.launchpad.net/nova/+bug/1215593
6411         data["supported_instances"] = self._get_instance_capabilities()
6412 
6413         data["vcpus"] = self._get_vcpu_total()
6414         data["memory_mb"] = self._host.get_memory_mb_total()
6415         data["local_gb"] = disk_info_dict['total']
6416         data["vcpus_used"] = self._get_vcpu_used()
6417         data["memory_mb_used"] = self._host.get_memory_mb_used()
6418         data["local_gb_used"] = disk_info_dict['used']
6419         data["hypervisor_type"] = self._host.get_driver_type()
6420         data["hypervisor_version"] = self._host.get_version()
6421         data["hypervisor_hostname"] = self._host.get_hostname()
6422         # TODO(berrange): why do we bother converting the
6423         # libvirt capabilities XML into a special JSON format ?
6424         # The data format is different across all the drivers
6425         # so we could just return the raw capabilities XML
6426         # which 'compare_cpu' could use directly
6427         #
6428         # That said, arch_filter.py now seems to rely on
6429         # the libvirt drivers format which suggests this
6430         # data format needs to be standardized across drivers
6431         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
6432 
6433         disk_free_gb = disk_info_dict['free']
6434         disk_over_committed = self._get_disk_over_committed_size_total()
6435         available_least = disk_free_gb * units.Gi - disk_over_committed
6436         data['disk_available_least'] = available_least / units.Gi
6437 
6438         data['pci_passthrough_devices'] = \
6439             self._get_pci_passthrough_devices()
6440 
6441         numa_topology = self._get_host_numa_topology()
6442         if numa_topology:
6443             data['numa_topology'] = numa_topology._to_json()
6444         else:
6445             data['numa_topology'] = None
6446 
6447         return data
6448 
6449     def check_instance_shared_storage_local(self, context, instance):
6450         """Check if instance files located on shared storage.
6451 
6452         This runs check on the destination host, and then calls
6453         back to the source host to check the results.
6454 
6455         :param context: security context
6456         :param instance: nova.objects.instance.Instance object
6457         :returns:
6458          - tempfile: A dict containing the tempfile info on the destination
6459                      host
6460          - None:
6461 
6462             1. If the instance path is not existing.
6463             2. If the image backend is shared block storage type.
6464         """
6465         if self.image_backend.backend().is_shared_block_storage():
6466             return None
6467 
6468         dirpath = libvirt_utils.get_instance_path(instance)
6469 
6470         if not os.path.exists(dirpath):
6471             return None
6472 
6473         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
6474         LOG.debug("Creating tmpfile %s to verify with other "
6475                   "compute node that the instance is on "
6476                   "the same shared storage.",
6477                   tmp_file, instance=instance)
6478         os.close(fd)
6479         return {"filename": tmp_file}
6480 
6481     def check_instance_shared_storage_remote(self, context, data):
6482         return os.path.exists(data['filename'])
6483 
6484     def check_instance_shared_storage_cleanup(self, context, data):
6485         fileutils.delete_if_exists(data["filename"])
6486 
6487     def check_can_live_migrate_destination(self, context, instance,
6488                                            src_compute_info, dst_compute_info,
6489                                            block_migration=False,
6490                                            disk_over_commit=False):
6491         """Check if it is possible to execute live migration.
6492 
6493         This runs checks on the destination host, and then calls
6494         back to the source host to check the results.
6495 
6496         :param context: security context
6497         :param instance: nova.db.sqlalchemy.models.Instance
6498         :param block_migration: if true, prepare for block migration
6499         :param disk_over_commit: if true, allow disk over commit
6500         :returns: a LibvirtLiveMigrateData object
6501         """
6502         if disk_over_commit:
6503             disk_available_gb = dst_compute_info['local_gb']
6504         else:
6505             disk_available_gb = dst_compute_info['disk_available_least']
6506         disk_available_mb = (
6507             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
6508 
6509         # Compare CPU
6510         if not instance.vcpu_model or not instance.vcpu_model.model:
6511             source_cpu_info = src_compute_info['cpu_info']
6512             self._compare_cpu(None, source_cpu_info, instance)
6513         else:
6514             self._compare_cpu(instance.vcpu_model, None, instance)
6515 
6516         # Create file on storage, to be checked on source host
6517         filename = self._create_shared_storage_test_file(instance)
6518 
6519         data = objects.LibvirtLiveMigrateData()
6520         data.filename = filename
6521         data.image_type = CONF.libvirt.images_type
6522         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
6523         data.graphics_listen_addr_spice = CONF.spice.server_listen
6524         if CONF.serial_console.enabled:
6525             data.serial_listen_addr = CONF.serial_console.proxyclient_address
6526         else:
6527             data.serial_listen_addr = None
6528         # Notes(eliqiao): block_migration and disk_over_commit are not
6529         # nullable, so just don't set them if they are None
6530         if block_migration is not None:
6531             data.block_migration = block_migration
6532         if disk_over_commit is not None:
6533             data.disk_over_commit = disk_over_commit
6534         data.disk_available_mb = disk_available_mb
6535         return data
6536 
6537     def cleanup_live_migration_destination_check(self, context,
6538                                                  dest_check_data):
6539         """Do required cleanup on dest host after check_can_live_migrate calls
6540 
6541         :param context: security context
6542         """
6543         filename = dest_check_data.filename
6544         self._cleanup_shared_storage_test_file(filename)
6545 
6546     def check_can_live_migrate_source(self, context, instance,
6547                                       dest_check_data,
6548                                       block_device_info=None):
6549         """Check if it is possible to execute live migration.
6550 
6551         This checks if the live migration can succeed, based on the
6552         results from check_can_live_migrate_destination.
6553 
6554         :param context: security context
6555         :param instance: nova.db.sqlalchemy.models.Instance
6556         :param dest_check_data: result of check_can_live_migrate_destination
6557         :param block_device_info: result of _get_instance_block_device_info
6558         :returns: a LibvirtLiveMigrateData object
6559         """
6560         if not isinstance(dest_check_data, migrate_data_obj.LiveMigrateData):
6561             md_obj = objects.LibvirtLiveMigrateData()
6562             md_obj.from_legacy_dict(dest_check_data)
6563             dest_check_data = md_obj
6564 
6565         # Checking shared storage connectivity
6566         # if block migration, instances_path should not be on shared storage.
6567         source = CONF.host
6568 
6569         dest_check_data.is_shared_instance_path = (
6570             self._check_shared_storage_test_file(
6571                 dest_check_data.filename, instance))
6572 
6573         dest_check_data.is_shared_block_storage = (
6574             self._is_shared_block_storage(instance, dest_check_data,
6575                                           block_device_info))
6576 
6577         if 'block_migration' not in dest_check_data:
6578             dest_check_data.block_migration = (
6579                 not dest_check_data.is_on_shared_storage())
6580 
6581         if dest_check_data.block_migration:
6582             # TODO(eliqiao): Once block_migration flag is removed from the API
6583             # we can safely remove the if condition
6584             if dest_check_data.is_on_shared_storage():
6585                 reason = _("Block migration can not be used "
6586                            "with shared storage.")
6587                 raise exception.InvalidLocalStorage(reason=reason, path=source)
6588             if 'disk_over_commit' in dest_check_data:
6589                 self._assert_dest_node_has_enough_disk(context, instance,
6590                                         dest_check_data.disk_available_mb,
6591                                         dest_check_data.disk_over_commit,
6592                                         block_device_info)
6593             if block_device_info:
6594                 bdm = block_device_info.get('block_device_mapping')
6595                 # NOTE(pkoniszewski): libvirt from version 1.2.17 upwards
6596                 # supports selective block device migration. It means that it
6597                 # is possible to define subset of block devices to be copied
6598                 # during migration. If they are not specified - block devices
6599                 # won't be migrated. However, it does not work when live
6600                 # migration is tunnelled through libvirt.
6601                 if bdm and not self._host.has_min_version(
6602                         MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION):
6603                     # NOTE(stpierre): if this instance has mapped volumes,
6604                     # we can't do a block migration, since that will result
6605                     # in volumes being copied from themselves to themselves,
6606                     # which is a recipe for disaster.
6607                     ver = ".".join([str(x) for x in
6608                                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION])
6609                     msg = (_('Cannot block migrate instance %(uuid)s with'
6610                              ' mapped volumes. Selective block device'
6611                              ' migration feature requires libvirt version'
6612                              ' %(libvirt_ver)s') %
6613                            {'uuid': instance.uuid, 'libvirt_ver': ver})
6614                     LOG.error(msg, instance=instance)
6615                     raise exception.MigrationPreCheckError(reason=msg)
6616                 # NOTE(eliqiao): Selective disk migrations are not supported
6617                 # with tunnelled block migrations so we can block them early.
6618                 if (bdm and
6619                     (self._block_migration_flags &
6620                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
6621                     msg = (_('Cannot block migrate instance %(uuid)s with'
6622                              ' mapped volumes. Selective block device'
6623                              ' migration is not supported with tunnelled'
6624                              ' block migrations.') % {'uuid': instance.uuid})
6625                     LOG.error(msg, instance=instance)
6626                     raise exception.MigrationPreCheckError(reason=msg)
6627         elif not (dest_check_data.is_shared_block_storage or
6628                   dest_check_data.is_shared_instance_path):
6629             reason = _("Shared storage live-migration requires either shared "
6630                        "storage or boot-from-volume with no local disks.")
6631             raise exception.InvalidSharedStorage(reason=reason, path=source)
6632 
6633         # NOTE(mikal): include the instance directory name here because it
6634         # doesn't yet exist on the destination but we want to force that
6635         # same name to be used
6636         instance_path = libvirt_utils.get_instance_path(instance,
6637                                                         relative=True)
6638         dest_check_data.instance_relative_path = instance_path
6639 
6640         # NOTE(lyarwood): Used to indicate to the dest that the src is capable
6641         # of wiring up the encrypted disk configuration for the domain.
6642         # Note that this does not require the QEMU and Libvirt versions to
6643         # decrypt LUKS to be installed on the source node. Only the Nova
6644         # utility code to generate the correct XML is required, so we can
6645         # default to True here for all computes >= Queens.
6646         dest_check_data.src_supports_native_luks = True
6647 
6648         return dest_check_data
6649 
6650     def _is_shared_block_storage(self, instance, dest_check_data,
6651                                  block_device_info=None):
6652         """Check if all block storage of an instance can be shared
6653         between source and destination of a live migration.
6654 
6655         Returns true if the instance is volume backed and has no local disks,
6656         or if the image backend is the same on source and destination and the
6657         backend shares block storage between compute nodes.
6658 
6659         :param instance: nova.objects.instance.Instance object
6660         :param dest_check_data: dict with boolean fields image_type,
6661                                 is_shared_instance_path, and is_volume_backed
6662         """
6663         if (dest_check_data.obj_attr_is_set('image_type') and
6664                 CONF.libvirt.images_type == dest_check_data.image_type and
6665                 self.image_backend.backend().is_shared_block_storage()):
6666             # NOTE(dgenin): currently true only for RBD image backend
6667             return True
6668 
6669         if (dest_check_data.is_shared_instance_path and
6670                 self.image_backend.backend().is_file_in_instance_path()):
6671             # NOTE(angdraug): file based image backends (Flat, Qcow2)
6672             # place block device files under the instance path
6673             return True
6674 
6675         if (dest_check_data.is_volume_backed and
6676                 not bool(self._get_instance_disk_info(instance,
6677                                                       block_device_info))):
6678             return True
6679 
6680         return False
6681 
6682     def _assert_dest_node_has_enough_disk(self, context, instance,
6683                                              available_mb, disk_over_commit,
6684                                              block_device_info):
6685         """Checks if destination has enough disk for block migration."""
6686         # Libvirt supports qcow2 disk format,which is usually compressed
6687         # on compute nodes.
6688         # Real disk image (compressed) may enlarged to "virtual disk size",
6689         # that is specified as the maximum disk size.
6690         # (See qemu-img -f path-to-disk)
6691         # Scheduler recognizes destination host still has enough disk space
6692         # if real disk size < available disk size
6693         # if disk_over_commit is True,
6694         #  otherwise virtual disk size < available disk size.
6695 
6696         available = 0
6697         if available_mb:
6698             available = available_mb * units.Mi
6699 
6700         disk_infos = self._get_instance_disk_info(instance, block_device_info)
6701 
6702         necessary = 0
6703         if disk_over_commit:
6704             for info in disk_infos:
6705                 necessary += int(info['disk_size'])
6706         else:
6707             for info in disk_infos:
6708                 necessary += int(info['virt_disk_size'])
6709 
6710         # Check that available disk > necessary disk
6711         if (available - necessary) < 0:
6712             reason = (_('Unable to migrate %(instance_uuid)s: '
6713                         'Disk of instance is too large(available'
6714                         ' on destination host:%(available)s '
6715                         '< need:%(necessary)s)') %
6716                       {'instance_uuid': instance.uuid,
6717                        'available': available,
6718                        'necessary': necessary})
6719             raise exception.MigrationPreCheckError(reason=reason)
6720 
6721     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
6722         """Check the host is compatible with the requested CPU
6723 
6724         :param guest_cpu: nova.objects.VirtCPUModel or None
6725         :param host_cpu_str: JSON from _get_cpu_info() method
6726 
6727         If the 'guest_cpu' parameter is not None, this will be
6728         validated for migration compatibility with the host.
6729         Otherwise the 'host_cpu_str' JSON string will be used for
6730         validation.
6731 
6732         :returns:
6733             None. if given cpu info is not compatible to this server,
6734             raise exception.
6735         """
6736 
6737         # NOTE(kchamart): Comparing host to guest CPU model for emulated
6738         # guests (<domain type='qemu'>) should not matter -- in this
6739         # mode (QEMU "TCG") the CPU is fully emulated in software and no
6740         # hardware acceleration, like KVM, is involved. So, skip the CPU
6741         # compatibility check for the QEMU domain type, and retain it for
6742         # KVM guests.
6743         if CONF.libvirt.virt_type not in ['kvm']:
6744             return
6745 
6746         if guest_cpu is None:
6747             info = jsonutils.loads(host_cpu_str)
6748             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
6749             cpu = vconfig.LibvirtConfigCPU()
6750             cpu.arch = info['arch']
6751             cpu.model = info['model']
6752             cpu.vendor = info['vendor']
6753             cpu.sockets = info['topology']['sockets']
6754             cpu.cores = info['topology']['cores']
6755             cpu.threads = info['topology']['threads']
6756             for f in info['features']:
6757                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
6758         else:
6759             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
6760 
6761         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
6762              "virCPUCompareResult")
6763         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
6764         # unknown character exists in xml, then libvirt complains
6765         try:
6766             cpu_xml = cpu.to_xml()
6767             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
6768             ret = self._host.compare_cpu(cpu_xml)
6769         except libvirt.libvirtError as e:
6770             error_code = e.get_error_code()
6771             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
6772                 LOG.debug("URI %(uri)s does not support cpu comparison. "
6773                           "It will be proceeded though. Error: %(error)s",
6774                           {'uri': self._uri(), 'error': e})
6775                 return
6776             else:
6777                 LOG.error(m, {'ret': e, 'u': u})
6778                 raise exception.MigrationPreCheckError(
6779                     reason=m % {'ret': e, 'u': u})
6780 
6781         if ret <= 0:
6782             LOG.error(m, {'ret': ret, 'u': u})
6783             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
6784 
6785     def _create_shared_storage_test_file(self, instance):
6786         """Makes tmpfile under CONF.instances_path."""
6787         dirpath = CONF.instances_path
6788         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
6789         LOG.debug("Creating tmpfile %s to notify to other "
6790                   "compute nodes that they should mount "
6791                   "the same storage.", tmp_file, instance=instance)
6792         os.close(fd)
6793         return os.path.basename(tmp_file)
6794 
6795     def _check_shared_storage_test_file(self, filename, instance):
6796         """Confirms existence of the tmpfile under CONF.instances_path.
6797 
6798         Cannot confirm tmpfile return False.
6799         """
6800         # NOTE(tpatzig): if instances_path is a shared volume that is
6801         # under heavy IO (many instances on many compute nodes),
6802         # then checking the existence of the testfile fails,
6803         # just because it takes longer until the client refreshes and new
6804         # content gets visible.
6805         # os.utime (like touch) on the directory forces the client to refresh.
6806         os.utime(CONF.instances_path, None)
6807 
6808         tmp_file = os.path.join(CONF.instances_path, filename)
6809         if not os.path.exists(tmp_file):
6810             exists = False
6811         else:
6812             exists = True
6813         LOG.debug('Check if temp file %s exists to indicate shared storage '
6814                   'is being used for migration. Exists? %s', tmp_file, exists,
6815                   instance=instance)
6816         return exists
6817 
6818     def _cleanup_shared_storage_test_file(self, filename):
6819         """Removes existence of the tmpfile under CONF.instances_path."""
6820         tmp_file = os.path.join(CONF.instances_path, filename)
6821         os.remove(tmp_file)
6822 
6823     def ensure_filtering_rules_for_instance(self, instance, network_info):
6824         """Ensure that an instance's filtering rules are enabled.
6825 
6826         When migrating an instance, we need the filtering rules to
6827         be configured on the destination host before starting the
6828         migration.
6829 
6830         Also, when restarting the compute service, we need to ensure
6831         that filtering rules exist for all running services.
6832         """
6833 
6834         self.firewall_driver.setup_basic_filtering(instance, network_info)
6835         self.firewall_driver.prepare_instance_filter(instance,
6836                 network_info)
6837 
6838         # nwfilters may be defined in a separate thread in the case
6839         # of libvirt non-blocking mode, so we wait for completion
6840         timeout_count = list(range(CONF.live_migration_retry_count))
6841         while timeout_count:
6842             if self.firewall_driver.instance_filter_exists(instance,
6843                                                            network_info):
6844                 break
6845             timeout_count.pop()
6846             if len(timeout_count) == 0:
6847                 msg = _('The firewall filter for %s does not exist')
6848                 raise exception.InternalError(msg % instance.name)
6849             greenthread.sleep(1)
6850 
6851     def filter_defer_apply_on(self):
6852         self.firewall_driver.filter_defer_apply_on()
6853 
6854     def filter_defer_apply_off(self):
6855         self.firewall_driver.filter_defer_apply_off()
6856 
6857     def live_migration(self, context, instance, dest,
6858                        post_method, recover_method, block_migration=False,
6859                        migrate_data=None):
6860         """Spawning live_migration operation for distributing high-load.
6861 
6862         :param context: security context
6863         :param instance:
6864             nova.db.sqlalchemy.models.Instance object
6865             instance object that is migrated.
6866         :param dest: destination host
6867         :param post_method:
6868             post operation method.
6869             expected nova.compute.manager._post_live_migration.
6870         :param recover_method:
6871             recovery method when any exception occurs.
6872             expected nova.compute.manager._rollback_live_migration.
6873         :param block_migration: if true, do block migration.
6874         :param migrate_data: a LibvirtLiveMigrateData object
6875 
6876         """
6877 
6878         # 'dest' will be substituted into 'migration_uri' so ensure
6879         # it does't contain any characters that could be used to
6880         # exploit the URI accepted by libivrt
6881         if not libvirt_utils.is_valid_hostname(dest):
6882             raise exception.InvalidHostname(hostname=dest)
6883 
6884         self._live_migration(context, instance, dest,
6885                              post_method, recover_method, block_migration,
6886                              migrate_data)
6887 
6888     def live_migration_abort(self, instance):
6889         """Aborting a running live-migration.
6890 
6891         :param instance: instance object that is in migration
6892 
6893         """
6894 
6895         guest = self._host.get_guest(instance)
6896         dom = guest._domain
6897 
6898         try:
6899             dom.abortJob()
6900         except libvirt.libvirtError as e:
6901             LOG.error("Failed to cancel migration %s",
6902                     encodeutils.exception_to_unicode(e), instance=instance)
6903             raise
6904 
6905     def _verify_serial_console_is_disabled(self):
6906         if CONF.serial_console.enabled:
6907 
6908             msg = _('Your destination node does not support'
6909                     ' retrieving listen addresses.  In order'
6910                     ' for live migration to work properly you'
6911                     ' must disable serial console.')
6912             raise exception.MigrationError(reason=msg)
6913 
6914     def _live_migration_operation(self, context, instance, dest,
6915                                   block_migration, migrate_data, guest,
6916                                   device_names):
6917         """Invoke the live migration operation
6918 
6919         :param context: security context
6920         :param instance:
6921             nova.db.sqlalchemy.models.Instance object
6922             instance object that is migrated.
6923         :param dest: destination host
6924         :param block_migration: if true, do block migration.
6925         :param migrate_data: a LibvirtLiveMigrateData object
6926         :param guest: the guest domain object
6927         :param device_names: list of device names that are being migrated with
6928             instance
6929 
6930         This method is intended to be run in a background thread and will
6931         block that thread until the migration is finished or failed.
6932         """
6933         try:
6934             if migrate_data.block_migration:
6935                 migration_flags = self._block_migration_flags
6936             else:
6937                 migration_flags = self._live_migration_flags
6938 
6939             serial_listen_addr = libvirt_migrate.serial_listen_addr(
6940                 migrate_data)
6941             if not serial_listen_addr:
6942                 # In this context we want to ensure that serial console is
6943                 # disabled on source node. This is because nova couldn't
6944                 # retrieve serial listen address from destination node, so we
6945                 # consider that destination node might have serial console
6946                 # disabled as well.
6947                 self._verify_serial_console_is_disabled()
6948 
6949             # NOTE(aplanas) migrate_uri will have a value only in the
6950             # case that `live_migration_inbound_addr` parameter is
6951             # set, and we propose a non tunneled migration.
6952             migrate_uri = None
6953             if ('target_connect_addr' in migrate_data and
6954                     migrate_data.target_connect_addr is not None):
6955                 dest = migrate_data.target_connect_addr
6956                 if (migration_flags &
6957                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
6958                     migrate_uri = self._migrate_uri(dest)
6959 
6960             params = None
6961             new_xml_str = None
6962             if CONF.libvirt.virt_type != "parallels":
6963                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
6964                     # TODO(sahid): It's not a really good idea to pass
6965                     # the method _get_volume_config and we should to find
6966                     # a way to avoid this in future.
6967                     guest, migrate_data, self._get_volume_config)
6968             if self._host.has_min_version(
6969                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION):
6970                 params = {
6971                     'destination_xml': new_xml_str,
6972                     'migrate_disks': device_names,
6973                 }
6974                 # NOTE(pkoniszewski): Because of precheck which blocks
6975                 # tunnelled block live migration with mapped volumes we
6976                 # can safely remove migrate_disks when tunnelling is on.
6977                 # Otherwise we will block all tunnelled block migrations,
6978                 # even when an instance does not have volumes mapped.
6979                 # This is because selective disk migration is not
6980                 # supported in tunnelled block live migration. Also we
6981                 # cannot fallback to migrateToURI2 in this case because of
6982                 # bug #1398999
6983                 if (migration_flags &
6984                     libvirt.VIR_MIGRATE_TUNNELLED != 0):
6985                     params.pop('migrate_disks')
6986 
6987             # TODO(sahid): This should be in
6988             # post_live_migration_at_source but no way to retrieve
6989             # ports acquired on the host for the guest at this
6990             # step. Since the domain is going to be removed from
6991             # libvird on source host after migration, we backup the
6992             # serial ports to release them if all went well.
6993             serial_ports = []
6994             if CONF.serial_console.enabled:
6995                 serial_ports = list(self._get_serial_ports_from_guest(guest))
6996 
6997             guest.migrate(self._live_migration_uri(dest),
6998                           migrate_uri=migrate_uri,
6999                           flags=migration_flags,
7000                           params=params,
7001                           domain_xml=new_xml_str,
7002                           bandwidth=CONF.libvirt.live_migration_bandwidth)
7003 
7004             for hostname, port in serial_ports:
7005                 serial_console.release_port(host=hostname, port=port)
7006         except Exception as e:
7007             with excutils.save_and_reraise_exception():
7008                 LOG.error("Live Migration failure: %s", e, instance=instance)
7009 
7010         # If 'migrateToURI' fails we don't know what state the
7011         # VM instances on each host are in. Possibilities include
7012         #
7013         #  1. src==running, dst==none
7014         #
7015         #     Migration failed & rolled back, or never started
7016         #
7017         #  2. src==running, dst==paused
7018         #
7019         #     Migration started but is still ongoing
7020         #
7021         #  3. src==paused,  dst==paused
7022         #
7023         #     Migration data transfer completed, but switchover
7024         #     is still ongoing, or failed
7025         #
7026         #  4. src==paused,  dst==running
7027         #
7028         #     Migration data transfer completed, switchover
7029         #     happened but cleanup on source failed
7030         #
7031         #  5. src==none,    dst==running
7032         #
7033         #     Migration fully succeeded.
7034         #
7035         # Libvirt will aim to complete any migration operation
7036         # or roll it back. So even if the migrateToURI call has
7037         # returned an error, if the migration was not finished
7038         # libvirt should clean up.
7039         #
7040         # So we take the error raise here with a pinch of salt
7041         # and rely on the domain job info status to figure out
7042         # what really happened to the VM, which is a much more
7043         # reliable indicator.
7044         #
7045         # In particular we need to try very hard to ensure that
7046         # Nova does not "forget" about the guest. ie leaving it
7047         # running on a different host to the one recorded in
7048         # the database, as that would be a serious resource leak
7049 
7050         LOG.debug("Migration operation thread has finished",
7051                   instance=instance)
7052 
7053     def _live_migration_copy_disk_paths(self, context, instance, guest):
7054         '''Get list of disks to copy during migration
7055 
7056         :param context: security context
7057         :param instance: the instance being migrated
7058         :param guest: the Guest instance being migrated
7059 
7060         Get the list of disks to copy during migration.
7061 
7062         :returns: a list of local source paths and a list of device names to
7063             copy
7064         '''
7065 
7066         disk_paths = []
7067         device_names = []
7068         block_devices = []
7069 
7070         # TODO(pkoniszewski): Remove version check when we bump min libvirt
7071         # version to >= 1.2.17.
7072         if (self._block_migration_flags &
7073                 libvirt.VIR_MIGRATE_TUNNELLED == 0 and
7074                 self._host.has_min_version(
7075                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION)):
7076             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
7077                 context, instance.uuid)
7078             block_device_info = driver.get_block_device_info(instance,
7079                                                              bdm_list)
7080 
7081             block_device_mappings = driver.block_device_info_get_mapping(
7082                 block_device_info)
7083             for bdm in block_device_mappings:
7084                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
7085                 block_devices.append(device_name)
7086 
7087         for dev in guest.get_all_disks():
7088             if dev.readonly or dev.shareable:
7089                 continue
7090             if dev.source_type not in ["file", "block"]:
7091                 continue
7092             if dev.target_dev in block_devices:
7093                 continue
7094             disk_paths.append(dev.source_path)
7095             device_names.append(dev.target_dev)
7096         return (disk_paths, device_names)
7097 
7098     def _live_migration_data_gb(self, instance, disk_paths):
7099         '''Calculate total amount of data to be transferred
7100 
7101         :param instance: the nova.objects.Instance being migrated
7102         :param disk_paths: list of disk paths that are being migrated
7103         with instance
7104 
7105         Calculates the total amount of data that needs to be
7106         transferred during the live migration. The actual
7107         amount copied will be larger than this, due to the
7108         guest OS continuing to dirty RAM while the migration
7109         is taking place. So this value represents the minimal
7110         data size possible.
7111 
7112         :returns: data size to be copied in GB
7113         '''
7114 
7115         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
7116         if ram_gb < 2:
7117             ram_gb = 2
7118 
7119         disk_gb = 0
7120         for path in disk_paths:
7121             try:
7122                 size = os.stat(path).st_size
7123                 size_gb = (size / units.Gi)
7124                 if size_gb < 2:
7125                     size_gb = 2
7126                 disk_gb += size_gb
7127             except OSError as e:
7128                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
7129                             {'disk': path, 'ex': e})
7130                 # Ignore error since we don't want to break
7131                 # the migration monitoring thread operation
7132 
7133         return ram_gb + disk_gb
7134 
7135     def _get_migration_flags(self, is_block_migration):
7136         if is_block_migration:
7137             return self._block_migration_flags
7138         return self._live_migration_flags
7139 
7140     def _live_migration_monitor(self, context, instance, guest,
7141                                 dest, post_method,
7142                                 recover_method, block_migration,
7143                                 migrate_data, finish_event,
7144                                 disk_paths):
7145         on_migration_failure = deque()
7146         data_gb = self._live_migration_data_gb(instance, disk_paths)
7147         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
7148         migration = migrate_data.migration
7149         curdowntime = None
7150 
7151         migration_flags = self._get_migration_flags(
7152                                   migrate_data.block_migration)
7153 
7154         n = 0
7155         start = time.time()
7156         progress_time = start
7157         progress_watermark = None
7158         previous_data_remaining = -1
7159         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
7160         while True:
7161             info = guest.get_job_info()
7162 
7163             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
7164                 # Either still running, or failed or completed,
7165                 # lets untangle the mess
7166                 if not finish_event.ready():
7167                     LOG.debug("Operation thread is still running",
7168                               instance=instance)
7169                 else:
7170                     info.type = libvirt_migrate.find_job_type(guest, instance)
7171                     LOG.debug("Fixed incorrect job type to be %d",
7172                               info.type, instance=instance)
7173 
7174             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
7175                 # Migration is not yet started
7176                 LOG.debug("Migration not running yet",
7177                           instance=instance)
7178             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
7179                 # Migration is still running
7180                 #
7181                 # This is where we wire up calls to change live
7182                 # migration status. eg change max downtime, cancel
7183                 # the operation, change max bandwidth
7184                 libvirt_migrate.run_tasks(guest, instance,
7185                                           self.active_migrations,
7186                                           on_migration_failure,
7187                                           migration,
7188                                           is_post_copy_enabled)
7189 
7190                 now = time.time()
7191                 elapsed = now - start
7192 
7193                 if ((progress_watermark is None) or
7194                     (progress_watermark == 0) or
7195                     (progress_watermark > info.data_remaining)):
7196                     progress_watermark = info.data_remaining
7197                     progress_time = now
7198 
7199                 progress_timeout = CONF.libvirt.live_migration_progress_timeout
7200                 completion_timeout = int(
7201                     CONF.libvirt.live_migration_completion_timeout * data_gb)
7202                 if libvirt_migrate.should_abort(instance, now, progress_time,
7203                                                 progress_timeout, elapsed,
7204                                                 completion_timeout,
7205                                                 migration.status):
7206                     try:
7207                         guest.abort_job()
7208                     except libvirt.libvirtError as e:
7209                         LOG.warning("Failed to abort migration %s",
7210                                 encodeutils.exception_to_unicode(e),
7211                                 instance=instance)
7212                         self._clear_empty_migration(instance)
7213                         raise
7214 
7215                 if (is_post_copy_enabled and
7216                     libvirt_migrate.should_switch_to_postcopy(
7217                     info.memory_iteration, info.data_remaining,
7218                     previous_data_remaining, migration.status)):
7219                     libvirt_migrate.trigger_postcopy_switch(guest,
7220                                                             instance,
7221                                                             migration)
7222                 previous_data_remaining = info.data_remaining
7223 
7224                 curdowntime = libvirt_migrate.update_downtime(
7225                     guest, instance, curdowntime,
7226                     downtime_steps, elapsed)
7227 
7228                 # We loop every 500ms, so don't log on every
7229                 # iteration to avoid spamming logs for long
7230                 # running migrations. Just once every 5 secs
7231                 # is sufficient for developers to debug problems.
7232                 # We log once every 30 seconds at info to help
7233                 # admins see slow running migration operations
7234                 # when debug logs are off.
7235                 if (n % 10) == 0:
7236                     # Ignoring memory_processed, as due to repeated
7237                     # dirtying of data, this can be way larger than
7238                     # memory_total. Best to just look at what's
7239                     # remaining to copy and ignore what's done already
7240                     #
7241                     # TODO(berrange) perhaps we could include disk
7242                     # transfer stats in the progress too, but it
7243                     # might make memory info more obscure as large
7244                     # disk sizes might dwarf memory size
7245                     remaining = 100
7246                     if info.memory_total != 0:
7247                         remaining = round(info.memory_remaining *
7248                                           100 / info.memory_total)
7249 
7250                     libvirt_migrate.save_stats(instance, migration,
7251                                                info, remaining)
7252 
7253                     lg = LOG.debug
7254                     if (n % 60) == 0:
7255                         lg = LOG.info
7256 
7257                     lg("Migration running for %(secs)d secs, "
7258                        "memory %(remaining)d%% remaining; "
7259                        "(bytes processed=%(processed_memory)d, "
7260                        "remaining=%(remaining_memory)d, "
7261                        "total=%(total_memory)d)",
7262                        {"secs": n / 2, "remaining": remaining,
7263                         "processed_memory": info.memory_processed,
7264                         "remaining_memory": info.memory_remaining,
7265                         "total_memory": info.memory_total}, instance=instance)
7266                     if info.data_remaining > progress_watermark:
7267                         lg("Data remaining %(remaining)d bytes, "
7268                            "low watermark %(watermark)d bytes "
7269                            "%(last)d seconds ago",
7270                            {"remaining": info.data_remaining,
7271                             "watermark": progress_watermark,
7272                             "last": (now - progress_time)}, instance=instance)
7273 
7274                 n = n + 1
7275             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
7276                 # Migration is all done
7277                 LOG.info("Migration operation has completed",
7278                          instance=instance)
7279                 post_method(context, instance, dest, block_migration,
7280                             migrate_data)
7281                 break
7282             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
7283                 # Migration did not succeed
7284                 LOG.error("Migration operation has aborted", instance=instance)
7285                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
7286                                                   on_migration_failure)
7287                 recover_method(context, instance, dest, migrate_data)
7288                 break
7289             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
7290                 # Migration was stopped by admin
7291                 LOG.warning("Migration operation was cancelled",
7292                             instance=instance)
7293                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
7294                                                   on_migration_failure)
7295                 recover_method(context, instance, dest, migrate_data,
7296                                migration_status='cancelled')
7297                 break
7298             else:
7299                 LOG.warning("Unexpected migration job type: %d",
7300                             info.type, instance=instance)
7301 
7302             time.sleep(0.5)
7303         self._clear_empty_migration(instance)
7304 
7305     def _clear_empty_migration(self, instance):
7306         try:
7307             del self.active_migrations[instance.uuid]
7308         except KeyError:
7309             LOG.warning("There are no records in active migrations "
7310                         "for instance", instance=instance)
7311 
7312     def _live_migration(self, context, instance, dest, post_method,
7313                         recover_method, block_migration,
7314                         migrate_data):
7315         """Do live migration.
7316 
7317         :param context: security context
7318         :param instance:
7319             nova.db.sqlalchemy.models.Instance object
7320             instance object that is migrated.
7321         :param dest: destination host
7322         :param post_method:
7323             post operation method.
7324             expected nova.compute.manager._post_live_migration.
7325         :param recover_method:
7326             recovery method when any exception occurs.
7327             expected nova.compute.manager._rollback_live_migration.
7328         :param block_migration: if true, do block migration.
7329         :param migrate_data: a LibvirtLiveMigrateData object
7330 
7331         This fires off a new thread to run the blocking migration
7332         operation, and then this thread monitors the progress of
7333         migration and controls its operation
7334         """
7335 
7336         guest = self._host.get_guest(instance)
7337 
7338         disk_paths = []
7339         device_names = []
7340         if (migrate_data.block_migration and
7341                 CONF.libvirt.virt_type != "parallels"):
7342             disk_paths, device_names = self._live_migration_copy_disk_paths(
7343                 context, instance, guest)
7344 
7345         opthread = utils.spawn(self._live_migration_operation,
7346                                      context, instance, dest,
7347                                      block_migration,
7348                                      migrate_data, guest,
7349                                      device_names)
7350 
7351         finish_event = eventlet.event.Event()
7352         self.active_migrations[instance.uuid] = deque()
7353 
7354         def thread_finished(thread, event):
7355             LOG.debug("Migration operation thread notification",
7356                       instance=instance)
7357             event.send()
7358         opthread.link(thread_finished, finish_event)
7359 
7360         # Let eventlet schedule the new thread right away
7361         time.sleep(0)
7362 
7363         try:
7364             LOG.debug("Starting monitoring of live migration",
7365                       instance=instance)
7366             self._live_migration_monitor(context, instance, guest, dest,
7367                                          post_method, recover_method,
7368                                          block_migration, migrate_data,
7369                                          finish_event, disk_paths)
7370         except Exception as ex:
7371             LOG.warning("Error monitoring migration: %(ex)s",
7372                         {"ex": ex}, instance=instance, exc_info=True)
7373             raise
7374         finally:
7375             LOG.debug("Live migration monitoring is all done",
7376                       instance=instance)
7377 
7378     def _is_post_copy_enabled(self, migration_flags):
7379         if self._is_post_copy_available():
7380             if (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0:
7381                 return True
7382         return False
7383 
7384     def live_migration_force_complete(self, instance):
7385         try:
7386             self.active_migrations[instance.uuid].append('force-complete')
7387         except KeyError:
7388             raise exception.NoActiveMigrationForInstance(
7389                 instance_id=instance.uuid)
7390 
7391     def _try_fetch_image(self, context, path, image_id, instance,
7392                          fallback_from_host=None):
7393         try:
7394             libvirt_utils.fetch_image(context, path, image_id)
7395         except exception.ImageNotFound:
7396             if not fallback_from_host:
7397                 raise
7398             LOG.debug("Image %(image_id)s doesn't exist anymore on "
7399                       "image service, attempting to copy image "
7400                       "from %(host)s",
7401                       {'image_id': image_id, 'host': fallback_from_host})
7402             libvirt_utils.copy_image(src=path, dest=path,
7403                                      host=fallback_from_host,
7404                                      receive=True)
7405 
7406     def _fetch_instance_kernel_ramdisk(self, context, instance,
7407                                        fallback_from_host=None):
7408         """Download kernel and ramdisk for instance in instance directory."""
7409         instance_dir = libvirt_utils.get_instance_path(instance)
7410         if instance.kernel_id:
7411             kernel_path = os.path.join(instance_dir, 'kernel')
7412             # NOTE(dsanders): only fetch image if it's not available at
7413             # kernel_path. This also avoids ImageNotFound exception if
7414             # the image has been deleted from glance
7415             if not os.path.exists(kernel_path):
7416                 self._try_fetch_image(context,
7417                                       kernel_path,
7418                                       instance.kernel_id,
7419                                       instance, fallback_from_host)
7420             if instance.ramdisk_id:
7421                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
7422                 # NOTE(dsanders): only fetch image if it's not available at
7423                 # ramdisk_path. This also avoids ImageNotFound exception if
7424                 # the image has been deleted from glance
7425                 if not os.path.exists(ramdisk_path):
7426                     self._try_fetch_image(context,
7427                                           ramdisk_path,
7428                                           instance.ramdisk_id,
7429                                           instance, fallback_from_host)
7430 
7431     def rollback_live_migration_at_destination(self, context, instance,
7432                                                network_info,
7433                                                block_device_info,
7434                                                destroy_disks=True,
7435                                                migrate_data=None):
7436         """Clean up destination node after a failed live migration."""
7437         try:
7438             self.destroy(context, instance, network_info, block_device_info,
7439                          destroy_disks)
7440         finally:
7441             # NOTE(gcb): Failed block live migration may leave instance
7442             # directory at destination node, ensure it is always deleted.
7443             is_shared_instance_path = True
7444             if migrate_data:
7445                 is_shared_instance_path = migrate_data.is_shared_instance_path
7446                 if (migrate_data.obj_attr_is_set("serial_listen_ports")
7447                     and migrate_data.serial_listen_ports):
7448                     # Releases serial ports reserved.
7449                     for port in migrate_data.serial_listen_ports:
7450                         serial_console.release_port(
7451                             host=migrate_data.serial_listen_addr, port=port)
7452 
7453             if not is_shared_instance_path:
7454                 instance_dir = libvirt_utils.get_instance_path_at_destination(
7455                     instance, migrate_data)
7456                 if os.path.exists(instance_dir):
7457                     shutil.rmtree(instance_dir)
7458 
7459     def pre_live_migration(self, context, instance, block_device_info,
7460                            network_info, disk_info, migrate_data):
7461         """Preparation live migration."""
7462         if disk_info is not None:
7463             disk_info = jsonutils.loads(disk_info)
7464 
7465         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
7466                   instance=instance)
7467         is_shared_block_storage = migrate_data.is_shared_block_storage
7468         is_shared_instance_path = migrate_data.is_shared_instance_path
7469         is_block_migration = migrate_data.block_migration
7470 
7471         if not is_shared_instance_path:
7472             instance_dir = libvirt_utils.get_instance_path_at_destination(
7473                             instance, migrate_data)
7474 
7475             if os.path.exists(instance_dir):
7476                 raise exception.DestinationDiskExists(path=instance_dir)
7477 
7478             LOG.debug('Creating instance directory: %s', instance_dir,
7479                       instance=instance)
7480             os.mkdir(instance_dir)
7481 
7482             # Recreate the disk.info file and in doing so stop the
7483             # imagebackend from recreating it incorrectly by inspecting the
7484             # contents of each file when using the Raw backend.
7485             if disk_info:
7486                 image_disk_info = {}
7487                 for info in disk_info:
7488                     image_file = os.path.basename(info['path'])
7489                     image_path = os.path.join(instance_dir, image_file)
7490                     image_disk_info[image_path] = info['type']
7491 
7492                 LOG.debug('Creating disk.info with the contents: %s',
7493                           image_disk_info, instance=instance)
7494 
7495                 image_disk_info_path = os.path.join(instance_dir,
7496                                                     'disk.info')
7497                 libvirt_utils.write_to_file(image_disk_info_path,
7498                                             jsonutils.dumps(image_disk_info))
7499 
7500             if not is_shared_block_storage:
7501                 # Ensure images and backing files are present.
7502                 LOG.debug('Checking to make sure images and backing files are '
7503                           'present before live migration.', instance=instance)
7504                 self._create_images_and_backing(
7505                     context, instance, instance_dir, disk_info,
7506                     fallback_from_host=instance.host)
7507                 if (configdrive.required_by(instance) and
7508                         CONF.config_drive_format == 'iso9660'):
7509                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
7510                     # drive needs to be copied to destination prior to
7511                     # migration when instance path is not shared and block
7512                     # storage is not shared. Files that are already present
7513                     # on destination are excluded from a list of files that
7514                     # need to be copied to destination. If we don't do that
7515                     # live migration will fail on copying iso config drive to
7516                     # destination and writing to read-only device.
7517                     # Please see bug/1246201 for more details.
7518                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
7519                     self._remotefs.copy_file(src, instance_dir)
7520 
7521             if not is_block_migration:
7522                 # NOTE(angdraug): when block storage is shared between source
7523                 # and destination and instance path isn't (e.g. volume backed
7524                 # or rbd backed instance), instance path on destination has to
7525                 # be prepared
7526 
7527                 # Required by Quobyte CI
7528                 self._ensure_console_log_for_instance(instance)
7529 
7530                 # if image has kernel and ramdisk, just download
7531                 # following normal way.
7532                 self._fetch_instance_kernel_ramdisk(context, instance)
7533 
7534         # Establishing connection to volume server.
7535         block_device_mapping = driver.block_device_info_get_mapping(
7536             block_device_info)
7537 
7538         if len(block_device_mapping):
7539             LOG.debug('Connecting volumes before live migration.',
7540                       instance=instance)
7541 
7542         for bdm in block_device_mapping:
7543             connection_info = bdm['connection_info']
7544             # NOTE(lyarwood): Handle the P to Q LM during upgrade use case
7545             # where an instance has encrypted volumes attached using the
7546             # os-brick encryptors. Do not attempt to attach the encrypted
7547             # volume using native LUKS decryption on the destionation.
7548             src_native_luks = False
7549             if migrate_data.obj_attr_is_set('src_supports_native_luks'):
7550                 src_native_luks = migrate_data.src_supports_native_luks
7551             dest_native_luks = self._is_native_luks_available()
7552             allow_native_luks = src_native_luks and dest_native_luks
7553             self._connect_volume(context, connection_info, instance,
7554                                  allow_native_luks=allow_native_luks)
7555 
7556         # We call plug_vifs before the compute manager calls
7557         # ensure_filtering_rules_for_instance, to ensure bridge is set up
7558         # Retry operation is necessary because continuously request comes,
7559         # concurrent request occurs to iptables, then it complains.
7560         LOG.debug('Plugging VIFs before live migration.', instance=instance)
7561         max_retry = CONF.live_migration_retry_count
7562         for cnt in range(max_retry):
7563             try:
7564                 self.plug_vifs(instance, network_info)
7565                 break
7566             except processutils.ProcessExecutionError:
7567                 if cnt == max_retry - 1:
7568                     raise
7569                 else:
7570                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
7571                                 '%(max_retry)d.',
7572                                 {'cnt': cnt, 'max_retry': max_retry},
7573                                 instance=instance)
7574                     greenthread.sleep(1)
7575 
7576         # Store server_listen and latest disk device info
7577         if not migrate_data:
7578             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
7579         else:
7580             migrate_data.bdms = []
7581         # Store live_migration_inbound_addr
7582         migrate_data.target_connect_addr = \
7583             CONF.libvirt.live_migration_inbound_addr
7584         migrate_data.supported_perf_events = self._supported_perf_events
7585 
7586         migrate_data.serial_listen_ports = []
7587         if CONF.serial_console.enabled:
7588             num_ports = hardware.get_number_of_serial_ports(
7589                 instance.flavor, instance.image_meta)
7590             for port in six.moves.range(num_ports):
7591                 migrate_data.serial_listen_ports.append(
7592                     serial_console.acquire_port(
7593                         migrate_data.serial_listen_addr))
7594 
7595         for vol in block_device_mapping:
7596             connection_info = vol['connection_info']
7597             if connection_info.get('serial'):
7598                 disk_info = blockinfo.get_info_from_bdm(
7599                     instance, CONF.libvirt.virt_type,
7600                     instance.image_meta, vol)
7601 
7602                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
7603                 bdmi.serial = connection_info['serial']
7604                 bdmi.connection_info = connection_info
7605                 bdmi.bus = disk_info['bus']
7606                 bdmi.dev = disk_info['dev']
7607                 bdmi.type = disk_info['type']
7608                 bdmi.format = disk_info.get('format')
7609                 bdmi.boot_index = disk_info.get('boot_index')
7610                 volume_secret = self._host.find_secret('volume', vol.volume_id)
7611                 if volume_secret:
7612                     bdmi.encryption_secret_uuid = volume_secret.UUIDString()
7613 
7614                 migrate_data.bdms.append(bdmi)
7615 
7616         return migrate_data
7617 
7618     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
7619                                image_id, instance, size,
7620                                fallback_from_host=None):
7621         try:
7622             image.cache(fetch_func=fetch_func,
7623                         context=context,
7624                         filename=filename,
7625                         image_id=image_id,
7626                         size=size)
7627         except exception.ImageNotFound:
7628             if not fallback_from_host:
7629                 raise
7630             LOG.debug("Image %(image_id)s doesn't exist anymore "
7631                       "on image service, attempting to copy "
7632                       "image from %(host)s",
7633                       {'image_id': image_id, 'host': fallback_from_host},
7634                       instance=instance)
7635 
7636             def copy_from_host(target):
7637                 libvirt_utils.copy_image(src=target,
7638                                          dest=target,
7639                                          host=fallback_from_host,
7640                                          receive=True)
7641             image.cache(fetch_func=copy_from_host,
7642                         filename=filename)
7643 
7644     def _create_images_and_backing(self, context, instance, instance_dir,
7645                                    disk_info, fallback_from_host=None):
7646         """:param context: security context
7647            :param instance:
7648                nova.db.sqlalchemy.models.Instance object
7649                instance object that is migrated.
7650            :param instance_dir:
7651                instance path to use, calculated externally to handle block
7652                migrating an instance with an old style instance path
7653            :param disk_info:
7654                disk info specified in _get_instance_disk_info_from_config
7655                (list of dicts)
7656            :param fallback_from_host:
7657                host where we can retrieve images if the glance images are
7658                not available.
7659 
7660         """
7661 
7662         # Virtuozzo containers don't use backing file
7663         if (CONF.libvirt.virt_type == "parallels" and
7664                 instance.vm_mode == fields.VMMode.EXE):
7665             return
7666 
7667         if not disk_info:
7668             disk_info = []
7669 
7670         for info in disk_info:
7671             base = os.path.basename(info['path'])
7672             # Get image type and create empty disk image, and
7673             # create backing file in case of qcow2.
7674             instance_disk = os.path.join(instance_dir, base)
7675             if not info['backing_file'] and not os.path.exists(instance_disk):
7676                 libvirt_utils.create_image(info['type'], instance_disk,
7677                                            info['virt_disk_size'])
7678             elif info['backing_file']:
7679                 # Creating backing file follows same way as spawning instances.
7680                 cache_name = os.path.basename(info['backing_file'])
7681 
7682                 disk = self.image_backend.by_name(instance, instance_disk,
7683                                                   CONF.libvirt.images_type)
7684                 if cache_name.startswith('ephemeral'):
7685                     # The argument 'size' is used by image.cache to
7686                     # validate disk size retrieved from cache against
7687                     # the instance disk size (should always return OK)
7688                     # and ephemeral_size is used by _create_ephemeral
7689                     # to build the image if the disk is not already
7690                     # cached.
7691                     disk.cache(
7692                         fetch_func=self._create_ephemeral,
7693                         fs_label=cache_name,
7694                         os_type=instance.os_type,
7695                         filename=cache_name,
7696                         size=info['virt_disk_size'],
7697                         ephemeral_size=info['virt_disk_size'] / units.Gi)
7698                 elif cache_name.startswith('swap'):
7699                     inst_type = instance.get_flavor()
7700                     swap_mb = inst_type.swap
7701                     disk.cache(fetch_func=self._create_swap,
7702                                 filename="swap_%s" % swap_mb,
7703                                 size=swap_mb * units.Mi,
7704                                 swap_mb=swap_mb)
7705                 else:
7706                     self._try_fetch_image_cache(disk,
7707                                                 libvirt_utils.fetch_image,
7708                                                 context, cache_name,
7709                                                 instance.image_ref,
7710                                                 instance,
7711                                                 info['virt_disk_size'],
7712                                                 fallback_from_host)
7713 
7714         # if disk has kernel and ramdisk, just download
7715         # following normal way.
7716         self._fetch_instance_kernel_ramdisk(
7717             context, instance, fallback_from_host=fallback_from_host)
7718 
7719     def post_live_migration(self, context, instance, block_device_info,
7720                             migrate_data=None):
7721         # Disconnect from volume server
7722         block_device_mapping = driver.block_device_info_get_mapping(
7723                 block_device_info)
7724         volume_api = self._volume_api
7725         for vol in block_device_mapping:
7726             volume_id = vol['connection_info']['serial']
7727             if vol['attachment_id'] is None:
7728                 # Cinder v2 api flow: Retrieve connection info from Cinder's
7729                 # initialize_connection API. The info returned will be
7730                 # accurate for the source server.
7731                 connector = self.get_volume_connector(instance)
7732                 connection_info = volume_api.initialize_connection(
7733                     context, volume_id, connector)
7734             else:
7735                 # cinder v3.44 api flow: Retrieve the connection_info for
7736                 # the old attachment from cinder.
7737                 old_attachment_id = \
7738                     migrate_data.old_vol_attachment_ids[volume_id]
7739                 old_attachment = volume_api.attachment_get(
7740                     context, old_attachment_id)
7741                 connection_info = old_attachment['connection_info']
7742 
7743             # TODO(leeantho) The following multipath_id logic is temporary
7744             # and will be removed in the future once os-brick is updated
7745             # to handle multipath for drivers in a more efficient way.
7746             # For now this logic is needed to ensure the connection info
7747             # data is correct.
7748 
7749             # Pull out multipath_id from the bdm information. The
7750             # multipath_id can be placed into the connection info
7751             # because it is based off of the volume and will be the
7752             # same on the source and destination hosts.
7753             if 'multipath_id' in vol['connection_info']['data']:
7754                 multipath_id = vol['connection_info']['data']['multipath_id']
7755                 connection_info['data']['multipath_id'] = multipath_id
7756 
7757             self._disconnect_volume(context, connection_info, instance)
7758 
7759     def post_live_migration_at_source(self, context, instance, network_info):
7760         """Unplug VIFs from networks at source.
7761 
7762         :param context: security context
7763         :param instance: instance object reference
7764         :param network_info: instance network information
7765         """
7766         self.unplug_vifs(instance, network_info)
7767 
7768     def post_live_migration_at_destination(self, context,
7769                                            instance,
7770                                            network_info,
7771                                            block_migration=False,
7772                                            block_device_info=None):
7773         """Post operation of live migration at destination host.
7774 
7775         :param context: security context
7776         :param instance:
7777             nova.db.sqlalchemy.models.Instance object
7778             instance object that is migrated.
7779         :param network_info: instance network information
7780         :param block_migration: if true, post operation of block_migration.
7781         """
7782         # The source node set the VIR_MIGRATE_PERSIST_DEST flag when live
7783         # migrating so the guest xml should already be persisted on the
7784         # destination host, so just perform a sanity check to make sure it
7785         # made it as expected.
7786         self._host.get_guest(instance)
7787 
7788     def _get_instance_disk_info_from_config(self, guest_config,
7789                                             block_device_info):
7790         """Get the non-volume disk information from the domain xml
7791 
7792         :param LibvirtConfigGuest guest_config: the libvirt domain config
7793                                                 for the instance
7794         :param dict block_device_info: block device info for BDMs
7795         :returns disk_info: list of dicts with keys:
7796 
7797           * 'type': the disk type (str)
7798           * 'path': the disk path (str)
7799           * 'virt_disk_size': the virtual disk size (int)
7800           * 'backing_file': backing file of a disk image (str)
7801           * 'disk_size': physical disk size (int)
7802           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
7803         """
7804         block_device_mapping = driver.block_device_info_get_mapping(
7805             block_device_info)
7806 
7807         volume_devices = set()
7808         for vol in block_device_mapping:
7809             disk_dev = vol['mount_device'].rpartition("/")[2]
7810             volume_devices.add(disk_dev)
7811 
7812         disk_info = []
7813 
7814         if (guest_config.virt_type == 'parallels' and
7815                 guest_config.os_type == fields.VMMode.EXE):
7816             node_type = 'filesystem'
7817         else:
7818             node_type = 'disk'
7819 
7820         for device in guest_config.devices:
7821             if device.root_name != node_type:
7822                 continue
7823             disk_type = device.source_type
7824             if device.root_name == 'filesystem':
7825                 target = device.target_dir
7826                 if device.source_type == 'file':
7827                     path = device.source_file
7828                 elif device.source_type == 'block':
7829                     path = device.source_dev
7830                 else:
7831                     path = None
7832             else:
7833                 target = device.target_dev
7834                 path = device.source_path
7835 
7836             if not path:
7837                 LOG.debug('skipping disk for %s as it does not have a path',
7838                           guest_config.name)
7839                 continue
7840 
7841             if disk_type not in ['file', 'block']:
7842                 LOG.debug('skipping disk because it looks like a volume', path)
7843                 continue
7844 
7845             if target in volume_devices:
7846                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
7847                           'volume', {'path': path, 'target': target})
7848                 continue
7849 
7850             if device.root_name == 'filesystem':
7851                 driver_type = device.driver_type
7852             else:
7853                 driver_type = device.driver_format
7854             # get the real disk size or
7855             # raise a localized error if image is unavailable
7856             if disk_type == 'file':
7857                 if driver_type == 'ploop':
7858                     dk_size = 0
7859                     for dirpath, dirnames, filenames in os.walk(path):
7860                         for f in filenames:
7861                             fp = os.path.join(dirpath, f)
7862                             dk_size += os.path.getsize(fp)
7863                 else:
7864                     dk_size = int(os.path.getsize(path))
7865             elif disk_type == 'block' and block_device_info:
7866                 dk_size = lvm.get_volume_size(path)
7867             else:
7868                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
7869                           'determine if volume',
7870                           {'path': path, 'target': target})
7871                 continue
7872 
7873             if driver_type in ("qcow2", "ploop"):
7874                 backing_file = libvirt_utils.get_disk_backing_file(path)
7875                 virt_size = disk_api.get_disk_size(path)
7876                 over_commit_size = int(virt_size) - dk_size
7877             else:
7878                 backing_file = ""
7879                 virt_size = dk_size
7880                 over_commit_size = 0
7881 
7882             disk_info.append({'type': driver_type,
7883                               'path': path,
7884                               'virt_disk_size': virt_size,
7885                               'backing_file': backing_file,
7886                               'disk_size': dk_size,
7887                               'over_committed_disk_size': over_commit_size})
7888         return disk_info
7889 
7890     def _get_instance_disk_info(self, instance, block_device_info):
7891         try:
7892             guest = self._host.get_guest(instance)
7893             config = guest.get_config()
7894         except libvirt.libvirtError as ex:
7895             error_code = ex.get_error_code()
7896             LOG.warning('Error from libvirt while getting description of '
7897                         '%(instance_name)s: [Error Code %(error_code)s] '
7898                         '%(ex)s',
7899                         {'instance_name': instance.name,
7900                          'error_code': error_code,
7901                          'ex': encodeutils.exception_to_unicode(ex)},
7902                         instance=instance)
7903             raise exception.InstanceNotFound(instance_id=instance.uuid)
7904 
7905         return self._get_instance_disk_info_from_config(config,
7906                                                         block_device_info)
7907 
7908     def get_instance_disk_info(self, instance,
7909                                block_device_info=None):
7910         return jsonutils.dumps(
7911             self._get_instance_disk_info(instance, block_device_info))
7912 
7913     def _get_disk_over_committed_size_total(self):
7914         """Return total over committed disk size for all instances."""
7915         # Disk size that all instance uses : virtual_size - disk_size
7916         disk_over_committed_size = 0
7917         instance_domains = self._host.list_instance_domains(only_running=False)
7918         if not instance_domains:
7919             return disk_over_committed_size
7920 
7921         # Get all instance uuids
7922         instance_uuids = [dom.UUIDString() for dom in instance_domains]
7923         ctx = nova_context.get_admin_context()
7924         # Get instance object list by uuid filter
7925         filters = {'uuid': instance_uuids}
7926         # NOTE(ankit): objects.InstanceList.get_by_filters method is
7927         # getting called twice one is here and another in the
7928         # _update_available_resource method of resource_tracker. Since
7929         # _update_available_resource method is synchronized, there is a
7930         # possibility the instances list retrieved here to calculate
7931         # disk_over_committed_size would differ to the list you would get
7932         # in _update_available_resource method for calculating usages based
7933         # on instance utilization.
7934         local_instance_list = objects.InstanceList.get_by_filters(
7935             ctx, filters, use_slave=True)
7936         # Convert instance list to dictionary with instance uuid as key.
7937         local_instances = {inst.uuid: inst for inst in local_instance_list}
7938 
7939         # Get bdms by instance uuids
7940         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
7941             ctx, instance_uuids)
7942 
7943         for dom in instance_domains:
7944             try:
7945                 guest = libvirt_guest.Guest(dom)
7946                 config = guest.get_config()
7947 
7948                 block_device_info = None
7949                 if guest.uuid in local_instances \
7950                         and (bdms and guest.uuid in bdms):
7951                     # Get block device info for instance
7952                     block_device_info = driver.get_block_device_info(
7953                         local_instances[guest.uuid], bdms[guest.uuid])
7954 
7955                 disk_infos = self._get_instance_disk_info_from_config(
7956                     config, block_device_info)
7957                 if not disk_infos:
7958                     continue
7959 
7960                 for info in disk_infos:
7961                     disk_over_committed_size += int(
7962                         info['over_committed_disk_size'])
7963             except libvirt.libvirtError as ex:
7964                 error_code = ex.get_error_code()
7965                 LOG.warning(
7966                     'Error from libvirt while getting description of '
7967                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
7968                     {'instance_name': guest.name,
7969                      'error_code': error_code,
7970                      'ex': encodeutils.exception_to_unicode(ex)})
7971             except OSError as e:
7972                 if e.errno in (errno.ENOENT, errno.ESTALE):
7973                     LOG.warning('Periodic task is updating the host stat, '
7974                                 'it is trying to get disk %(i_name)s, '
7975                                 'but disk file was removed by concurrent '
7976                                 'operations such as resize.',
7977                                 {'i_name': guest.name})
7978                 elif e.errno == errno.EACCES:
7979                     LOG.warning('Periodic task is updating the host stat, '
7980                                 'it is trying to get disk %(i_name)s, '
7981                                 'but access is denied. It is most likely '
7982                                 'due to a VM that exists on the compute '
7983                                 'node but is not managed by Nova.',
7984                                 {'i_name': guest.name})
7985                 else:
7986                     raise
7987             except exception.VolumeBDMPathNotFound as e:
7988                 LOG.warning('Periodic task is updating the host stats, '
7989                             'it is trying to get disk info for %(i_name)s, '
7990                             'but the backing volume block device was removed '
7991                             'by concurrent operations such as resize. '
7992                             'Error: %(error)s',
7993                             {'i_name': guest.name, 'error': e})
7994             # NOTE(gtt116): give other tasks a chance.
7995             greenthread.sleep(0)
7996         return disk_over_committed_size
7997 
7998     def unfilter_instance(self, instance, network_info):
7999         """See comments of same method in firewall_driver."""
8000         self.firewall_driver.unfilter_instance(instance,
8001                                                network_info=network_info)
8002 
8003     def get_available_nodes(self, refresh=False):
8004         return [self._host.get_hostname()]
8005 
8006     def get_host_cpu_stats(self):
8007         """Return the current CPU state of the host."""
8008         return self._host.get_cpu_stats()
8009 
8010     def get_host_uptime(self):
8011         """Returns the result of calling "uptime"."""
8012         out, err = utils.execute('env', 'LANG=C', 'uptime')
8013         return out
8014 
8015     def manage_image_cache(self, context, all_instances):
8016         """Manage the local cache of images."""
8017         self.image_cache_manager.update(context, all_instances)
8018 
8019     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
8020                                   shared_storage=False):
8021         """Used only for cleanup in case migrate_disk_and_power_off fails."""
8022         try:
8023             if os.path.exists(inst_base_resize):
8024                 shutil.rmtree(inst_base)
8025                 utils.execute('mv', inst_base_resize, inst_base)
8026                 if not shared_storage:
8027                     self._remotefs.remove_dir(dest, inst_base)
8028         except Exception:
8029             pass
8030 
8031     def _is_storage_shared_with(self, dest, inst_base):
8032         # NOTE (rmk): There are two methods of determining whether we are
8033         #             on the same filesystem: the source and dest IP are the
8034         #             same, or we create a file on the dest system via SSH
8035         #             and check whether the source system can also see it.
8036         # NOTE (drwahl): Actually, there is a 3rd way: if images_type is rbd,
8037         #                it will always be shared storage
8038         if CONF.libvirt.images_type == 'rbd':
8039             return True
8040         shared_storage = (dest == self.get_host_ip_addr())
8041         if not shared_storage:
8042             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
8043             tmp_path = os.path.join(inst_base, tmp_file)
8044 
8045             try:
8046                 self._remotefs.create_file(dest, tmp_path)
8047                 if os.path.exists(tmp_path):
8048                     shared_storage = True
8049                     os.unlink(tmp_path)
8050                 else:
8051                     self._remotefs.remove_file(dest, tmp_path)
8052             except Exception:
8053                 pass
8054         return shared_storage
8055 
8056     def migrate_disk_and_power_off(self, context, instance, dest,
8057                                    flavor, network_info,
8058                                    block_device_info=None,
8059                                    timeout=0, retry_interval=0):
8060         LOG.debug("Starting migrate_disk_and_power_off",
8061                    instance=instance)
8062 
8063         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
8064 
8065         # get_bdm_ephemeral_disk_size() will return 0 if the new
8066         # instance's requested block device mapping contain no
8067         # ephemeral devices. However, we still want to check if
8068         # the original instance's ephemeral_gb property was set and
8069         # ensure that the new requested flavor ephemeral size is greater
8070         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
8071                     instance.flavor.ephemeral_gb)
8072 
8073         # Checks if the migration needs a disk resize down.
8074         root_down = flavor.root_gb < instance.flavor.root_gb
8075         ephemeral_down = flavor.ephemeral_gb < eph_size
8076         booted_from_volume = self._is_booted_from_volume(block_device_info)
8077 
8078         if (root_down and not booted_from_volume) or ephemeral_down:
8079             reason = _("Unable to resize disk down.")
8080             raise exception.InstanceFaultRollback(
8081                 exception.ResizeError(reason=reason))
8082 
8083         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
8084         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
8085             reason = _("Migration is not supported for LVM backed instances")
8086             raise exception.InstanceFaultRollback(
8087                 exception.MigrationPreCheckError(reason=reason))
8088 
8089         # copy disks to destination
8090         # rename instance dir to +_resize at first for using
8091         # shared storage for instance dir (eg. NFS).
8092         inst_base = libvirt_utils.get_instance_path(instance)
8093         inst_base_resize = inst_base + "_resize"
8094         shared_storage = self._is_storage_shared_with(dest, inst_base)
8095 
8096         # try to create the directory on the remote compute node
8097         # if this fails we pass the exception up the stack so we can catch
8098         # failures here earlier
8099         if not shared_storage:
8100             try:
8101                 self._remotefs.create_dir(dest, inst_base)
8102             except processutils.ProcessExecutionError as e:
8103                 reason = _("not able to execute ssh command: %s") % e
8104                 raise exception.InstanceFaultRollback(
8105                     exception.ResizeError(reason=reason))
8106 
8107         self.power_off(instance, timeout, retry_interval)
8108 
8109         block_device_mapping = driver.block_device_info_get_mapping(
8110             block_device_info)
8111         for vol in block_device_mapping:
8112             connection_info = vol['connection_info']
8113             self._disconnect_volume(context, connection_info, instance)
8114 
8115         disk_info = self._get_instance_disk_info(instance, block_device_info)
8116 
8117         try:
8118             utils.execute('mv', inst_base, inst_base_resize)
8119             # if we are migrating the instance with shared storage then
8120             # create the directory.  If it is a remote node the directory
8121             # has already been created
8122             if shared_storage:
8123                 dest = None
8124                 fileutils.ensure_tree(inst_base)
8125 
8126             on_execute = lambda process: \
8127                 self.job_tracker.add_job(instance, process.pid)
8128             on_completion = lambda process: \
8129                 self.job_tracker.remove_job(instance, process.pid)
8130 
8131             for info in disk_info:
8132                 # assume inst_base == dirname(info['path'])
8133                 img_path = info['path']
8134                 fname = os.path.basename(img_path)
8135                 from_path = os.path.join(inst_base_resize, fname)
8136 
8137                 # We will not copy over the swap disk here, and rely on
8138                 # finish_migration to re-create it for us. This is ok because
8139                 # the OS is shut down, and as recreating a swap disk is very
8140                 # cheap it is more efficient than copying either locally or
8141                 # over the network. This also means we don't have to resize it.
8142                 if fname == 'disk.swap':
8143                     continue
8144 
8145                 compression = info['type'] not in NO_COMPRESSION_TYPES
8146                 libvirt_utils.copy_image(from_path, img_path, host=dest,
8147                                          on_execute=on_execute,
8148                                          on_completion=on_completion,
8149                                          compression=compression)
8150 
8151             # Ensure disk.info is written to the new path to avoid disks being
8152             # reinspected and potentially changing format.
8153             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
8154             if os.path.exists(src_disk_info_path):
8155                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
8156                 libvirt_utils.copy_image(src_disk_info_path,
8157                                          dst_disk_info_path,
8158                                          host=dest, on_execute=on_execute,
8159                                          on_completion=on_completion)
8160         except Exception:
8161             with excutils.save_and_reraise_exception():
8162                 self._cleanup_remote_migration(dest, inst_base,
8163                                                inst_base_resize,
8164                                                shared_storage)
8165 
8166         return jsonutils.dumps(disk_info)
8167 
8168     def _wait_for_running(self, instance):
8169         state = self.get_info(instance).state
8170 
8171         if state == power_state.RUNNING:
8172             LOG.info("Instance running successfully.", instance=instance)
8173             raise loopingcall.LoopingCallDone()
8174 
8175     @staticmethod
8176     def _disk_raw_to_qcow2(path):
8177         """Converts a raw disk to qcow2."""
8178         path_qcow = path + '_qcow'
8179         utils.execute('qemu-img', 'convert', '-f', 'raw',
8180                       '-O', 'qcow2', path, path_qcow)
8181         utils.execute('mv', path_qcow, path)
8182 
8183     @staticmethod
8184     def _disk_qcow2_to_raw(path):
8185         """Converts a qcow2 disk to raw."""
8186         path_raw = path + '_raw'
8187         utils.execute('qemu-img', 'convert', '-f', 'qcow2',
8188                       '-O', 'raw', path, path_raw)
8189         utils.execute('mv', path_raw, path)
8190 
8191     def finish_migration(self, context, migration, instance, disk_info,
8192                          network_info, image_meta, resize_instance,
8193                          block_device_info=None, power_on=True):
8194         LOG.debug("Starting finish_migration", instance=instance)
8195 
8196         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
8197                                                   instance,
8198                                                   image_meta,
8199                                                   block_device_info)
8200         # assume _create_image does nothing if a target file exists.
8201         # NOTE: This has the intended side-effect of fetching a missing
8202         # backing file.
8203         self._create_image(context, instance, block_disk_info['mapping'],
8204                            block_device_info=block_device_info,
8205                            ignore_bdi_for_swap=True,
8206                            fallback_from_host=migration.source_compute)
8207 
8208         # Required by Quobyte CI
8209         self._ensure_console_log_for_instance(instance)
8210 
8211         gen_confdrive = functools.partial(
8212             self._create_configdrive, context, instance,
8213             InjectionInfo(admin_pass=None, network_info=network_info,
8214                           files=None))
8215 
8216         # Convert raw disks to qcow2 if migrating to host which uses
8217         # qcow2 from host which uses raw.
8218         disk_info = jsonutils.loads(disk_info)
8219         for info in disk_info:
8220             path = info['path']
8221             disk_name = os.path.basename(path)
8222 
8223             # NOTE(mdbooth): The code below looks wrong, but is actually
8224             # required to prevent a security hole when migrating from a host
8225             # with use_cow_images=False to one with use_cow_images=True.
8226             # Imagebackend uses use_cow_images to select between the
8227             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
8228             # writes to disk.info, but does not read it as it assumes qcow2.
8229             # Therefore if we don't convert raw to qcow2 here, a raw disk will
8230             # be incorrectly assumed to be qcow2, which is a severe security
8231             # flaw. The reverse is not true, because the atrociously-named-Raw
8232             # backend supports both qcow2 and raw disks, and will choose
8233             # appropriately between them as long as disk.info exists and is
8234             # correctly populated, which it is because Qcow2 writes to
8235             # disk.info.
8236             #
8237             # In general, we do not yet support format conversion during
8238             # migration. For example:
8239             #   * Converting from use_cow_images=True to use_cow_images=False
8240             #     isn't handled. This isn't a security bug, but is almost
8241             #     certainly buggy in other cases, as the 'Raw' backend doesn't
8242             #     expect a backing file.
8243             #   * Converting to/from lvm and rbd backends is not supported.
8244             #
8245             # This behaviour is inconsistent, and therefore undesirable for
8246             # users. It is tightly-coupled to implementation quirks of 2
8247             # out of 5 backends in imagebackend and defends against a severe
8248             # security flaw which is not at all obvious without deep analysis,
8249             # and is therefore undesirable to developers. We should aim to
8250             # remove it. This will not be possible, though, until we can
8251             # represent the storage layout of a specific instance
8252             # independent of the default configuration of the local compute
8253             # host.
8254 
8255             # Config disks are hard-coded to be raw even when
8256             # use_cow_images=True (see _get_disk_config_image_type),so don't
8257             # need to be converted.
8258             if (disk_name != 'disk.config' and
8259                         info['type'] == 'raw' and CONF.use_cow_images):
8260                 self._disk_raw_to_qcow2(info['path'])
8261 
8262         xml = self._get_guest_xml(context, instance, network_info,
8263                                   block_disk_info, image_meta,
8264                                   block_device_info=block_device_info)
8265         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
8266         # or not we've migrated to another host, because we unplug VIFs locally
8267         # and the status change in the port might go undetected by the neutron
8268         # L2 agent (or neutron server) so neutron may not know that the VIF was
8269         # unplugged in the first place and never send an event.
8270         guest = self._create_domain_and_network(context, xml, instance,
8271                                         network_info,
8272                                         block_device_info=block_device_info,
8273                                         power_on=power_on,
8274                                         vifs_already_plugged=True,
8275                                         post_xml_callback=gen_confdrive)
8276         if power_on:
8277             timer = loopingcall.FixedIntervalLoopingCall(
8278                                                     self._wait_for_running,
8279                                                     instance)
8280             timer.start(interval=0.5).wait()
8281 
8282             # Sync guest time after migration.
8283             guest.sync_guest_time()
8284 
8285         LOG.debug("finish_migration finished successfully.", instance=instance)
8286 
8287     def _cleanup_failed_migration(self, inst_base):
8288         """Make sure that a failed migrate doesn't prevent us from rolling
8289         back in a revert.
8290         """
8291         try:
8292             shutil.rmtree(inst_base)
8293         except OSError as e:
8294             if e.errno != errno.ENOENT:
8295                 raise
8296 
8297     def finish_revert_migration(self, context, instance, network_info,
8298                                 block_device_info=None, power_on=True):
8299         LOG.debug("Starting finish_revert_migration",
8300                   instance=instance)
8301 
8302         inst_base = libvirt_utils.get_instance_path(instance)
8303         inst_base_resize = inst_base + "_resize"
8304 
8305         # NOTE(danms): if we're recovering from a failed migration,
8306         # make sure we don't have a left-over same-host base directory
8307         # that would conflict. Also, don't fail on the rename if the
8308         # failure happened early.
8309         if os.path.exists(inst_base_resize):
8310             self._cleanup_failed_migration(inst_base)
8311             utils.execute('mv', inst_base_resize, inst_base)
8312 
8313         root_disk = self.image_backend.by_name(instance, 'disk')
8314         # Once we rollback, the snapshot is no longer needed, so remove it
8315         # TODO(nic): Remove the try/except/finally in a future release
8316         # To avoid any upgrade issues surrounding instances being in pending
8317         # resize state when the software is updated, this portion of the
8318         # method logs exceptions rather than failing on them.  Once it can be
8319         # reasonably assumed that no such instances exist in the wild
8320         # anymore, the try/except/finally should be removed,
8321         # and ignore_errors should be set back to False (the default) so
8322         # that problems throw errors, like they should.
8323         if root_disk.exists():
8324             try:
8325                 root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
8326             except exception.SnapshotNotFound:
8327                 LOG.warning("Failed to rollback snapshot (%s)",
8328                             libvirt_utils.RESIZE_SNAPSHOT_NAME)
8329             finally:
8330                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
8331                                       ignore_errors=True)
8332 
8333         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
8334                                             instance,
8335                                             instance.image_meta,
8336                                             block_device_info)
8337         xml = self._get_guest_xml(context, instance, network_info, disk_info,
8338                                   instance.image_meta,
8339                                   block_device_info=block_device_info)
8340         self._create_domain_and_network(context, xml, instance, network_info,
8341                                         block_device_info=block_device_info,
8342                                         power_on=power_on,
8343                                         vifs_already_plugged=True)
8344 
8345         if power_on:
8346             timer = loopingcall.FixedIntervalLoopingCall(
8347                                                     self._wait_for_running,
8348                                                     instance)
8349             timer.start(interval=0.5).wait()
8350 
8351         LOG.debug("finish_revert_migration finished successfully.",
8352                   instance=instance)
8353 
8354     def confirm_migration(self, context, migration, instance, network_info):
8355         """Confirms a resize, destroying the source VM."""
8356         self._cleanup_resize(context, instance, network_info)
8357 
8358     @staticmethod
8359     def _get_io_devices(xml_doc):
8360         """get the list of io devices from the xml document."""
8361         result = {"volumes": [], "ifaces": []}
8362         try:
8363             doc = etree.fromstring(xml_doc)
8364         except Exception:
8365             return result
8366         blocks = [('./devices/disk', 'volumes'),
8367             ('./devices/interface', 'ifaces')]
8368         for block, key in blocks:
8369             section = doc.findall(block)
8370             for node in section:
8371                 for child in node.getchildren():
8372                     if child.tag == 'target' and child.get('dev'):
8373                         result[key].append(child.get('dev'))
8374         return result
8375 
8376     def get_diagnostics(self, instance):
8377         guest = self._host.get_guest(instance)
8378 
8379         # TODO(sahid): We are converting all calls from a
8380         # virDomain object to use nova.virt.libvirt.Guest.
8381         # We should be able to remove domain at the end.
8382         domain = guest._domain
8383         output = {}
8384         # get cpu time, might launch an exception if the method
8385         # is not supported by the underlying hypervisor being
8386         # used by libvirt
8387         try:
8388             for vcpu in guest.get_vcpus_info():
8389                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
8390         except libvirt.libvirtError:
8391             pass
8392         # get io status
8393         xml = guest.get_xml_desc()
8394         dom_io = LibvirtDriver._get_io_devices(xml)
8395         for guest_disk in dom_io["volumes"]:
8396             try:
8397                 # blockStats might launch an exception if the method
8398                 # is not supported by the underlying hypervisor being
8399                 # used by libvirt
8400                 stats = domain.blockStats(guest_disk)
8401                 output[guest_disk + "_read_req"] = stats[0]
8402                 output[guest_disk + "_read"] = stats[1]
8403                 output[guest_disk + "_write_req"] = stats[2]
8404                 output[guest_disk + "_write"] = stats[3]
8405                 output[guest_disk + "_errors"] = stats[4]
8406             except libvirt.libvirtError:
8407                 pass
8408         for interface in dom_io["ifaces"]:
8409             try:
8410                 # interfaceStats might launch an exception if the method
8411                 # is not supported by the underlying hypervisor being
8412                 # used by libvirt
8413                 stats = domain.interfaceStats(interface)
8414                 output[interface + "_rx"] = stats[0]
8415                 output[interface + "_rx_packets"] = stats[1]
8416                 output[interface + "_rx_errors"] = stats[2]
8417                 output[interface + "_rx_drop"] = stats[3]
8418                 output[interface + "_tx"] = stats[4]
8419                 output[interface + "_tx_packets"] = stats[5]
8420                 output[interface + "_tx_errors"] = stats[6]
8421                 output[interface + "_tx_drop"] = stats[7]
8422             except libvirt.libvirtError:
8423                 pass
8424         output["memory"] = domain.maxMemory()
8425         # memoryStats might launch an exception if the method
8426         # is not supported by the underlying hypervisor being
8427         # used by libvirt
8428         try:
8429             mem = domain.memoryStats()
8430             for key in mem.keys():
8431                 output["memory-" + key] = mem[key]
8432         except (libvirt.libvirtError, AttributeError):
8433             pass
8434         return output
8435 
8436     def get_instance_diagnostics(self, instance):
8437         guest = self._host.get_guest(instance)
8438 
8439         # TODO(sahid): We are converting all calls from a
8440         # virDomain object to use nova.virt.libvirt.Guest.
8441         # We should be able to remove domain at the end.
8442         domain = guest._domain
8443 
8444         xml = guest.get_xml_desc()
8445         xml_doc = etree.fromstring(xml)
8446 
8447         # TODO(sahid): Needs to use get_info but more changes have to
8448         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
8449         # needed.
8450         (state, max_mem, mem, num_cpu, cpu_time) = \
8451             guest._get_domain_info(self._host)
8452         config_drive = configdrive.required_by(instance)
8453         launched_at = timeutils.normalize_time(instance.launched_at)
8454         uptime = timeutils.delta_seconds(launched_at,
8455                                          timeutils.utcnow())
8456         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
8457                                         driver='libvirt',
8458                                         config_drive=config_drive,
8459                                         hypervisor=CONF.libvirt.virt_type,
8460                                         hypervisor_os='linux',
8461                                         uptime=uptime)
8462         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
8463             maximum=max_mem / units.Mi,
8464             used=mem / units.Mi)
8465 
8466         # get cpu time, might launch an exception if the method
8467         # is not supported by the underlying hypervisor being
8468         # used by libvirt
8469         try:
8470             for vcpu in guest.get_vcpus_info():
8471                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
8472         except libvirt.libvirtError:
8473             pass
8474         # get io status
8475         dom_io = LibvirtDriver._get_io_devices(xml)
8476         for guest_disk in dom_io["volumes"]:
8477             try:
8478                 # blockStats might launch an exception if the method
8479                 # is not supported by the underlying hypervisor being
8480                 # used by libvirt
8481                 stats = domain.blockStats(guest_disk)
8482                 diags.add_disk(read_bytes=stats[1],
8483                                read_requests=stats[0],
8484                                write_bytes=stats[3],
8485                                write_requests=stats[2],
8486                                errors_count=stats[4])
8487             except libvirt.libvirtError:
8488                 pass
8489         for interface in dom_io["ifaces"]:
8490             try:
8491                 # interfaceStats might launch an exception if the method
8492                 # is not supported by the underlying hypervisor being
8493                 # used by libvirt
8494                 stats = domain.interfaceStats(interface)
8495                 diags.add_nic(rx_octets=stats[0],
8496                               rx_errors=stats[2],
8497                               rx_drop=stats[3],
8498                               rx_packets=stats[1],
8499                               tx_octets=stats[4],
8500                               tx_errors=stats[6],
8501                               tx_drop=stats[7],
8502                               tx_packets=stats[5])
8503             except libvirt.libvirtError:
8504                 pass
8505 
8506         # Update mac addresses of interface if stats have been reported
8507         if diags.nic_details:
8508             nodes = xml_doc.findall('./devices/interface/mac')
8509             for index, node in enumerate(nodes):
8510                 diags.nic_details[index].mac_address = node.get('address')
8511         return diags
8512 
8513     @staticmethod
8514     def _prepare_device_bus(dev):
8515         """Determines the device bus and its hypervisor assigned address
8516         """
8517         bus = None
8518         address = (dev.device_addr.format_address() if
8519                    dev.device_addr else None)
8520         if isinstance(dev.device_addr,
8521                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
8522             bus = objects.PCIDeviceBus()
8523         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
8524             if dev.target_bus == 'scsi':
8525                 bus = objects.SCSIDeviceBus()
8526             elif dev.target_bus == 'ide':
8527                 bus = objects.IDEDeviceBus()
8528             elif dev.target_bus == 'usb':
8529                 bus = objects.USBDeviceBus()
8530         if address is not None and bus is not None:
8531             bus.address = address
8532         return bus
8533 
8534     def _build_device_metadata(self, context, instance):
8535         """Builds a metadata object for instance devices, that maps the user
8536            provided tag to the hypervisor assigned device address.
8537         """
8538         def _get_device_name(bdm):
8539             return block_device.strip_dev(bdm.device_name)
8540 
8541         network_info = instance.info_cache.network_info
8542         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
8543         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
8544                                                                  instance.uuid)
8545         vifs_to_expose = {vif.address: vif for vif in vifs
8546                           if ('tag' in vif and vif.tag) or
8547                              vlans_by_mac.get(vif.address)}
8548         # TODO(mriedem): We should be able to avoid the DB query here by using
8549         # block_device_info['block_device_mapping'] which is passed into most
8550         # methods that call this function.
8551         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8552             context, instance.uuid)
8553         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
8554 
8555         devices = []
8556         guest = self._host.get_guest(instance)
8557         xml = guest.get_xml_desc()
8558         xml_dom = etree.fromstring(xml)
8559         guest_config = vconfig.LibvirtConfigGuest()
8560         guest_config.parse_dom(xml_dom)
8561 
8562         for dev in guest_config.devices:
8563             # Build network interfaces related metadata
8564             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
8565                 vif = vifs_to_expose.get(dev.mac_addr)
8566                 if not vif:
8567                     continue
8568                 bus = self._prepare_device_bus(dev)
8569                 device = objects.NetworkInterfaceMetadata(mac=vif.address)
8570                 if 'tag' in vif and vif.tag:
8571                     device.tags = [vif.tag]
8572                 if bus:
8573                     device.bus = bus
8574                 vlan = vlans_by_mac.get(vif.address)
8575                 if vlan:
8576                     device.vlan = int(vlan)
8577                 devices.append(device)
8578 
8579             # Build disks related metadata
8580             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
8581                 bdm = tagged_bdms.get(dev.target_dev)
8582                 if not bdm:
8583                     continue
8584                 bus = self._prepare_device_bus(dev)
8585                 device = objects.DiskMetadata(tags=[bdm.tag])
8586                 # NOTE(artom) Setting the serial (which corresponds to
8587                 # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
8588                 # find the disks's BlockDeviceMapping object when we detach the
8589                 # volume and want to clean up its metadata.
8590                 device.serial = bdm.volume_id
8591                 if bus:
8592                     device.bus = bus
8593                 devices.append(device)
8594         if devices:
8595             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
8596             return dev_meta
8597 
8598     def instance_on_disk(self, instance):
8599         # ensure directories exist and are writable
8600         instance_path = libvirt_utils.get_instance_path(instance)
8601         LOG.debug('Checking instance files accessibility %s', instance_path,
8602                   instance=instance)
8603         shared_instance_path = os.access(instance_path, os.W_OK)
8604         # NOTE(flwang): For shared block storage scenario, the file system is
8605         # not really shared by the two hosts, but the volume of evacuated
8606         # instance is reachable.
8607         shared_block_storage = (self.image_backend.backend().
8608                                 is_shared_block_storage())
8609         return shared_instance_path or shared_block_storage
8610 
8611     def inject_network_info(self, instance, nw_info):
8612         self.firewall_driver.setup_basic_filtering(instance, nw_info)
8613 
8614     def delete_instance_files(self, instance):
8615         target = libvirt_utils.get_instance_path(instance)
8616         # A resize may be in progress
8617         target_resize = target + '_resize'
8618         # Other threads may attempt to rename the path, so renaming the path
8619         # to target + '_del' (because it is atomic) and iterating through
8620         # twice in the unlikely event that a concurrent rename occurs between
8621         # the two rename attempts in this method. In general this method
8622         # should be fairly thread-safe without these additional checks, since
8623         # other operations involving renames are not permitted when the task
8624         # state is not None and the task state should be set to something
8625         # other than None by the time this method is invoked.
8626         target_del = target + '_del'
8627         for i in range(2):
8628             try:
8629                 utils.execute('mv', target, target_del)
8630                 break
8631             except Exception:
8632                 pass
8633             try:
8634                 utils.execute('mv', target_resize, target_del)
8635                 break
8636             except Exception:
8637                 pass
8638         # Either the target or target_resize path may still exist if all
8639         # rename attempts failed.
8640         remaining_path = None
8641         for p in (target, target_resize):
8642             if os.path.exists(p):
8643                 remaining_path = p
8644                 break
8645 
8646         # A previous delete attempt may have been interrupted, so target_del
8647         # may exist even if all rename attempts during the present method
8648         # invocation failed due to the absence of both target and
8649         # target_resize.
8650         if not remaining_path and os.path.exists(target_del):
8651             self.job_tracker.terminate_jobs(instance)
8652 
8653             LOG.info('Deleting instance files %s', target_del,
8654                      instance=instance)
8655             remaining_path = target_del
8656             try:
8657                 shutil.rmtree(target_del)
8658             except OSError as e:
8659                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
8660                           {'target': target_del, 'e': e}, instance=instance)
8661 
8662         # It is possible that the delete failed, if so don't mark the instance
8663         # as cleaned.
8664         if remaining_path and os.path.exists(remaining_path):
8665             LOG.info('Deletion of %s failed', remaining_path,
8666                      instance=instance)
8667             return False
8668 
8669         LOG.info('Deletion of %s complete', target_del, instance=instance)
8670         return True
8671 
8672     @property
8673     def need_legacy_block_device_info(self):
8674         return False
8675 
8676     def default_root_device_name(self, instance, image_meta, root_bdm):
8677         disk_bus = blockinfo.get_disk_bus_for_device_type(
8678             instance, CONF.libvirt.virt_type, image_meta, "disk")
8679         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
8680             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
8681         root_info = blockinfo.get_root_info(
8682             instance, CONF.libvirt.virt_type, image_meta,
8683             root_bdm, disk_bus, cdrom_bus)
8684         return block_device.prepend_dev(root_info['dev'])
8685 
8686     def default_device_names_for_instance(self, instance, root_device_name,
8687                                           *block_device_lists):
8688         block_device_mapping = list(itertools.chain(*block_device_lists))
8689         # NOTE(ndipanov): Null out the device names so that blockinfo code
8690         #                 will assign them
8691         for bdm in block_device_mapping:
8692             if bdm.device_name is not None:
8693                 LOG.warning(
8694                     "Ignoring supplied device name: %(device_name)s. "
8695                     "Libvirt can't honour user-supplied dev names",
8696                     {'device_name': bdm.device_name}, instance=instance)
8697                 bdm.device_name = None
8698         block_device_info = driver.get_block_device_info(instance,
8699                                                          block_device_mapping)
8700 
8701         blockinfo.default_device_names(CONF.libvirt.virt_type,
8702                                        nova_context.get_admin_context(),
8703                                        instance,
8704                                        block_device_info,
8705                                        instance.image_meta)
8706 
8707     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
8708         block_device_info = driver.get_block_device_info(instance, bdms)
8709         instance_info = blockinfo.get_disk_info(
8710                 CONF.libvirt.virt_type, instance,
8711                 instance.image_meta, block_device_info=block_device_info)
8712 
8713         suggested_dev_name = block_device_obj.device_name
8714         if suggested_dev_name is not None:
8715             LOG.warning(
8716                 'Ignoring supplied device name: %(suggested_dev)s',
8717                 {'suggested_dev': suggested_dev_name}, instance=instance)
8718 
8719         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
8720         #                 only when it's actually not set on the bd object
8721         block_device_obj.device_name = None
8722         disk_info = blockinfo.get_info_from_bdm(
8723             instance, CONF.libvirt.virt_type, instance.image_meta,
8724             block_device_obj, mapping=instance_info['mapping'])
8725         return block_device.prepend_dev(disk_info['dev'])
8726 
8727     def is_supported_fs_format(self, fs_type):
8728         return fs_type in [disk_api.FS_FORMAT_EXT2, disk_api.FS_FORMAT_EXT3,
8729                            disk_api.FS_FORMAT_EXT4, disk_api.FS_FORMAT_XFS]
