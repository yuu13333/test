I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # needs:fix_opt_description
2 # needs:check_deprecation_status
3 # needs:check_opt_group_and_type
4 # needs:fix_opt_description_indentation
5 # needs:fix_opt_registration_consistency
6 
7 # Copyright 2016 OpenStack Foundation
8 # All Rights Reserved.
9 #
10 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
11 #    not use this file except in compliance with the License. You may obtain
12 #    a copy of the License at
13 #
14 #         http://www.apache.org/licenses/LICENSE-2.0
15 #
16 #    Unless required by applicable law or agreed to in writing, software
17 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
18 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
19 #    License for the specific language governing permissions and limitations
20 #    under the License.
21 
22 import itertools
23 
24 from oslo_config import cfg
25 
26 from oslo_config import types
27 
28 from nova.conf import paths
29 
30 
31 libvirt_group = cfg.OptGroup("libvirt",
32                              title="Libvirt Options",
33                              help="""
34 Libvirt options allows cloud administrator to configure related
35 libvirt hypervisor driver to be used within an OpenStack deployment.
36 
37 Almost all of the libvirt config options are influence by ``virt_type`` config
38 which describes the virtualization type (or so called domain type) libvirt
39 should use for specific features such as live migration, snapshot.
40 """)
41 
42 libvirt_general_opts = [
43     cfg.StrOpt('rescue_image_id',
44                help="""
45 The ID of the image to boot from to rescue data from a corrupted instance.
46 
47 If the rescue REST API operation doesn't provide an ID of an image to
48 use, the image which is referenced by this ID is used. If this
49 option is not set, the image from the instance is used.
50 
51 Possible values:
52 
53 * An ID of an image or nothing. If it points to an *Amazon Machine
54   Image* (AMI), consider to set the config options ``rescue_kernel_id``
55   and ``rescue_ramdisk_id`` too. If nothing is set, the image of the instance
56   is used.
57 
58 Related options:
59 
60 * ``rescue_kernel_id``: If the chosen rescue image allows the separate
61   definition of its kernel disk, the value of this option is used,
62   if specified. This is the case when *Amazon*'s AMI/AKI/ARI image
63   format is used for the rescue image.
64 * ``rescue_ramdisk_id``: If the chosen rescue image allows the separate
65   definition of its RAM disk, the value of this option is used if,
66   specified. This is the case when *Amazon*'s AMI/AKI/ARI image
67   format is used for the rescue image.
68 """),
69     cfg.StrOpt('rescue_kernel_id',
70                help="""
71 The ID of the kernel (AKI) image to use with the rescue image.
72 
73 If the chosen rescue image allows the separate definition of its kernel
74 disk, the value of this option is used, if specified. This is the case
75 when *Amazon*'s AMI/AKI/ARI image format is used for the rescue image.
76 
77 Possible values:
78 
79 * An ID of an kernel image or nothing. If nothing is specified, the kernel
80   disk from the instance is used if it was launched with one.
81 
82 Related options:
83 
84 * ``rescue_image_id``: If that option points to an image in *Amazon*'s
85   AMI/AKI/ARI image format, it's useful to use ``rescue_kernel_id`` too.
86 """),
87     cfg.StrOpt('rescue_ramdisk_id',
88                help="""
89 The ID of the RAM disk (ARI) image to use with the rescue image.
90 
91 If the chosen rescue image allows the separate definition of its RAM
92 disk, the value of this option is used, if specified. This is the case
93 when *Amazon*'s AMI/AKI/ARI image format is used for the rescue image.
94 
95 Possible values:
96 
97 * An ID of a RAM disk image or nothing. If nothing is specified, the RAM
98   disk from the instance is used if it was launched with one.
99 
100 Related options:
101 
102 * ``rescue_image_id``: If that option points to an image in *Amazon*'s
103   AMI/AKI/ARI image format, it's useful to use ``rescue_ramdisk_id`` too.
104 """),
105     cfg.StrOpt('virt_type',
106                default='kvm',
107                choices=('kvm', 'lxc', 'qemu', 'uml', 'xen', 'parallels'),
108                help="""
109 Describes the virtualization type (or so called domain type) libvirt should
110 use.
111 
112 The choice of this type must match the underlying virtualization strategy
113 you have chosen for this host.
114 
115 Related options:
116 
117 * ``connection_uri``: depends on this
118 * ``disk_prefix``: depends on this
119 * ``cpu_mode``: depends on this
120 * ``cpu_model``: depends on this
121 """),
122     cfg.StrOpt('connection_uri',
123                default='',
124                help="""
125 Overrides the default libvirt URI of the chosen virtualization type.
126 
127 If set, Nova will use this URI to connect to libvirt.
128 
129 Possible values:
130 
131 * An URI like ``qemu:///system`` or ``xen+ssh://oirase/`` for example.
132   This is only necessary if the URI differs to the commonly known URIs
133   for the chosen virtualization type.
134 
135 Related options:
136 
137 * ``virt_type``: Influences what is used as default value here.
138 """),
139     cfg.BoolOpt('inject_password',
140                 default=False,
141                 help="""
142 Allow the injection of an admin password for instance only at ``create`` and
143 ``rebuild`` process.
144 
145 There is no agent needed within the image to do this. If *libguestfs* is
146 available on the host, it will be used. Otherwise *nbd* is used. The file
147 system of the image will be mounted and the admin password, which is provided
148 in the REST API call will be injected as password for the root user. If no
149 root user is available, the instance won't be launched and an error is thrown.
150 Be aware that the injection is *not* possible when the instance gets launched
151 from a volume.
152 
153 *Linux* distribution guest only.
154 
155 Possible values:
156 
157 * True: Allows the injection.
158 * False: Disallows the injection. Any via the REST API provided admin password
159   will be silently ignored.
160 
161 Related options:
162 
163 * ``inject_partition``: That option will decide about the discovery and usage
164   of the file system. It also can disable the injection at all.
165 """),
166     cfg.BoolOpt('inject_key',
167                 default=False,
168                 help="""
169 Allow the injection of an SSH key at boot time.
170 
171 There is no agent needed within the image to do this. If *libguestfs* is
172 available on the host, it will be used. Otherwise *nbd* is used. The file
173 system of the image will be mounted and the SSH key, which is provided
174 in the REST API call will be injected as SSH key for the root user and
175 appended to the ``authorized_keys`` of that user. The SELinux context will
176 be set if necessary. Be aware that the injection is *not* possible when the
177 instance gets launched from a volume.
178 
179 This config option will enable directly modifying the instance disk and does
180 not affect what cloud-init may do using data from config_drive option or the
181 metadata service.
182 
183 *Linux* distribution guest only.
184 
185 Related options:
186 
187 * ``inject_partition``: That option will decide about the discovery and usage
188   of the file system. It also can disable the injection at all.
189 """),
190     cfg.IntOpt('inject_partition',
191                default=-2,
192                min=-2,
193                help="""
194 Determines the way how the file system is chosen to inject data into it.
195 
196 *libguestfs* will be used a first solution to inject data. If that's not
197 available on the host, the image will be locally mounted on the host as a
198 fallback solution. If libguestfs is not able to determine the root partition
199 (because there are more or less than one root partition) or cannot mount the
200 file system it will result in an error and the instance won't be boot.
201 
202 Possible values:
203 
204 * -2 => disable the injection of data.
205 * -1 => find the root partition with the file system to mount with libguestfs
206 *  0 => The image is not partitioned
207 * >0 => The number of the partition to use for the injection
208 
209 *Linux* distribution guest only.
210 
211 Related options:
212 
213 * ``inject_key``: If this option allows the injection of a SSH key it depends
214   on value greater or equal to -1 for ``inject_partition``.
215 * ``inject_password``: If this option allows the injection of an admin password
216   it depends on value greater or equal to -1 for ``inject_partition``.
217 * ``guestfs`` You can enable the debug log level of libguestfs with this
218   config option. A more verbose output will help in debugging issues.
219 * ``virt_type``: If you use ``lxc`` as virt_type it will be treated as a
220   single partition image
221 """),
222     cfg.BoolOpt('use_usb_tablet',
223                 default=True,
224                 deprecated_for_removal=True,
225                 deprecated_reason="This option is being replaced by the "
226                                   "'pointer_model' option.",
227                 deprecated_since='14.0.0',
228                 help="""
229 Enable a mouse cursor within a graphical VNC or SPICE sessions.
230 
231 This will only be taken into account if the VM is fully virtualized and VNC
232 and/or SPICE is enabled. If the node doesn't support a graphical framebuffer,
233 then it is valid to set this to False.
234 
235 Related options:
236 
237 * ``[vnc]enabled``: If VNC is enabled, ``use_usb_tablet`` will have an effect.
238 * ``[spice]enabled`` + ``[spice].agent_enabled``: If SPICE is enabled and the
239   spice agent is disabled, the config value of ``use_usb_tablet`` will have
240   an effect.
241 """),
242     cfg.StrOpt('live_migration_scheme',
243                help="""
244 URI scheme used for live migration.
245 
246 Override the default libvirt live migration scheme (which is dependent on
247 virt_type). If this option is set to None, nova will automatically choose a
248 sensible default based on the hypervisor. It is not recommended that you change
249 this unless you are very sure that hypervisor supports a particular scheme.
250 
251 Related options:
252 
253 * ``virt_type``: This option is meaningful only when ``virt_type`` is set to
254   `kvm` or `qemu`.
255 * ``live_migration_uri``: If ``live_migration_uri`` value is not None, the
256   scheme used for live migration is taken from ``live_migration_uri`` instead.
257 """),
258     cfg.HostAddressOpt('live_migration_inbound_addr',
259                        help="""
260 Target used for live migration traffic.
261 
262 If this option is set to None, the hostname of the migration target compute
263 node will be used.
264 
265 This option is useful in environments where the live-migration traffic can
266 impact the network plane significantly. A separate network for live-migration
267 traffic can then use this config option and avoids the impact on the
268 management network.
269 
270 Related options:
271 
272 * ``live_migration_tunnelled``: The live_migration_inbound_addr value is
273   ignored if tunneling is enabled.
274 """),
275     cfg.StrOpt('live_migration_uri',
276                deprecated_for_removal=True,
277                deprecated_since="15.0.0",
278                deprecated_reason="""
279 live_migration_uri is deprecated for removal in favor of two other options that
280 allow to change live migration scheme and target URI: ``live_migration_scheme``
281 and ``live_migration_inbound_addr`` respectively.
282 """,
283                help="""
284 Live migration target URI to use.
285 
286 Override the default libvirt live migration target URI (which is dependent
287 on virt_type). Any included "%s" is replaced with the migration target
288 hostname.
289 
290 If this option is set to None (which is the default), Nova will automatically
291 generate the `live_migration_uri` value based on only 4 supported `virt_type`
292 in following list:
293 
294 * 'kvm': 'qemu+tcp://%s/system'
295 * 'qemu': 'qemu+tcp://%s/system'
296 * 'xen': 'xenmigr://%s/system'
297 * 'parallels': 'parallels+tcp://%s/system'
298 
299 Related options:
300 
301 * ``live_migration_inbound_addr``: If ``live_migration_inbound_addr`` value
302   is not None and ``live_migration_tunnelled`` is False, the ip/hostname
303   address of target compute node is used instead of ``live_migration_uri`` as
304   the uri for live migration.
305 * ``live_migration_scheme``: If ``live_migration_uri`` is not set, the scheme
306   used for live migration is taken from ``live_migration_scheme`` instead.
307 """),
308     cfg.BoolOpt('live_migration_tunnelled',
309                 default=False,
310                 help="""
311 Enable tunnelled migration.
312 
313 This option enables the tunnelled migration feature, where migration data is
314 transported over the libvirtd connection. If enabled, we use the
315 VIR_MIGRATE_TUNNELLED migration flag, avoiding the need to configure
316 the network to allow direct hypervisor to hypervisor communication.
317 If False, use the native transport. If not set, Nova will choose a
318 sensible default based on, for example the availability of native
319 encryption support in the hypervisor. Enabling this option will definitely
320 impact performance massively.
321 
322 Note that this option is NOT compatible with use of block migration.
323 
324 Related options:
325 
326 * ``live_migration_inbound_addr``: The live_migration_inbound_addr value is
327   ignored if tunneling is enabled.
328 """),
329     cfg.IntOpt('live_migration_bandwidth',
330                default=0,
331                help="""
332 Maximum bandwidth(in MiB/s) to be used during migration.
333 
334 If set to 0, the hypervisor will choose a suitable default. Some hypervisors
335 do not support this feature and will return an error if bandwidth is not 0.
336 Please refer to the libvirt documentation for further details.
337 """),
338     cfg.IntOpt('live_migration_downtime',
339                default=500,
340                min=100,
341                help="""
342 Maximum permitted downtime, in milliseconds, for live migration
343 switchover.
344 
345 Will be rounded up to a minimum of 100ms. You can increase this value
346 if you want to allow live-migrations to complete faster, or avoid
347 live-migration timeout errors by allowing the guest to be paused for
348 longer during the live-migration switch over.
349 
350 Related options:
351 
352 * live_migration_completion_timeout
353 """),
354     cfg.IntOpt('live_migration_downtime_steps',
355                default=10,
356                min=3,
357                help="""
358 Number of incremental steps to reach max downtime value.
359 
360 Will be rounded up to a minimum of 3 steps.
361 """),
362     cfg.IntOpt('live_migration_downtime_delay',
363                default=75,
364                min=3,
365                help="""
366 Time to wait, in seconds, between each step increase of the migration
367 downtime.
368 
369 Minimum delay is 3 seconds. Value is per GiB of guest RAM + disk to be
370 transferred, with lower bound of a minimum of 2 GiB per device.
371 """),
372     cfg.IntOpt('live_migration_completion_timeout',
373                default=800,
374                min=0,
375                mutable=True,
376                help="""
377 Time to wait, in seconds, for migration to successfully complete transferring
378 data before aborting the operation.
379 
380 Value is per GiB of guest RAM + disk to be transferred, with lower bound of
381 a minimum of 2 GiB. Should usually be larger than downtime delay * downtime
382 steps. Set to 0 to disable timeouts.
383 
384 Related options:
385 
386 * live_migration_downtime
387 * live_migration_downtime_steps
388 * live_migration_downtime_delay
389 """),
390     cfg.StrOpt('live_migration_timeout_action',
391                default='abort',
392                choices=('abort', 'force_complete'),
393                mutable=True,
394                help="""
395 This option will be used to determine what action will be taken against a
396 VM after ``live_migration_completion_timeout`` expires. By default, the live
397 migrate operation will be aborted after completion timeout. If it is set to
398 ``force_complete``, the compute service will either pause the VM or trigger
399 post-copy depending on if post copy is enabled and available
400 (``live_migration_permit_post_copy`` is set to True).
401 
402 Related options:
403 
404 * live_migration_completion_timeout
405 * live_migration_permit_post_copy
406 """),
407     cfg.BoolOpt('live_migration_permit_post_copy',
408                 default=False,
409                 help="""
410 This option allows nova to switch an on-going live migration to post-copy
411 mode, i.e., switch the active VM to the one on the destination node before the
412 migration is complete, therefore ensuring an upper bound on the memory that
413 needs to be transferred. Post-copy requires libvirt>=1.3.3 and QEMU>=2.5.0.
414 
415 When permitted, post-copy mode will be automatically activated if
416 we reach the timeout defined by ``live_migration_completion_timeout`` and
417 ``live_migration_timeout_action`` is set to 'force_complete'. Note if you
418 change to no timeout or choose to use 'abort',
419 i.e. ``live_migration_completion_timeout = 0``, then there will be no
420 automatic switch to post-copy.
421 
422 The live-migration force complete API also uses post-copy when permitted. If
423 post-copy mode is not available, force complete falls back to pausing the VM
424 to ensure the live-migration operation will complete.
425 
426 When using post-copy mode, if the source and destination hosts lose network
427 connectivity, the VM being live-migrated will need to be rebooted. For more
428 details, please see the Administration guide.
429 
430 Related options:
431 
432 * live_migration_permit_auto_converge
433 * live_migration_timeout_action
434 """),
435     cfg.BoolOpt('live_migration_permit_auto_converge',
436                 default=False,
437                 help="""
438 This option allows nova to start live migration with auto converge on.
439 
440 Auto converge throttles down CPU if a progress of on-going live migration
441 is slow. Auto converge will only be used if this flag is set to True and
442 post copy is not permitted or post copy is unavailable due to the version
443 of libvirt and QEMU in use.
444 
445 Related options:
446 
447     * live_migration_permit_post_copy
448 """),
449     cfg.StrOpt('snapshot_image_format',
450         choices=[
451             ('raw', 'RAW disk format'),
452             ('qcow2', 'KVM default disk format'),
453             ('vmdk', 'VMWare default disk format'),
454             ('vdi', 'VirtualBox default disk format'),
455         ],
456         help="""
457 Determine the snapshot image format when sending to the image service.
458 
459 If set, this decides what format is used when sending the snapshot to the
460 image service. If not set, defaults to same type as source image.
461 """),
462     cfg.BoolOpt('live_migration_with_native_tls',
463                 default=False,
464                 help="""
465 Use QEMU-native TLS encryption when live migrating.
466 
467 This option will allow both migration stream (guest RAM plus device
468 state) *and* disk stream to be transported over native TLS, i.e. TLS
469 support built into QEMU.
470 
471 Prerequisite: TLS environment is configured correctly on all relevant
472 Compute nodes.  This means, Certificate Authority (CA), server, client
473 certificates, their corresponding keys, and their file permisssions are
474 in place, and are validated.
475 
476 Notes:
477 
478 * To have encryption for migration stream and disk stream (also called:
479   "block migration"), ``live_migration_with_native_tls`` is the
480   preferred config attribute instead of ``live_migration_tunnelled``.
481 
482 * The ``live_migration_tunnelled`` will be deprecated in the long-term
483   for two main reasons: (a) it incurs a huge performance penalty; and
484   (b) it is not compatible with block migration.  Therefore, if your
485   compute nodes have at least libvirt 4.4.0 and QEMU 2.11.0, it is
486   strongly recommended to use ``live_migration_with_native_tls``.
487 
488 * The ``live_migration_tunnelled`` and
489   ``live_migration_with_native_tls`` should not be used at the same
490   time.
491 
492 * Unlike ``live_migration_tunnelled``, the
493   ``live_migration_with_native_tls`` *is* compatible with block
494   migration.  That is, with this option, NBD stream, over which disks
495   are migrated to a target host, will be encrypted.
496 
497 Related options:
498 
499 ``live_migration_tunnelled``: This transports migration stream (but not
500 disk stream) over libvirtd.
501 
502 """),
503     cfg.StrOpt('disk_prefix',
504                help="""
505 Override the default disk prefix for the devices attached to an instance.
506 
507 If set, this is used to identify a free disk device name for a bus.
508 
509 Possible values:
510 
511 * Any prefix which will result in a valid disk device name like 'sda' or 'hda'
512   for example. This is only necessary if the device names differ to the
513   commonly known device name prefixes for a virtualization type such as: sd,
514   xvd, uvd, vd.
515 
516 Related options:
517 
518 * ``virt_type``: Influences which device type is used, which determines
519   the default disk prefix.
520 """),
521     cfg.IntOpt('wait_soft_reboot_seconds',
522                default=120,
523                help='Number of seconds to wait for instance to shut down after'
524                     ' soft reboot request is made. We fall back to hard reboot'
525                     ' if instance does not shutdown within this window.'),
526     cfg.StrOpt('cpu_mode',
527         choices=[
528             ('host-model', 'Clone the host CPU feature flags'),
529             ('host-passthrough', 'Use the host CPU model exactly'),
530             ('custom', 'Use the CPU model in ``[libvirt]cpu_model``'),
531             ('none', "Don't set a specific CPU model. For instances with "
532              "``[libvirt] virt_type`` as KVM/QEMU, the default CPU model from "
533              "QEMU will be used, which provides a basic set of CPU features "
534              "that are compatible with most hosts"),
535         ],
536         help="""
537 Is used to set the CPU mode an instance should have.
538 
539 If ``virt_type="kvm|qemu"``, it will default to ``host-model``, otherwise it
540 will default to ``none``.
541 
542 Related options:
543 
544 * ``cpu_model``: This should be set ONLY when ``cpu_mode`` is set to
545   ``custom``. Otherwise, it would result in an error and the instance launch
546   will fail.
547 """),
548     cfg.StrOpt('cpu_model',
549                help="""
550 Set the name of the libvirt CPU model the instance should use.
551 
552 Possible values:
553 
554 * The named CPU models listed in ``/usr/share/libvirt/cpu_map.xml`` for
555   libvirt prior to version 4.7.0 or ``/usr/share/libvirt/cpu_map/*.xml``
556   for version 4.7.0 and higher.
557 
558 Related options:
559 
560 * ``cpu_mode``: This should be set to ``custom`` ONLY when you want to
561   configure (via ``cpu_model``) a specific named CPU model.  Otherwise, it
562   would result in an error and the instance launch will fail.
563 * ``virt_type``: Only the virtualization types ``kvm`` and ``qemu`` use this.
564 """),
565     cfg.ListOpt(
566         'cpu_model_extra_flags',
567         item_type=types.String(
568             ignore_case=True,
569         ),
570         default=[],
571         help="""
572 This allows specifying granular CPU feature flags when configuring CPU
573 models.  For example, to explicitly specify the ``pcid``
574 (Process-Context ID, an Intel processor feature -- which is now required
575 to address the guest performance degradation as a result of applying the
576 "Meltdown" CVE fixes to certain Intel CPU models) flag to the
577 "IvyBridge" virtual CPU model::
578 
579     [libvirt]
580     cpu_mode = custom
581     cpu_model = IvyBridge
582     cpu_model_extra_flags = pcid
583 
584 To specify multiple CPU flags (e.g. the Intel ``VMX`` to expose the
585 virtualization extensions to the guest, or ``pdpe1gb`` to configure 1GB
586 huge pages for CPU models that do not provide it)::
587 
588     [libvirt]
589     cpu_mode = custom
590     cpu_model = Haswell-noTSX-IBRS
591     cpu_model_extra_flags = PCID, VMX, pdpe1gb
592 
593 As it can be noticed from above, the ``cpu_model_extra_flags`` config
594 attribute is case insensitive.  And specifying extra flags is valid in
595 combination with all the three possible values for ``cpu_mode``:
596 ``custom`` (this also requires an explicit ``cpu_model`` to be
597 specified), ``host-model``, or ``host-passthrough``.  A valid example
598 for allowing extra CPU flags even for ``host-passthrough`` mode is that
599 sometimes QEMU may disable certain CPU features -- e.g. Intel's
600 "invtsc", Invariable Time Stamp Counter, CPU flag.  And if you need to
601 expose that CPU flag to the Nova instance, the you need to explicitly
602 ask for it.
603 
604 The possible values for ``cpu_model_extra_flags`` depends on the CPU
605 model in use. Refer to ``/usr/share/libvirt/cpu_map.xml`` for libvirt
606 prior to version 4.7.0 or ``/usr/share/libvirt/cpu_map/*.xml`` thereafter
607 for possible CPU feature flags for a given CPU model.
608 
609 Note that when using this config attribute to set the 'PCID' CPU flag
610 with the ``custom`` CPU mode, not all virtual (i.e. libvirt / QEMU) CPU
611 models need it:
612 
613 * The only virtual CPU models that include the 'PCID' capability are
614   Intel "Haswell", "Broadwell", and "Skylake" variants.
615 
616 * The libvirt / QEMU CPU models "Nehalem", "Westmere", "SandyBridge",
617   and "IvyBridge" will _not_ expose the 'PCID' capability by default,
618   even if the host CPUs by the same name include it.  I.e.  'PCID' needs
619   to be explicitly specified when using the said virtual CPU models.
620 
621 The libvirt driver's default CPU mode, ``host-model``, will do the right
622 thing with respect to handling 'PCID' CPU flag for the guest --
623 *assuming* you are running updated processor microcode, host and guest
624 kernel, libvirt, and QEMU.  The other mode, ``host-passthrough``, checks
625 if 'PCID' is available in the hardware, and if so directly passes it
626 through to the Nova guests.  Thus, in context of 'PCID', with either of
627 these CPU modes (``host-model`` or ``host-passthrough``), there is no
628 need to use the ``cpu_model_extra_flags``.
629 
630 Related options:
631 
632 * cpu_mode
633 * cpu_model
634 """),
635     cfg.StrOpt('snapshots_directory',
636                default='$instances_path/snapshots',
637                help='Location where libvirt driver will store snapshots '
638                     'before uploading them to image service'),
639     cfg.StrOpt('xen_hvmloader_path',
640                default='/usr/lib/xen/boot/hvmloader',
641                help='Location where the Xen hvmloader is kept'),
642     cfg.ListOpt('disk_cachemodes',
643                 default=[],
644                 help="""
645 Specific cache modes to use for different disk types.
646 
647 For example: file=directsync,block=none,network=writeback
648 
649 For local or direct-attached storage, it is recommended that you use
650 writethrough (default) mode, as it ensures data integrity and has acceptable
651 I/O performance for applications running in the guest, especially for read
652 operations. However, caching mode none is recommended for remote NFS storage,
653 because direct I/O operations (O_DIRECT) perform better than synchronous I/O
654 operations (with O_SYNC). Caching mode none effectively turns all guest I/O
655 operations into direct I/O operations on the host, which is the NFS client in
656 this environment.
657 
658 Possible cache modes:
659 
660 * default: "It Depends" -- For Nova-managed disks, ``none``, if the host
661   file system is capable of Linux's 'O_DIRECT' semantics; otherwise
662   ``writeback``.  For volume drivers, the default is driver-dependent:
663   ``none`` for everything except for SMBFS and Virtuzzo (which use
664   ``writeback``).
665 * none: With caching mode set to none, the host page cache is disabled, but
666   the disk write cache is enabled for the guest. In this mode, the write
667   performance in the guest is optimal because write operations bypass the host
668   page cache and go directly to the disk write cache. If the disk write cache
669   is battery-backed, or if the applications or storage stack in the guest
670   transfer data properly (either through fsync operations or file system
671   barriers), then data integrity can be ensured. However, because the host
672   page cache is disabled, the read performance in the guest would not be as
673   good as in the modes where the host page cache is enabled, such as
674   writethrough mode. Shareable disk devices, like for a multi-attachable block
675   storage volume, will have their cache mode set to 'none' regardless of
676   configuration.
677 * writethrough: With caching set to writethrough mode, the host page cache is
678   enabled, but the disk write cache is disabled for the guest. Consequently,
679   this caching mode ensures data integrity even if the applications and storage
680   stack in the guest do not transfer data to permanent storage properly (either
681   through fsync operations or file system barriers). Because the host page
682   cache is enabled in this mode, the read performance for applications running
683   in the guest is generally better. However, the write performance might be
684   reduced because the disk write cache is disabled.
685 * writeback: With caching set to writeback mode, both the host page
686   cache and the disk write cache are enabled for the guest. Because of
687   this, the I/O performance for applications running in the guest is
688   good, but the data is not protected in a power failure. As a result,
689   this caching mode is recommended only for temporary data where
690   potential data loss is not a concern.
691   NOTE: Certain backend disk mechanisms may provide safe
692   writeback cache semantics. Specifically those that bypass the host
693   page cache, such as QEMU's integrated RBD driver. Ceph documentation
694   recommends setting this to writeback for maximum performance while
695   maintaining data safety.
696 * directsync: Like "writethrough", but it bypasses the host page cache.
697 * unsafe: Caching mode of unsafe ignores cache transfer operations
698   completely. As its name implies, this caching mode should be used only for
699   temporary data where data loss is not a concern. This mode can be useful for
700   speeding up guest installations, but you should switch to another caching
701   mode in production environments.
702 """),
703     cfg.StrOpt('rng_dev_path',
704                default='/dev/urandom',
705                help="""
706 The path to an RNG (Random Number Generator) device that will be used as
707 the source of entropy on the host.  Since libvirt 1.3.4, any path (that
708 returns random numbers when read) is accepted.  The recommended source
709 of entropy is ``/dev/urandom`` -- it is non-blocking, therefore
710 relatively fast; and avoids the limitations of ``/dev/random``, which is
711 a legacy interface.  For more details (and comparision between different
712 RNG sources), refer to the "Usage" section in the Linux kernel API
713 documentation for ``[u]random``:
714 http://man7.org/linux/man-pages/man4/urandom.4.html and
715 http://man7.org/linux/man-pages/man7/random.7.html.
716 """),
717     cfg.ListOpt('hw_machine_type',
718                 help='For qemu or KVM guests, set this option to specify '
719                      'a default machine type per host architecture. '
720                      'You can find a list of supported machine types '
721                      'in your environment by checking the output of '
722                      'the "virsh capabilities" command. The format of the '
723                      'value for this config option is host-arch=machine-type. '
724                      'For example: x86_64=machinetype1,armv7l=machinetype2'),
725     cfg.StrOpt('sysinfo_serial',
726                default='unique',
727                choices=(
728                    ('none', 'A serial number entry is not added to the guest '
729                             'domain xml.'),
730                    ('os', 'A UUID serial number is generated from the host '
731                           '``/etc/machine-id`` file.'),
732                    ('hardware', 'A UUID for the host hardware as reported by '
733                                 'libvirt. This is typically from the host '
734                                 'SMBIOS data, unless it has been overridden '
735                                 'in ``libvirtd.conf``.'),
736                    ('auto', 'Uses the "os" source if possible, else '
737                             '"hardware".'),
738                    ('unique', 'Uses instance UUID as the serial number.'),
739                ),
740                help="""
741 The data source used to the populate the host "serial" UUID exposed to guest
742 in the virtual BIOS. All choices except ``unique`` will change the serial when
743 migrating the instance to another host. Changing the choice of this option will
744 also affect existing instances on this host once they are stopped and started
745 again. It is recommended to use the default choice (``unique``) since that will
746 not change when an instance is migrated. However, if you have a need for
747 per-host serials in addition to per-instance serial numbers, then consider
748 restricting flavors via host aggregates.
749 """
750                ),
751     cfg.IntOpt('mem_stats_period_seconds',
752                default=10,
753                help='A number of seconds to memory usage statistics period. '
754                     'Zero or negative value mean to disable memory usage '
755                     'statistics.'),
756     cfg.ListOpt('uid_maps',
757                 default=[],
758                 help='List of uid targets and ranges.'
759                      'Syntax is guest-uid:host-uid:count. '
760                      'Maximum of 5 allowed.'),
761     cfg.ListOpt('gid_maps',
762                 default=[],
763                 help='List of guid targets and ranges.'
764                      'Syntax is guest-gid:host-gid:count. '
765                      'Maximum of 5 allowed.'),
766     cfg.IntOpt('realtime_scheduler_priority',
767                default=1,
768                help='In a realtime host context vCPUs for guest will run in '
769                     'that scheduling priority. Priority depends on the host '
770                     'kernel (usually 1-99)'),
771     cfg.ListOpt('enabled_perf_events',
772                default=[],
773                help= """
774 This will allow you to specify a list of events to monitor low-level
775 performance of guests, and collect related statsitics via the libvirt
776 driver, which in turn uses the Linux kernel's `perf` infrastructure.
777 With this config attribute set, Nova will generate libvirt guest XML to
778 monitor the specified events.  For more information, refer to the
779 "Performance monitoring events" section here:
780 https://libvirt.org/formatdomain.html#elementsPerf.  And here:
781 https://libvirt.org/html/libvirt-libvirt-domain.html -- look for
782 ``VIR_PERF_PARAM_*``
783 
784 For example, to monitor the count of CPU cycles (total/elapsed) and the
785 count of cache misses, enable them as follows::
786 
787     [libvirt]
788     enabled_perf_events = cpu_clock, cache_misses
789 
790 Possible values: A string list.  The list of supported events can be
791 found here: https://libvirt.org/formatdomain.html#elementsPerf.
792 
793 Note that support for Intel CMT events (`cmt`, `mbmbt`, `mbml`) is
794 deprecated, and will be removed in the "Stein" release.  That's because
795 the upstream Linux kernel (from 4.14 onwards) has deleted support for
796 Intel CMT, because it is broken by design.
797 """),
798     cfg.IntOpt('num_pcie_ports',
799                default=0,
800                min=0,
801                max=28,
802                help= """
803 The number of PCIe ports an instance will get.
804 
805 Libvirt allows a custom number of PCIe ports (pcie-root-port controllers) a
806 target instance will get. Some will be used by default, rest will be available
807 for hotplug use.
808 
809 By default we have just 1-2 free ports which limits hotplug.
810 
811 More info: https://github.com/qemu/qemu/blob/master/docs/pcie.txt
812 
813 Due to QEMU limitations for aarch64/virt maximum value is set to '28'.
814 
815 Default value '0' moves calculating amount of ports to libvirt.
816 """),
817     cfg.IntOpt('file_backed_memory',
818                default=0,
819                min=0,
820                help="""
821 Available capacity in MiB for file-backed memory.
822 
823 Set to 0 to disable file-backed memory.
824 
825 When enabled, instances will create memory files in the directory specified
826 in ``/etc/libvirt/qemu.conf``'s ``memory_backing_dir`` option. The default
827 location is ``/var/lib/libvirt/qemu/ram``.
828 
829 When enabled, the value defined for this option is reported as the node memory
830 capacity. Compute node system memory will be used as a cache for file-backed
831 memory, via the kernel's pagecache mechanism.
832 
833 .. note::
834    This feature is not compatible with hugepages.
835 
836 .. note::
837    This feature is not compatible with memory overcommit.
838 
839 Related options:
840 
841 * ``virt_type`` must be set to ``kvm`` or ``qemu``.
842 * ``ram_allocation_ratio`` must be set to 1.0.
843 """),
844     cfg.StrOpt('swtpm_user',
845                default='tss',
846                choices=(
847                    ('tss', 'swtpm runs as tss user '),
848                    ('root', 'swtpm runs as root user'),
849                ),
850                help="""
851 User that swtpm binary runs as.
852 
853 When using emulated TPM, the ``swtpm`` binary will run to emulate a TPM
854 device.  This binary runs as either ``root`` or ``tss``depending on libvirt
855 configuration, with ``tss`` being the default.
856 
857 In order to support cold migration and resize, needs to know what user
858 the swtpm binary is running as in order to ensure that files get the proper
859 ownership after being moved between nodes.
860 
861 Related options:
862 
863 * ``swtpm_group`` must also be set.
864 """),
865     cfg.StrOpt('swtpm_group',
866                default='tss',
867                choices=(
868                    ('tss', 'swtpm runs as tss group '),
869                    ('root', 'swtpm runs as root group'),
870                ),
871                help="""
872 Group that swtpm binary runs as.
873 
874 When using emulated TPM, the ``swtpm`` binary will run to emulate a TPM
875 device.  This binary runs as either ``root`` or ``tss``depending on libvirt
876 configuration, with ``tss`` being the default.
877 
878 In order to support cold migration and resize, needs to know what group
879 the swtpm binary is running as in order to ensure that files get the proper
880 ownership after being moved between nodes.
881 
882 Related options:
883 
884 * ``swtpm_user`` must also be set.
885 """),
886 ]
887 
888 libvirt_imagebackend_opts = [
889     cfg.StrOpt('images_type',
890                default='default',
891                choices=('raw', 'flat', 'qcow2', 'lvm', 'rbd', 'ploop',
892                         'default'),
893                help="""
894 VM Images format.
895 
896 If default is specified, then use_cow_images flag is used instead of this
897 one.
898 
899 Related options:
900 
901 * compute.use_cow_images
902 * images_volume_group
903 * [workarounds]/ensure_libvirt_rbd_instance_dir_cleanup
904 * compute.force_raw_images
905 """),
906     cfg.StrOpt('images_volume_group',
907                help="""
908 LVM Volume Group that is used for VM images, when you specify images_type=lvm
909 
910 Related options:
911 
912 * images_type
913 """),
914     cfg.BoolOpt('sparse_logical_volumes',
915                 default=False,
916                 deprecated_for_removal=True,
917                 deprecated_since='18.0.0',
918                 deprecated_reason="""
919 Sparse logical volumes is a feature that is not tested hence not supported.
920 LVM logical volumes are preallocated by default. If you want thin provisioning,
921 use Cinder thin-provisioned volumes.
922 """,
923                 help="""
924 Create sparse logical volumes (with virtualsize) if this flag is set to True.
925 """),
926     cfg.StrOpt('images_rbd_pool',
927                default='rbd',
928                help='The RADOS pool in which rbd volumes are stored'),
929     cfg.StrOpt('images_rbd_ceph_conf',
930                default='',  # default determined by librados
931                help='Path to the ceph configuration file to use'),
932     cfg.StrOpt('hw_disk_discard',
933                choices=('ignore', 'unmap'),
934                help="""
935 Discard option for nova managed disks.
936 
937 Requires:
938 
939 * Libvirt >= 1.0.6
940 * Qemu >= 1.5 (raw format)
941 * Qemu >= 1.6 (qcow2 format)
942 """),
943 ]
944 
945 libvirt_imagecache_opts = [
946     cfg.IntOpt('remove_unused_resized_minimum_age_seconds',
947                default=3600,
948                help='Unused resized base images younger than this will not be '
949                     'removed'),
950 ]
951 
952 libvirt_lvm_opts = [
953     cfg.StrOpt('volume_clear',
954         default='zero',
955         choices=[
956             ('zero', 'Overwrite volumes with zeroes'),
957             ('shred', 'Overwrite volumes repeatedly'),
958             ('none', 'Do not wipe deleted volumes'),
959         ],
960         help="""
961 Method used to wipe ephemeral disks when they are deleted. Only takes effect
962 if LVM is set as backing storage.
963 
964 Related options:
965 
966 * images_type - must be set to ``lvm``
967 * volume_clear_size
968 """),
969     cfg.IntOpt('volume_clear_size',
970                default=0,
971                min=0,
972                help="""
973 Size of area in MiB, counting from the beginning of the allocated volume,
974 that will be cleared using method set in ``volume_clear`` option.
975 
976 Possible values:
977 
978 * 0 - clear whole volume
979 * >0 - clear specified amount of MiB
980 
981 Related options:
982 
983 * images_type - must be set to ``lvm``
984 * volume_clear - must be set and the value must be different than ``none``
985   for this option to have any impact
986 """),
987 ]
988 
989 libvirt_utils_opts = [
990     cfg.BoolOpt('snapshot_compression',
991                 default=False,
992                 help="""
993 Enable snapshot compression for ``qcow2`` images.
994 
995 Note: you can set ``snapshot_image_format`` to ``qcow2`` to force all
996 snapshots to be in ``qcow2`` format, independently from their original image
997 type.
998 
999 Related options:
1000 
1001 * snapshot_image_format
1002 """),
1003 ]
1004 
1005 libvirt_vif_opts = [
1006     cfg.BoolOpt('use_virtio_for_bridges',
1007                 default=True,
1008                 help='Use virtio for bridge interfaces with KVM/QEMU'),
1009 ]
1010 
1011 libvirt_volume_opts = [
1012     cfg.BoolOpt('volume_use_multipath',
1013                 default=False,
1014                 deprecated_name='iscsi_use_multipath',
1015                 help="""
1016 Use multipath connection of the iSCSI or FC volume
1017 
1018 Volumes can be connected in the LibVirt as multipath devices. This will
1019 provide high availability and fault tolerance.
1020 """),
1021     cfg.IntOpt('num_volume_scan_tries',
1022                deprecated_name='num_iscsi_scan_tries',
1023                default=5,
1024                help="""
1025 Number of times to scan given storage protocol to find volume.
1026 """),
1027 ]
1028 
1029 libvirt_volume_aoe_opts = [
1030     cfg.IntOpt('num_aoe_discover_tries',
1031                default=3,
1032                help="""
1033 Number of times to rediscover AoE target to find volume.
1034 
1035 Nova provides support for block storage attaching to hosts via AOE (ATA over
1036 Ethernet). This option allows the user to specify the maximum number of retry
1037 attempts that can be made to discover the AoE device.
1038 """)
1039 ]
1040 
1041 libvirt_volume_iscsi_opts = [
1042     cfg.StrOpt('iscsi_iface',
1043                deprecated_name='iscsi_transport',
1044                help="""
1045 The iSCSI transport iface to use to connect to target in case offload support
1046 is desired.
1047 
1048 Default format is of the form <transport_name>.<hwaddress> where
1049 <transport_name> is one of (be2iscsi, bnx2i, cxgb3i, cxgb4i, qla4xxx, ocs) and
1050 <hwaddress> is the MAC address of the interface and can be generated via the
1051 iscsiadm -m iface command. Do not confuse the iscsi_iface parameter to be
1052 provided here with the actual transport name.
1053 """)
1054 # iser is also supported, but use LibvirtISERVolumeDriver
1055 # instead
1056 ]
1057 
1058 libvirt_volume_iser_opts = [
1059     cfg.IntOpt('num_iser_scan_tries',
1060                default=5,
1061                help="""
1062 Number of times to scan iSER target to find volume.
1063 
1064 iSER is a server network protocol that extends iSCSI protocol to use Remote
1065 Direct Memory Access (RDMA). This option allows the user to specify the maximum
1066 number of scan attempts that can be made to find iSER volume.
1067 """),
1068     cfg.BoolOpt('iser_use_multipath',
1069                 default=False,
1070                 help="""
1071 Use multipath connection of the iSER volume.
1072 
1073 iSER volumes can be connected as multipath devices. This will provide high
1074 availability and fault tolerance.
1075 """)
1076 ]
1077 
1078 libvirt_volume_net_opts = [
1079     cfg.StrOpt('rbd_user',
1080                help="""
1081 The RADOS client name for accessing rbd(RADOS Block Devices) volumes.
1082 
1083 Libvirt will refer to this user when connecting and authenticating with
1084 the Ceph RBD server.
1085 """),
1086     cfg.StrOpt('rbd_secret_uuid',
1087                help="""
1088 The libvirt UUID of the secret for the rbd_user volumes.
1089 """),
1090     cfg.IntOpt('rbd_connect_timeout',
1091                default=5,
1092                help="""
1093 The RADOS client timeout in seconds when initially connecting to the cluster.
1094 """),
1095 ]
1096 
1097 libvirt_volume_nfs_opts = [
1098     cfg.StrOpt('nfs_mount_point_base',
1099                default=paths.state_path_def('mnt'),
1100                help="""
1101 Directory where the NFS volume is mounted on the compute node.
1102 The default is 'mnt' directory of the location where nova's Python module
1103 is installed.
1104 
1105 NFS provides shared storage for the OpenStack Block Storage service.
1106 
1107 Possible values:
1108 
1109 * A string representing absolute path of mount point.
1110 """),
1111     cfg.StrOpt('nfs_mount_options',
1112                help="""
1113 Mount options passed to the NFS client. See section of the nfs man page
1114 for details.
1115 
1116 Mount options controls the way the filesystem is mounted and how the
1117 NFS client behaves when accessing files on this mount point.
1118 
1119 Possible values:
1120 
1121 * Any string representing mount options separated by commas.
1122 * Example string: vers=3,lookupcache=pos
1123 """),
1124 ]
1125 
1126 libvirt_volume_quobyte_opts = [
1127     cfg.StrOpt('quobyte_mount_point_base',
1128                default=paths.state_path_def('mnt'),
1129                help="""
1130 Directory where the Quobyte volume is mounted on the compute node.
1131 
1132 Nova supports Quobyte volume driver that enables storing Block Storage
1133 service volumes on a Quobyte storage back end. This Option specifies the
1134 path of the directory where Quobyte volume is mounted.
1135 
1136 Possible values:
1137 
1138 * A string representing absolute path of mount point.
1139 """),
1140     cfg.StrOpt('quobyte_client_cfg',
1141                help='Path to a Quobyte Client configuration file.'),
1142 ]
1143 
1144 libvirt_volume_smbfs_opts = [
1145     cfg.StrOpt('smbfs_mount_point_base',
1146                default=paths.state_path_def('mnt'),
1147                help="""
1148 Directory where the SMBFS shares are mounted on the compute node.
1149 """),
1150     cfg.StrOpt('smbfs_mount_options',
1151                default='',
1152                help="""
1153 Mount options passed to the SMBFS client.
1154 
1155 Provide SMBFS options as a single string containing all parameters.
1156 See mount.cifs man page for details. Note that the libvirt-qemu ``uid``
1157 and ``gid`` must be specified.
1158 """),
1159 ]
1160 
1161 libvirt_remotefs_opts = [
1162     cfg.StrOpt('remote_filesystem_transport',
1163                default='ssh',
1164                choices=('ssh', 'rsync'),
1165                help="""
1166 libvirt's transport method for remote file operations.
1167 
1168 Because libvirt cannot use RPC to copy files over network to/from other
1169 compute nodes, other method must be used for:
1170 
1171 * creating directory on remote host
1172 * creating file on remote host
1173 * removing file from remote host
1174 * copying file to remote host
1175 """)
1176 ]
1177 
1178 libvirt_volume_vzstorage_opts = [
1179     cfg.StrOpt('vzstorage_mount_point_base',
1180                default=paths.state_path_def('mnt'),
1181                help="""
1182 Directory where the Virtuozzo Storage clusters are mounted on the compute
1183 node.
1184 
1185 This option defines non-standard mountpoint for Vzstorage cluster.
1186 
1187 Related options:
1188 
1189 * vzstorage_mount_* group of parameters
1190 """
1191               ),
1192     cfg.StrOpt('vzstorage_mount_user',
1193                default='stack',
1194                help="""
1195 Mount owner user name.
1196 
1197 This option defines the owner user of Vzstorage cluster mountpoint.
1198 
1199 Related options:
1200 
1201 * vzstorage_mount_* group of parameters
1202 """
1203               ),
1204     cfg.StrOpt('vzstorage_mount_group',
1205                default='qemu',
1206                help="""
1207 Mount owner group name.
1208 
1209 This option defines the owner group of Vzstorage cluster mountpoint.
1210 
1211 Related options:
1212 
1213 * vzstorage_mount_* group of parameters
1214 """
1215               ),
1216     cfg.StrOpt('vzstorage_mount_perms',
1217                default='0770',
1218                help="""
1219 Mount access mode.
1220 
1221 This option defines the access bits of Vzstorage cluster mountpoint,
1222 in the format similar to one of chmod(1) utility, like this: 0770.
1223 It consists of one to four digits ranging from 0 to 7, with missing
1224 lead digits assumed to be 0's.
1225 
1226 Related options:
1227 
1228 * vzstorage_mount_* group of parameters
1229 """
1230               ),
1231     cfg.StrOpt('vzstorage_log_path',
1232                default='/var/log/vstorage/%(cluster_name)s/nova.log.gz',
1233                help="""
1234 Path to vzstorage client log.
1235 
1236 This option defines the log of cluster operations,
1237 it should include "%(cluster_name)s" template to separate
1238 logs from multiple shares.
1239 
1240 Related options:
1241 
1242 * vzstorage_mount_opts may include more detailed logging options.
1243 """
1244               ),
1245     cfg.StrOpt('vzstorage_cache_path',
1246                default=None,
1247                help="""
1248 Path to the SSD cache file.
1249 
1250 You can attach an SSD drive to a client and configure the drive to store
1251 a local cache of frequently accessed data. By having a local cache on a
1252 client's SSD drive, you can increase the overall cluster performance by
1253 up to 10 and more times.
1254 WARNING! There is a lot of SSD models which are not server grade and
1255 may loose arbitrary set of data changes on power loss.
1256 Such SSDs should not be used in Vstorage and are dangerous as may lead
1257 to data corruptions and inconsistencies. Please consult with the manual
1258 on which SSD models are known to be safe or verify it using
1259 vstorage-hwflush-check(1) utility.
1260 
1261 This option defines the path which should include "%(cluster_name)s"
1262 template to separate caches from multiple shares.
1263 
1264 Related options:
1265 
1266 * vzstorage_mount_opts may include more detailed cache options.
1267 """
1268               ),
1269     cfg.ListOpt('vzstorage_mount_opts',
1270                 default=[],
1271                help="""
1272 Extra mount options for pstorage-mount
1273 
1274 For full description of them, see
1275 https://static.openvz.org/vz-man/man1/pstorage-mount.1.gz.html
1276 Format is a python string representation of arguments list, like:
1277 "[\'-v\', \'-R\', \'500\']"
1278 Shouldn\'t include -c, -l, -C, -u, -g and -m as those have
1279 explicit vzstorage_* options.
1280 
1281 Related options:
1282 
1283 * All other vzstorage_* options
1284 """
1285 ),
1286 ]
1287 
1288 
1289 # The queue size requires value to be a power of two from [256, 1024]
1290 # range.
1291 # https://libvirt.org/formatdomain.html#elementsDriverBackendOptions
1292 QueueSizeType = types.Integer(choices=(256, 512, 1024))
1293 
1294 libvirt_virtio_queue_sizes = [
1295     cfg.Opt('rx_queue_size',
1296             type=QueueSizeType,
1297             help="""
1298 Configure virtio rx queue size.
1299 
1300 This option is only usable for virtio-net device with vhost and
1301 vhost-user backend. Available only with QEMU/KVM. Requires libvirt
1302 v2.3 QEMU v2.7."""),
1303     cfg.Opt('tx_queue_size',
1304             type=QueueSizeType,
1305             help="""
1306 Configure virtio tx queue size.
1307 
1308 This option is only usable for virtio-net device with vhost-user
1309 backend. Available only with QEMU/KVM. Requires libvirt v3.7 QEMU
1310 v2.10."""),
1311 
1312 ]
1313 
1314 
1315 libvirt_volume_nvmeof_opts = [
1316     cfg.IntOpt('num_nvme_discover_tries',
1317                default=5,
1318                help="""
1319 Number of times to rediscover NVMe target to find volume
1320 
1321 Nova provides support for block storage attaching to hosts via NVMe
1322 (Non-Volatile Memory Express). This option allows the user to specify the
1323 maximum number of retry attempts that can be made to discover the NVMe device.
1324 """),
1325 ]
1326 
1327 ALL_OPTS = list(itertools.chain(
1328     libvirt_general_opts,
1329     libvirt_imagebackend_opts,
1330     libvirt_imagecache_opts,
1331     libvirt_lvm_opts,
1332     libvirt_utils_opts,
1333     libvirt_vif_opts,
1334     libvirt_volume_opts,
1335     libvirt_volume_aoe_opts,
1336     libvirt_volume_iscsi_opts,
1337     libvirt_volume_iser_opts,
1338     libvirt_volume_net_opts,
1339     libvirt_volume_nfs_opts,
1340     libvirt_volume_quobyte_opts,
1341     libvirt_volume_smbfs_opts,
1342     libvirt_remotefs_opts,
1343     libvirt_volume_vzstorage_opts,
1344     libvirt_virtio_queue_sizes,
1345     libvirt_volume_nvmeof_opts,
1346 ))
1347 
1348 
1349 def register_opts(conf):
1350     conf.register_group(libvirt_group)
1351     conf.register_opts(ALL_OPTS, group=libvirt_group)
1352 
1353 
1354 def list_opts():
1355     return {libvirt_group: ALL_OPTS}
