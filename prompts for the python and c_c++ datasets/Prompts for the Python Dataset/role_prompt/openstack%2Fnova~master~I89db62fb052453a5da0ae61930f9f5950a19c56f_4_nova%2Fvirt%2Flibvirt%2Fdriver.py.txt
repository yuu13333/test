I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import binascii
29 import collections
30 from collections import deque
31 import contextlib
32 import copy
33 import errno
34 import functools
35 import glob
36 import itertools
37 import operator
38 import os
39 import pwd
40 import random
41 import shutil
42 import tempfile
43 import time
44 import uuid
45 
46 from castellan import key_manager
47 from copy import deepcopy
48 import eventlet
49 from eventlet import greenthread
50 from eventlet import tpool
51 from lxml import etree
52 from os_brick import encryptors
53 from os_brick.encryptors import luks as luks_encryptor
54 from os_brick import exception as brick_exception
55 from os_brick.initiator import connector
56 import os_resource_classes as orc
57 from oslo_concurrency import processutils
58 from oslo_log import log as logging
59 from oslo_serialization import base64
60 from oslo_serialization import jsonutils
61 from oslo_service import loopingcall
62 from oslo_utils import encodeutils
63 from oslo_utils import excutils
64 from oslo_utils import fileutils
65 from oslo_utils import importutils
66 from oslo_utils import netutils as oslo_netutils
67 from oslo_utils import strutils
68 from oslo_utils import timeutils
69 from oslo_utils import units
70 from oslo_utils import uuidutils
71 import six
72 from six.moves import range
73 
74 from nova.api.metadata import base as instance_metadata
75 from nova.api.metadata import password
76 from nova import block_device
77 from nova.compute import power_state
78 from nova.compute import task_states
79 from nova.compute import utils as compute_utils
80 from nova.compute import vm_states
81 import nova.conf
82 from nova.console import serial as serial_console
83 from nova.console import type as ctype
84 from nova import context as nova_context
85 from nova import crypto
86 from nova import exception
87 from nova.i18n import _
88 from nova import image
89 from nova.network import model as network_model
90 from nova import objects
91 from nova.objects import diagnostics as diagnostics_obj
92 from nova.objects import fields
93 from nova.pci import manager as pci_manager
94 from nova.pci import utils as pci_utils
95 import nova.privsep.libvirt
96 import nova.privsep.path
97 import nova.privsep.utils
98 from nova import utils
99 from nova import version
100 from nova.virt import block_device as driver_block_device
101 from nova.virt import configdrive
102 from nova.virt.disk import api as disk_api
103 from nova.virt.disk.vfs import guestfs
104 from nova.virt import driver
105 from nova.virt import firewall
106 from nova.virt import hardware
107 from nova.virt.image import model as imgmodel
108 from nova.virt import images
109 from nova.virt.libvirt import blockinfo
110 from nova.virt.libvirt import config as vconfig
111 from nova.virt.libvirt import designer
112 from nova.virt.libvirt import firewall as libvirt_firewall
113 from nova.virt.libvirt import guest as libvirt_guest
114 from nova.virt.libvirt import host
115 from nova.virt.libvirt import imagebackend
116 from nova.virt.libvirt import imagecache
117 from nova.virt.libvirt import instancejobtracker
118 from nova.virt.libvirt import migration as libvirt_migrate
119 from nova.virt.libvirt.storage import dmcrypt
120 from nova.virt.libvirt.storage import lvm
121 from nova.virt.libvirt.storage import rbd_utils
122 from nova.virt.libvirt import utils as libvirt_utils
123 from nova.virt.libvirt import vif as libvirt_vif
124 from nova.virt.libvirt.volume import mount
125 from nova.virt.libvirt.volume import remotefs
126 from nova.virt import netutils
127 from nova.volume import cinder
128 
129 libvirt = None
130 
131 uefi_logged = False
132 
133 LOG = logging.getLogger(__name__)
134 
135 CONF = nova.conf.CONF
136 
137 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
138     libvirt_firewall.__name__,
139     libvirt_firewall.IptablesFirewallDriver.__name__)
140 
141 DEFAULT_UEFI_LOADER_PATH = {
142     "x86_64": "/usr/share/OVMF/OVMF_CODE.fd",
143     "aarch64": "/usr/share/AAVMF/AAVMF_CODE.fd"
144 }
145 
146 MAX_CONSOLE_BYTES = 100 * units.Ki
147 
148 # The libvirt driver will prefix any disable reason codes with this string.
149 DISABLE_PREFIX = 'AUTO: '
150 # Disable reason for the service which was enabled or disabled without reason
151 DISABLE_REASON_UNDEFINED = None
152 
153 # Guest config console string
154 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
155 
156 GuestNumaConfig = collections.namedtuple(
157     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
158 
159 
160 class InjectionInfo(collections.namedtuple(
161         'InjectionInfo', ['network_info', 'files', 'admin_pass'])):
162     __slots__ = ()
163 
164     def __repr__(self):
165         return ('InjectionInfo(network_info=%r, files=%r, '
166                 'admin_pass=<SANITIZED>)') % (self.network_info, self.files)
167 
168 
169 libvirt_volume_drivers = [
170     'iscsi=nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
171     'iser=nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
172     'local=nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
173     'drbd=nova.virt.libvirt.volume.drbd.LibvirtDRBDVolumeDriver',
174     'fake=nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
175     'rbd=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
176     'sheepdog=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
177     'nfs=nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
178     'smbfs=nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
179     'aoe=nova.virt.libvirt.volume.aoe.LibvirtAOEVolumeDriver',
180     'fibre_channel='
181         'nova.virt.libvirt.volume.fibrechannel.'
182         'LibvirtFibreChannelVolumeDriver',
183     'gpfs=nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
184     'quobyte=nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
185     'hgst=nova.virt.libvirt.volume.hgst.LibvirtHGSTVolumeDriver',
186     'scaleio=nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
187     'disco=nova.virt.libvirt.volume.disco.LibvirtDISCOVolumeDriver',
188     'vzstorage='
189         'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',
190     'veritas_hyperscale='
191         'nova.virt.libvirt.volume.vrtshyperscale.'
192         'LibvirtHyperScaleVolumeDriver',
193     'storpool=nova.virt.libvirt.volume.storpool.LibvirtStorPoolVolumeDriver',
194     'nvmeof=nova.virt.libvirt.volume.nvme.LibvirtNVMEVolumeDriver',
195 ]
196 
197 
198 def patch_tpool_proxy():
199     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
200     or __repr__() calls. See bug #962840 for details.
201     We perform a monkey patch to replace those two instance methods.
202     """
203     def str_method(self):
204         return str(self._obj)
205 
206     def repr_method(self):
207         return repr(self._obj)
208 
209     tpool.Proxy.__str__ = str_method
210     tpool.Proxy.__repr__ = repr_method
211 
212 
213 patch_tpool_proxy()
214 
215 # For information about when MIN_LIBVIRT_VERSION and
216 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
217 #
218 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
219 #
220 # Currently this is effectively the min version for i686/x86_64
221 # + KVM/QEMU, as other architectures/hypervisors require newer
222 # versions. Over time, this will become a common min version
223 # for all architectures/hypervisors, as this value rises to
224 # meet them.
225 MIN_LIBVIRT_VERSION = (3, 0, 0)
226 MIN_QEMU_VERSION = (2, 8, 0)
227 # TODO(berrange): Re-evaluate this at start of each release cycle
228 # to decide if we want to plan a future min version bump.
229 # MIN_LIBVIRT_VERSION can be updated to match this after
230 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
231 # one cycle
232 NEXT_MIN_LIBVIRT_VERSION = (4, 0, 0)
233 NEXT_MIN_QEMU_VERSION = (2, 11, 0)
234 
235 
236 # Virtuozzo driver support
237 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
238 
239 # aarch64 architecture with KVM
240 # 'chardev' support got sorted out in 3.6.0
241 MIN_LIBVIRT_KVM_AARCH64_VERSION = (3, 6, 0)
242 
243 # Names of the types that do not get compressed during migration
244 NO_COMPRESSION_TYPES = ('qcow2',)
245 
246 
247 # number of serial console limit
248 QEMU_MAX_SERIAL_PORTS = 4
249 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
250 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
251 
252 MIN_LIBVIRT_OTHER_ARCH = {
253     fields.Architecture.AARCH64: MIN_LIBVIRT_KVM_AARCH64_VERSION,
254 }
255 
256 # perf events support
257 MIN_LIBVIRT_PERF_VERSION = (2, 0, 0)
258 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
259 
260 PERF_EVENTS_CPU_FLAG_MAPPING = {'cmt': 'cmt',
261                                 'mbml': 'mbm_local',
262                                 'mbmt': 'mbm_total',
263                                }
264 
265 # Mediated devices support
266 MIN_LIBVIRT_MDEV_SUPPORT = (3, 4, 0)
267 
268 # libvirt>=3.10 is required for volume multiattach unless qemu<2.10.
269 # See https://bugzilla.redhat.com/show_bug.cgi?id=1378242
270 # for details.
271 MIN_LIBVIRT_MULTIATTACH = (3, 10, 0)
272 
273 MIN_LIBVIRT_LUKS_VERSION = (2, 2, 0)
274 MIN_QEMU_LUKS_VERSION = (2, 6, 0)
275 
276 MIN_LIBVIRT_FILE_BACKED_VERSION = (4, 0, 0)
277 MIN_QEMU_FILE_BACKED_VERSION = (2, 6, 0)
278 
279 MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION = (4, 4, 0)
280 MIN_QEMU_FILE_BACKED_DISCARD_VERSION = (2, 10, 0)
281 
282 MIN_LIBVIRT_NATIVE_TLS_VERSION = (4, 4, 0)
283 MIN_QEMU_NATIVE_TLS_VERSION = (2, 11, 0)
284 
285 # If the host has this libvirt version, then we skip the retry loop of
286 # instance destroy() call, as libvirt itself increased the wait time
287 # before the SIGKILL signal takes effect.
288 MIN_LIBVIRT_BETTER_SIGKILL_HANDLING = (4, 7, 0)
289 
290 VGPU_RESOURCE_SEMAPHORE = "vgpu_resources"
291 
292 # see https://libvirt.org/formatdomain.html#elementsVideo
293 MIN_LIBVIRT_VIDEO_MODEL_VERSIONS = {
294     fields.VideoModel.GOP: (3, 2, 0),
295     fields.VideoModel.NONE: (4, 6, 0),
296 }
297 
298 
299 # This is used to save the pmem namespace info temporarily
300 PMEMNamespace = collections.namedtuple('PMEMNamespace',
301         ['label', 'name', 'dev', 'size',
302          'align', 'numa_node'])
303 
304 
305 class LibvirtDriver(driver.ComputeDriver):
306     def __init__(self, virtapi, read_only=False):
307         # NOTE(aspiers) Some of these are dynamic, so putting
308         # capabilities on the instance rather than on the class.
309         # This prevents the risk of one test setting a capability
310         # which bleeds over into other tests.
311 
312         # LVM and RBD require raw images. If we are not configured to
313         # force convert images into raw format, then we _require_ raw
314         # images only.
315         raw_only = ('rbd', 'lvm')
316         requires_raw_image = (CONF.libvirt.images_type in raw_only and
317                               not CONF.force_raw_images)
318 
319         self.capabilities = {
320             "has_imagecache": True,
321             "supports_evacuate": True,
322             "supports_migrate_to_same_host": False,
323             "supports_attach_interface": True,
324             "supports_device_tagging": True,
325             "supports_tagged_attach_interface": True,
326             "supports_tagged_attach_volume": True,
327             "supports_extend_volume": True,
328             # Multiattach support is conditional on qemu and libvirt versions
329             # determined in init_host.
330             "supports_multiattach": False,
331             "supports_trusted_certs": True,
332             # Supported image types
333             "supports_image_type_aki": True,
334             "supports_image_type_ari": True,
335             "supports_image_type_ami": True,
336             # FIXME(danms): I can see a future where people might want to
337             # configure certain compute nodes to not allow giant raw images
338             # to be booted (like nodes that are across a WAN). Thus, at some
339             # point we may want to be able to _not_ expose "supports raw" on
340             # some nodes by policy. Until then, raw is always supported.
341             "supports_image_type_raw": True,
342             "supports_image_type_iso": True,
343             # NOTE(danms): Certain backends do not work with complex image
344             # formats. If we are configured for those backends, then we
345             # should not expose the corresponding support traits.
346             "supports_image_type_qcow2": not requires_raw_image,
347         }
348         super(LibvirtDriver, self).__init__(virtapi)
349 
350         global libvirt
351         if libvirt is None:
352             libvirt = importutils.import_module('libvirt')
353             libvirt_migrate.libvirt = libvirt
354 
355         self._host = host.Host(self._uri(), read_only,
356                                lifecycle_event_handler=self.emit_event,
357                                conn_event_handler=self._handle_conn_event)
358         self._initiator = None
359         self._fc_wwnns = None
360         self._fc_wwpns = None
361         self._caps = None
362         self._supported_perf_events = []
363         self.firewall_driver = firewall.load_driver(
364             DEFAULT_FIREWALL_DRIVER,
365             host=self._host)
366 
367         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
368 
369         # TODO(mriedem): Long-term we should load up the volume drivers on
370         # demand as needed rather than doing this on startup, as there might
371         # be unsupported volume drivers in this list based on the underlying
372         # platform.
373         self.volume_drivers = self._get_volume_drivers()
374 
375         self._disk_cachemode = None
376         self.image_cache_manager = imagecache.ImageCacheManager()
377         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
378 
379         self.disk_cachemodes = {}
380 
381         self.valid_cachemodes = ["default",
382                                  "none",
383                                  "writethrough",
384                                  "writeback",
385                                  "directsync",
386                                  "unsafe",
387                                 ]
388         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
389                                                                       'qemu')
390 
391         for mode_str in CONF.libvirt.disk_cachemodes:
392             disk_type, sep, cache_mode = mode_str.partition('=')
393             if cache_mode not in self.valid_cachemodes:
394                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
395                             'for disk type %(disk_type)s.',
396                             {'cache_mode': cache_mode, 'disk_type': disk_type})
397                 continue
398             self.disk_cachemodes[disk_type] = cache_mode
399 
400         self._volume_api = cinder.API()
401         self._image_api = image.API()
402 
403         # The default choice for the sysinfo_serial config option is "unique"
404         # which does not have a special function since the value is just the
405         # instance.uuid.
406         sysinfo_serial_funcs = {
407             'none': lambda: None,
408             'hardware': self._get_host_sysinfo_serial_hardware,
409             'os': self._get_host_sysinfo_serial_os,
410             'auto': self._get_host_sysinfo_serial_auto,
411         }
412 
413         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
414             CONF.libvirt.sysinfo_serial)
415 
416         self.job_tracker = instancejobtracker.InstanceJobTracker()
417         self._remotefs = remotefs.RemoteFilesystem()
418 
419         self._live_migration_flags = self._block_migration_flags = 0
420         self.active_migrations = {}
421 
422         # Compute reserved hugepages from conf file at the very
423         # beginning to ensure any syntax error will be reported and
424         # avoid any re-calculation when computing resources.
425         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
426 
427         # Copy of the compute service ProviderTree object that is updated
428         # every time update_provider_tree() is called.
429         # NOTE(sbauza): We only want a read-only cache, this attribute is not
430         # intended to be updatable directly
431         self.provider_tree = None
432 
433         # get pmem namespaces info
434         self._pmem_namespaces = self._get_pmem_namespaces()
435 
436     def _get_pmem_namespaces(self):
437         # pmem namespace dict {ns_name:PMEMNamespace,...}
438         pmems_by_name = {}
439         # pmem name list group by label
440         pmem_names_by_label = collections.defaultdict(list)
441         if CONF.libvirt.pmem_namespaces:
442             pmems_host = self._get_pmem_namespaces_on_host()
443             for ns_conf in CONF.libvirt.pmem_namespaces:
444                 ns_label = ns_conf.split(":")[0].strip()
445                 ns_names = ns_conf.split(":")[1].split("|")
446                 for ns_name in ns_names:
447                     if pmems_host.get(ns_name):
448                         pmem_ns = pmems_host[ns_name]
449                         pmem_ns_updated = PMEMNamespace(
450                                 label=ns_label,
451                                 name=pmem_ns.name,
452                                 dev=pmem_ns.dev,
453                                 size=pmem_ns.size,
454                                 align=pmem_ns.align,
455                                 numa_node=pmem_ns.numa_node)
456                         pmems_by_name[ns_name] = pmem_ns_updated
457                         pmem_names_by_label[ns_label].append(ns_name)
458                     else:
459                         raise exception.PMEMNamespaceNotFound(
460                                 namespace=ns_name)
461         return pmems_by_name, pmem_names_by_label
462 
463     def _get_pmem_namespaces_on_host(self):
464         pmems_host = {}
465         nss = jsonutils.loads(nova.privsep.libvirt.get_pmem_namespaces())
466         for ns in nss:
467             if not ns.get('name'):
468                 continue
469             pmems_host[ns['name']] = \
470                 PMEMNamespace(label=None,
471                               name=ns['name'],
472                               dev=ns['daxregion']['devices'][0]['chardev'],
473                               size=ns['size'],
474                               align=ns['daxregion']['align'],
475                               numa_node=ns['numa_node'])
476         return pmems_host
477 
478     def _get_assigned_pmem_names_by_label(self, context, instances):
479         assigned_pmem_names_by_label = {}
480         vpmems = []
481         for item in instances:
482             if isinstance(item, objects.Instance):
483                 inst = item
484             else:
485                 inst = objects.Instance.get_by_uuid(
486                         context, item, expected_attrs=['vpmems'])
487             if 'vpmems' in inst and inst.vpmems:
488                 vpmems.extend(inst.vpmems.vpmems)
489         for vpmem in vpmems:
490             if 'ns_name' in vpmem and vpmem.ns_name:
491                 assigned_pmem_names_by_label.setdefault(vpmem.label, [])
492                 assigned_pmem_names_by_label[vpmem.label].append(vpmem.ns_name)
493         return assigned_pmem_names_by_label
494 
495     def allocate_pmems(self, context, vpmems):
496         curr_vpmems = vpmems
497         new_vpmems = []
498 
499         instance_domains = self._host.list_instance_domains(
500                 only_running=False)
501         instance_uuids = [dom.UUIDString() for dom in instance_domains]
502         assigned_pmems = self._get_assigned_pmem_names_by_label(
503                 context, instance_uuids)
504         pmems, pmems_by_label = self._pmem_namespaces
505         available_pmems_by_label = {}
506         for label in pmems_by_label.keys():
507             available_pmems_by_label[label] = set(pmems_by_label[label]) - \
508                     set(assigned_pmems.get(label) or [])
509         for vpmem in curr_vpmems:
510             label = vpmem.label
511             try:
512                 ns_name = available_pmems_by_label[label].pop()
513                 pmem = pmems.get(ns_name)
514             except Exception:
515                 raise exception.NoAvailablePMEMNamespace(label=label)
516             new_vpmem = objects.VirtualPMEM(
517                     label=label,
518                     ns_size=pmem.size,
519                     ns_name=pmem.name,
520                     ns_dev=pmem.dev,
521                     ns_align=pmem.align)
522             new_vpmems.append(new_vpmem)
523         return new_vpmems
524 
525     def _get_volume_drivers(self):
526         driver_registry = dict()
527 
528         for driver_str in libvirt_volume_drivers:
529             driver_type, _sep, driver = driver_str.partition('=')
530             driver_class = importutils.import_class(driver)
531             try:
532                 driver_registry[driver_type] = driver_class(self._host)
533             except brick_exception.InvalidConnectorProtocol:
534                 LOG.debug('Unable to load volume driver %s. It is not '
535                           'supported on this host.', driver)
536 
537         return driver_registry
538 
539     @property
540     def disk_cachemode(self):
541         # It can be confusing to understand the QEMU cache mode
542         # behaviour, because each cache=$MODE is a convenient shorthand
543         # to toggle _three_ cache.* booleans.  Consult the below table
544         # (quoting from the QEMU man page):
545         #
546         #              | cache.writeback | cache.direct | cache.no-flush
547         # --------------------------------------------------------------
548         # writeback    | on              | off          | off
549         # none         | on              | on           | off
550         # writethrough | off             | off          | off
551         # directsync   | off             | on           | off
552         # unsafe       | on              | off          | on
553         #
554         # Where:
555         #
556         #  - 'cache.writeback=off' means: QEMU adds an automatic fsync()
557         #    after each write request.
558         #
559         #  - 'cache.direct=on' means: Use Linux's O_DIRECT, i.e. bypass
560         #    the kernel page cache.  Caches in any other layer (disk
561         #    cache, QEMU metadata caches, etc.) can still be present.
562         #
563         #  - 'cache.no-flush=on' means: Ignore flush requests, i.e.
564         #    never call fsync(), even if the guest explicitly requested
565         #    it.
566         #
567         # Use cache mode "none" (cache.writeback=on, cache.direct=on,
568         # cache.no-flush=off) for consistent performance and
569         # migration correctness.  Some filesystems don't support
570         # O_DIRECT, though.  For those we fallback to the next
571         # reasonable option that is "writeback" (cache.writeback=on,
572         # cache.direct=off, cache.no-flush=off).
573 
574         if self._disk_cachemode is None:
575             self._disk_cachemode = "none"
576             if not nova.privsep.utils.supports_direct_io(CONF.instances_path):
577                 self._disk_cachemode = "writeback"
578         return self._disk_cachemode
579 
580     def _set_cache_mode(self, conf):
581         """Set cache mode on LibvirtConfigGuestDisk object."""
582         try:
583             source_type = conf.source_type
584             driver_cache = conf.driver_cache
585         except AttributeError:
586             return
587 
588         # Shareable disks like for a multi-attach volume need to have the
589         # driver cache disabled.
590         if getattr(conf, 'shareable', False):
591             conf.driver_cache = 'none'
592         else:
593             cache_mode = self.disk_cachemodes.get(source_type,
594                                                   driver_cache)
595             conf.driver_cache = cache_mode
596 
597     def _do_quality_warnings(self):
598         """Warn about potential configuration issues.
599 
600         This will log a warning message for things such as untested driver or
601         host arch configurations in order to indicate potential issues to
602         administrators.
603         """
604         caps = self._host.get_capabilities()
605         hostarch = caps.host.cpu.arch
606         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
607             hostarch not in (fields.Architecture.I686,
608                              fields.Architecture.X86_64)):
609             LOG.warning('The libvirt driver is not tested on '
610                         '%(type)s/%(arch)s by the OpenStack project and '
611                         'thus its quality can not be ensured. For more '
612                         'information, see: https://docs.openstack.org/'
613                         'nova/latest/user/support-matrix.html',
614                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
615 
616         if CONF.vnc.keymap:
617             LOG.warning('The option "[vnc] keymap" has been deprecated '
618                         'in favor of configuration within the guest. '
619                         'Update nova.conf to address this change and '
620                         'refer to bug #1682020 for more information.')
621 
622         if CONF.spice.keymap:
623             LOG.warning('The option "[spice] keymap" has been deprecated '
624                         'in favor of configuration within the guest. '
625                         'Update nova.conf to address this change and '
626                         'refer to bug #1682020 for more information.')
627 
628     def _handle_conn_event(self, enabled, reason):
629         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
630                  {'enabled': enabled, 'reason': reason})
631         self._set_host_enabled(enabled, reason)
632 
633     def _check_pmem_namespaces(self, context):
634         instance_domains = self._host.list_instance_domains(
635                 only_running=False)
636         instances = [dom.UUIDString() for dom in instance_domains]
637         assigned_pmems_by_label = self._get_assigned_pmem_names_by_label(
638                 context, instances)
639         pmems, pmems_by_label = self._pmem_namespaces
640         for label in assigned_pmems_by_label:
641             for assigned_pmem in assigned_pmems_by_label[label]:
642                 if assigned_pmem not in pmems:
643                     raise exception.PMEMNamespaceConfigERROR(
644                             assigned_pmem=assigned_pmem)
645 
646     def init_host(self, host):
647         self._host.initialize()
648 
649         self._do_quality_warnings()
650 
651         self._parse_migration_flags()
652 
653         self._supported_perf_events = self._get_supported_perf_events()
654 
655         self._set_multiattach_support()
656 
657         self._check_file_backed_memory_support()
658 
659         self._check_pmem_namespaces(nova_context.get_admin_context())
660 
661         if (CONF.libvirt.virt_type == 'lxc' and
662                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
663             LOG.warning("Running libvirt-lxc without user namespaces is "
664                         "dangerous. Containers spawned by Nova will be run "
665                         "as the host's root user. It is highly suggested "
666                         "that user namespaces be used in a public or "
667                         "multi-tenant environment.")
668 
669         # Stop libguestfs using KVM unless we're also configured
670         # to use this. This solves problem where people need to
671         # stop Nova use of KVM because nested-virt is broken
672         if CONF.libvirt.virt_type != "kvm":
673             guestfs.force_tcg()
674 
675         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
676             raise exception.InternalError(
677                 _('Nova requires libvirt version %s or greater.') %
678                 libvirt_utils.version_to_string(MIN_LIBVIRT_VERSION))
679 
680         if CONF.libvirt.virt_type in ("qemu", "kvm"):
681             if self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
682                 # "qemu-img info" calls are version dependent, so we need to
683                 # store the version in the images module.
684                 images.QEMU_VERSION = self._host.get_connection().getVersion()
685             else:
686                 raise exception.InternalError(
687                     _('Nova requires QEMU version %s or greater.') %
688                     libvirt_utils.version_to_string(MIN_QEMU_VERSION))
689 
690         if CONF.libvirt.virt_type == 'parallels':
691             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
692                 raise exception.InternalError(
693                     _('Nova requires Virtuozzo version %s or greater.') %
694                     libvirt_utils.version_to_string(MIN_VIRTUOZZO_VERSION))
695 
696         # Give the cloud admin a heads up if we are intending to
697         # change the MIN_LIBVIRT_VERSION in the next release.
698         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
699             LOG.warning('Running Nova with a libvirt version less than '
700                         '%(version)s is deprecated. The required minimum '
701                         'version of libvirt will be raised to %(version)s '
702                         'in the next release.',
703                         {'version': libvirt_utils.version_to_string(
704                             NEXT_MIN_LIBVIRT_VERSION)})
705         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
706             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
707             LOG.warning('Running Nova with a QEMU version less than '
708                         '%(version)s is deprecated. The required minimum '
709                         'version of QEMU will be raised to %(version)s '
710                         'in the next release.',
711                         {'version': libvirt_utils.version_to_string(
712                             NEXT_MIN_QEMU_VERSION)})
713 
714         kvm_arch = fields.Architecture.from_host()
715         if (CONF.libvirt.virt_type in ('kvm', 'qemu') and
716             kvm_arch in MIN_LIBVIRT_OTHER_ARCH and
717                 not self._host.has_min_version(
718                     MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))):
719             raise exception.InternalError(
720                 _('Running Nova with qemu/kvm virt_type on %(arch)s '
721                   'requires libvirt version %(libvirt_ver)s or greater') %
722                 {'arch': kvm_arch,
723                  'libvirt_ver': libvirt_utils.version_to_string(
724                      MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))})
725 
726         # Allowing both "tunnelling via libvirtd" (which will be
727         # deprecated once the MIN_{LIBVIRT,QEMU}_VERSION is sufficiently
728         # new enough) and "native TLS" options at the same time is
729         # nonsensical.
730         if (CONF.libvirt.live_migration_tunnelled and
731                 CONF.libvirt.live_migration_with_native_tls):
732             msg = _("Setting both 'live_migration_tunnelled' and "
733                     "'live_migration_with_native_tls' at the same "
734                     "time is invalid. If you have the relevant "
735                     "libvirt and QEMU versions, and TLS configured "
736                     "in your environment, pick "
737                     "'live_migration_with_native_tls'.")
738             raise exception.Invalid(msg)
739 
740         # Some imagebackends are only able to import raw disk images,
741         # and will fail if given any other format. See the bug
742         # https://bugs.launchpad.net/nova/+bug/1816686 for more details.
743         if CONF.libvirt.images_type in ('rbd',):
744             if not CONF.force_raw_images:
745                 msg = _("'[DEFAULT]/force_raw_images = False' is not "
746                         "allowed with '[libvirt]/images_type = rbd'. "
747                         "Please check the two configs and if you really "
748                         "do want to use rbd as images_type, set "
749                         "force_raw_images to True.")
750                 raise exception.InvalidConfiguration(msg)
751 
752         # TODO(sbauza): Remove this code once mediated devices are persisted
753         # across reboots.
754         if self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
755             self._recreate_assigned_mediated_devices()
756 
757     @staticmethod
758     def _is_existing_mdev(uuid):
759         # FIXME(sbauza): Some kernel can have a uevent race meaning that the
760         # libvirt daemon won't know when a mediated device is created unless
761         # you restart that daemon. Until all kernels we support are not having
762         # that possible race, check the sysfs directly instead of asking the
763         # libvirt API.
764         # See https://bugzilla.redhat.com/show_bug.cgi?id=1376907 for ref.
765         return os.path.exists('/sys/bus/mdev/devices/{0}'.format(uuid))
766 
767     def _recreate_assigned_mediated_devices(self):
768         """Recreate assigned mdevs that could have disappeared if we reboot
769         the host.
770         """
771         # FIXME(sbauza): We blindly recreate mediated devices without checking
772         # which ResourceProvider was allocated for the instance so it would use
773         # another pGPU.
774         # TODO(sbauza): Pass all instances' allocations here.
775         mdevs = self._get_all_assigned_mediated_devices()
776         requested_types = self._get_supported_vgpu_types()
777         for (mdev_uuid, instance_uuid) in six.iteritems(mdevs):
778             if not self._is_existing_mdev(mdev_uuid):
779                 self._create_new_mediated_device(requested_types, mdev_uuid)
780 
781     def _set_multiattach_support(self):
782         # Check to see if multiattach is supported. Based on bugzilla
783         # https://bugzilla.redhat.com/show_bug.cgi?id=1378242 and related
784         # clones, the shareable flag on a disk device will only work with
785         # qemu<2.10 or libvirt>=3.10. So check those versions here and set
786         # the capability appropriately.
787         if (self._host.has_min_version(lv_ver=MIN_LIBVIRT_MULTIATTACH) or
788                 not self._host.has_min_version(hv_ver=(2, 10, 0))):
789             self.capabilities['supports_multiattach'] = True
790         else:
791             LOG.debug('Volume multiattach is not supported based on current '
792                       'versions of QEMU and libvirt. QEMU must be less than '
793                       '2.10 or libvirt must be greater than or equal to 3.10.')
794 
795     def _check_file_backed_memory_support(self):
796         if CONF.libvirt.file_backed_memory:
797             # file_backed_memory is only compatible with qemu/kvm virts
798             if CONF.libvirt.virt_type not in ("qemu", "kvm"):
799                 raise exception.InternalError(
800                     _('Running Nova with file_backed_memory and virt_type '
801                       '%(type)s is not supported. file_backed_memory is only '
802                       'supported with qemu and kvm types.') %
803                     {'type': CONF.libvirt.virt_type})
804 
805             # Check needed versions for file_backed_memory
806             if not self._host.has_min_version(
807                     MIN_LIBVIRT_FILE_BACKED_VERSION,
808                     MIN_QEMU_FILE_BACKED_VERSION):
809                 raise exception.InternalError(
810                     _('Running Nova with file_backed_memory requires libvirt '
811                       'version %(libvirt)s and qemu version %(qemu)s') %
812                     {'libvirt': libvirt_utils.version_to_string(
813                         MIN_LIBVIRT_FILE_BACKED_VERSION),
814                     'qemu': libvirt_utils.version_to_string(
815                         MIN_QEMU_FILE_BACKED_VERSION)})
816 
817             # file-backed memory doesn't work with memory overcommit.
818             # Block service startup if file-backed memory is enabled and
819             # ram_allocation_ratio is not 1.0
820             if CONF.ram_allocation_ratio != 1.0:
821                 raise exception.InternalError(
822                     'Running Nova with file_backed_memory requires '
823                     'ram_allocation_ratio configured to 1.0')
824 
825     def _prepare_migration_flags(self):
826         migration_flags = 0
827 
828         migration_flags |= libvirt.VIR_MIGRATE_LIVE
829 
830         # Adding p2p flag only if xen is not in use, because xen does not
831         # support p2p migrations
832         if CONF.libvirt.virt_type != 'xen':
833             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
834 
835         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
836         # instance will remain defined on the source host
837         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
838 
839         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
840         # destination host
841         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
842 
843         live_migration_flags = block_migration_flags = migration_flags
844 
845         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
846         # will be live-migrations instead
847         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
848 
849         return (live_migration_flags, block_migration_flags)
850 
851     # TODO(kchamart) Once the MIN_LIBVIRT_VERSION and MIN_QEMU_VERSION
852     # reach 4.4.0 and 2.11.0, which provide "native TLS" support by
853     # default, deprecate and remove the support for "tunnelled live
854     # migration" (and related config attribute), because:
855     #
856     #  (a) it cannot handle live migration of disks in a non-shared
857     #      storage setup (a.k.a. "block migration");
858     #
859     #  (b) has a huge performance overhead and latency, because it burns
860     #      more CPU and memory bandwidth due to increased number of data
861     #      copies on both source and destination hosts.
862     #
863     # Both the above limitations are addressed by the QEMU-native TLS
864     # support (`live_migration_with_native_tls`).
865     def _handle_live_migration_tunnelled(self, migration_flags):
866         if CONF.libvirt.live_migration_tunnelled:
867             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
868         return migration_flags
869 
870     def _is_native_tls_available(self):
871         return self._host.has_min_version(MIN_LIBVIRT_NATIVE_TLS_VERSION,
872                                           MIN_QEMU_NATIVE_TLS_VERSION)
873 
874     def _handle_native_tls(self, migration_flags):
875         if (CONF.libvirt.live_migration_with_native_tls and
876                 self._is_native_tls_available()):
877             migration_flags |= libvirt.VIR_MIGRATE_TLS
878         return migration_flags
879 
880     def _is_native_luks_available(self):
881         return self._host.has_min_version(MIN_LIBVIRT_LUKS_VERSION,
882                                           MIN_QEMU_LUKS_VERSION)
883 
884     def _handle_live_migration_post_copy(self, migration_flags):
885         if CONF.libvirt.live_migration_permit_post_copy:
886             migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
887         return migration_flags
888 
889     def _handle_live_migration_auto_converge(self, migration_flags):
890         if self._is_post_copy_enabled(migration_flags):
891             LOG.info('The live_migration_permit_post_copy is set to '
892                      'True and post copy live migration is available '
893                      'so auto-converge will not be in use.')
894         elif CONF.libvirt.live_migration_permit_auto_converge:
895             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
896         return migration_flags
897 
898     def _parse_migration_flags(self):
899         (live_migration_flags,
900             block_migration_flags) = self._prepare_migration_flags()
901 
902         live_migration_flags = self._handle_live_migration_tunnelled(
903             live_migration_flags)
904         block_migration_flags = self._handle_live_migration_tunnelled(
905             block_migration_flags)
906 
907         live_migration_flags = self._handle_native_tls(
908             live_migration_flags)
909         block_migration_flags = self._handle_native_tls(
910             block_migration_flags)
911 
912         live_migration_flags = self._handle_live_migration_post_copy(
913             live_migration_flags)
914         block_migration_flags = self._handle_live_migration_post_copy(
915             block_migration_flags)
916 
917         live_migration_flags = self._handle_live_migration_auto_converge(
918             live_migration_flags)
919         block_migration_flags = self._handle_live_migration_auto_converge(
920             block_migration_flags)
921 
922         self._live_migration_flags = live_migration_flags
923         self._block_migration_flags = block_migration_flags
924 
925     # TODO(sahid): This method is targeted for removal when the tests
926     # have been updated to avoid its use
927     #
928     # All libvirt API calls on the libvirt.Connect object should be
929     # encapsulated by methods on the nova.virt.libvirt.host.Host
930     # object, rather than directly invoking the libvirt APIs. The goal
931     # is to avoid a direct dependency on the libvirt API from the
932     # driver.py file.
933     def _get_connection(self):
934         return self._host.get_connection()
935 
936     _conn = property(_get_connection)
937 
938     @staticmethod
939     def _uri():
940         if CONF.libvirt.virt_type == 'uml':
941             uri = CONF.libvirt.connection_uri or 'uml:///system'
942         elif CONF.libvirt.virt_type == 'xen':
943             uri = CONF.libvirt.connection_uri or 'xen:///'
944         elif CONF.libvirt.virt_type == 'lxc':
945             uri = CONF.libvirt.connection_uri or 'lxc:///'
946         elif CONF.libvirt.virt_type == 'parallels':
947             uri = CONF.libvirt.connection_uri or 'parallels:///system'
948         else:
949             uri = CONF.libvirt.connection_uri or 'qemu:///system'
950         return uri
951 
952     @staticmethod
953     def _live_migration_uri(dest):
954         uris = {
955             'kvm': 'qemu+%s://%s/system',
956             'qemu': 'qemu+%s://%s/system',
957             'xen': 'xenmigr://%s/system',
958             'parallels': 'parallels+tcp://%s/system',
959         }
960         dest = oslo_netutils.escape_ipv6(dest)
961 
962         virt_type = CONF.libvirt.virt_type
963         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
964         uri = CONF.libvirt.live_migration_uri
965         if uri:
966             return uri % dest
967 
968         uri = uris.get(virt_type)
969         if uri is None:
970             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
971 
972         str_format = (dest,)
973         if virt_type in ('kvm', 'qemu'):
974             scheme = CONF.libvirt.live_migration_scheme or 'tcp'
975             str_format = (scheme, dest)
976         return uris.get(virt_type) % str_format
977 
978     @staticmethod
979     def _migrate_uri(dest):
980         uri = None
981         dest = oslo_netutils.escape_ipv6(dest)
982 
983         # Only QEMU live migrations supports migrate-uri parameter
984         virt_type = CONF.libvirt.virt_type
985         if virt_type in ('qemu', 'kvm'):
986             # QEMU accept two schemes: tcp and rdma.  By default
987             # libvirt build the URI using the remote hostname and the
988             # tcp schema.
989             uri = 'tcp://%s' % dest
990         # Because dest might be of type unicode, here we might return value of
991         # type unicode as well which is not acceptable by libvirt python
992         # binding when Python 2.7 is in use, so let's convert it explicitly
993         # back to string. When Python 3.x is in use, libvirt python binding
994         # accepts unicode type so it is completely fine to do a no-op str(uri)
995         # conversion which will return value of type unicode.
996         return uri and str(uri)
997 
998     def instance_exists(self, instance):
999         """Efficient override of base instance_exists method."""
1000         try:
1001             self._host.get_guest(instance)
1002             return True
1003         except (exception.InternalError, exception.InstanceNotFound):
1004             return False
1005 
1006     def estimate_instance_overhead(self, instance_info):
1007         overhead = super(LibvirtDriver, self).estimate_instance_overhead(
1008             instance_info)
1009         if isinstance(instance_info, objects.Flavor):
1010             # A flavor object is passed during case of migrate
1011             emu_policy = hardware.get_emulator_thread_policy_constraint(
1012                 instance_info)
1013             if emu_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
1014                 overhead['vcpus'] += 1
1015         else:
1016             # An instance object is passed during case of spawing or a
1017             # dict is passed when computing resource for an instance
1018             numa_topology = hardware.instance_topology_from_instance(
1019                 instance_info)
1020             if numa_topology and numa_topology.emulator_threads_isolated:
1021                 overhead['vcpus'] += 1
1022         return overhead
1023 
1024     def list_instances(self):
1025         names = []
1026         for guest in self._host.list_guests(only_running=False):
1027             names.append(guest.name)
1028 
1029         return names
1030 
1031     def list_instance_uuids(self):
1032         uuids = []
1033         for guest in self._host.list_guests(only_running=False):
1034             uuids.append(guest.uuid)
1035 
1036         return uuids
1037 
1038     def plug_vifs(self, instance, network_info):
1039         """Plug VIFs into networks."""
1040         for vif in network_info:
1041             self.vif_driver.plug(instance, vif)
1042 
1043     def _unplug_vifs(self, instance, network_info, ignore_errors):
1044         """Unplug VIFs from networks."""
1045         for vif in network_info:
1046             try:
1047                 self.vif_driver.unplug(instance, vif)
1048             except exception.NovaException:
1049                 if not ignore_errors:
1050                     raise
1051 
1052     def unplug_vifs(self, instance, network_info):
1053         self._unplug_vifs(instance, network_info, False)
1054 
1055     def _teardown_container(self, instance):
1056         inst_path = libvirt_utils.get_instance_path(instance)
1057         container_dir = os.path.join(inst_path, 'rootfs')
1058         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
1059         LOG.debug('Attempting to teardown container at path %(dir)s with '
1060                   'root device: %(rootfs_dev)s',
1061                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
1062                   instance=instance)
1063         disk_api.teardown_container(container_dir, rootfs_dev)
1064 
1065     def _destroy(self, instance, attempt=1):
1066         try:
1067             guest = self._host.get_guest(instance)
1068             if CONF.serial_console.enabled:
1069                 # This method is called for several events: destroy,
1070                 # rebuild, hard-reboot, power-off - For all of these
1071                 # events we want to release the serial ports acquired
1072                 # for the guest before destroying it.
1073                 serials = self._get_serial_ports_from_guest(guest)
1074                 for hostname, port in serials:
1075                     serial_console.release_port(host=hostname, port=port)
1076         except exception.InstanceNotFound:
1077             guest = None
1078 
1079         # If the instance is already terminated, we're still happy
1080         # Otherwise, destroy it
1081         old_domid = -1
1082         if guest is not None:
1083             try:
1084                 old_domid = guest.id
1085                 guest.poweroff()
1086 
1087             except libvirt.libvirtError as e:
1088                 is_okay = False
1089                 errcode = e.get_error_code()
1090                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1091                     # Domain already gone. This can safely be ignored.
1092                     is_okay = True
1093                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
1094                     # If the instance is already shut off, we get this:
1095                     # Code=55 Error=Requested operation is not valid:
1096                     # domain is not running
1097 
1098                     state = guest.get_power_state(self._host)
1099                     if state == power_state.SHUTDOWN:
1100                         is_okay = True
1101                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
1102                     errmsg = e.get_error_message()
1103                     if (CONF.libvirt.virt_type == 'lxc' and
1104                         errmsg == 'internal error: '
1105                                   'Some processes refused to die'):
1106                         # Some processes in the container didn't die
1107                         # fast enough for libvirt. The container will
1108                         # eventually die. For now, move on and let
1109                         # the wait_for_destroy logic take over.
1110                         is_okay = True
1111                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
1112                     LOG.warning("Cannot destroy instance, operation time out",
1113                                 instance=instance)
1114                     reason = _("operation time out")
1115                     raise exception.InstancePowerOffFailure(reason=reason)
1116                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
1117                     if e.get_int1() == errno.EBUSY:
1118                         # NOTE(danpb): When libvirt kills a process it sends it
1119                         # SIGTERM first and waits 10 seconds. If it hasn't gone
1120                         # it sends SIGKILL and waits another 5 seconds. If it
1121                         # still hasn't gone then you get this EBUSY error.
1122                         # Usually when a QEMU process fails to go away upon
1123                         # SIGKILL it is because it is stuck in an
1124                         # uninterruptible kernel sleep waiting on I/O from
1125                         # some non-responsive server.
1126                         # Given the CPU load of the gate tests though, it is
1127                         # conceivable that the 15 second timeout is too short,
1128                         # particularly if the VM running tempest has a high
1129                         # steal time from the cloud host. ie 15 wallclock
1130                         # seconds may have passed, but the VM might have only
1131                         # have a few seconds of scheduled run time.
1132                         #
1133                         # TODO(kchamart): Once MIN_LIBVIRT_VERSION
1134                         # reaches v4.7.0, (a) rewrite the above note,
1135                         # and (b) remove the following code that retries
1136                         # _destroy() API call (which gives SIGKILL 30
1137                         # seconds to take effect) -- because from v4.7.0
1138                         # onwards, libvirt _automatically_ increases the
1139                         # timeout to 30 seconds.  This was added in the
1140                         # following libvirt commits:
1141                         #
1142                         #   - 9a4e4b942 (process: wait longer 5->30s on
1143                         #     hard shutdown)
1144                         #
1145                         #   - be2ca0444 (process: wait longer on kill
1146                         #     per assigned Hostdev)
1147                         with excutils.save_and_reraise_exception() as ctxt:
1148                             if not self._host.has_min_version(
1149                                     MIN_LIBVIRT_BETTER_SIGKILL_HANDLING):
1150                                 LOG.warning('Error from libvirt during '
1151                                             'destroy. Code=%(errcode)s '
1152                                             'Error=%(e)s; attempt '
1153                                             '%(attempt)d of 6 ',
1154                                             {'errcode': errcode, 'e': e,
1155                                              'attempt': attempt},
1156                                             instance=instance)
1157                                 # Try up to 6 times before giving up.
1158                                 if attempt < 6:
1159                                     ctxt.reraise = False
1160                                     self._destroy(instance, attempt + 1)
1161                                     return
1162 
1163                 if not is_okay:
1164                     with excutils.save_and_reraise_exception():
1165                         LOG.error('Error from libvirt during destroy. '
1166                                   'Code=%(errcode)s Error=%(e)s',
1167                                   {'errcode': errcode, 'e': e},
1168                                   instance=instance)
1169 
1170         def _wait_for_destroy(expected_domid):
1171             """Called at an interval until the VM is gone."""
1172             # NOTE(vish): If the instance disappears during the destroy
1173             #             we ignore it so the cleanup can still be
1174             #             attempted because we would prefer destroy to
1175             #             never fail.
1176             try:
1177                 dom_info = self.get_info(instance)
1178                 state = dom_info.state
1179                 new_domid = dom_info.internal_id
1180             except exception.InstanceNotFound:
1181                 LOG.debug("During wait destroy, instance disappeared.",
1182                           instance=instance)
1183                 state = power_state.SHUTDOWN
1184 
1185             if state == power_state.SHUTDOWN:
1186                 LOG.info("Instance destroyed successfully.", instance=instance)
1187                 raise loopingcall.LoopingCallDone()
1188 
1189             # NOTE(wangpan): If the instance was booted again after destroy,
1190             #                this may be an endless loop, so check the id of
1191             #                domain here, if it changed and the instance is
1192             #                still running, we should destroy it again.
1193             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
1194             if new_domid != expected_domid:
1195                 LOG.info("Instance may be started again.", instance=instance)
1196                 kwargs['is_running'] = True
1197                 raise loopingcall.LoopingCallDone()
1198 
1199         kwargs = {'is_running': False}
1200         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
1201                                                      old_domid)
1202         timer.start(interval=0.5).wait()
1203         if kwargs['is_running']:
1204             LOG.info("Going to destroy instance again.", instance=instance)
1205             self._destroy(instance)
1206         else:
1207             # NOTE(GuanQiang): teardown container to avoid resource leak
1208             if CONF.libvirt.virt_type == 'lxc':
1209                 self._teardown_container(instance)
1210 
1211     def destroy(self, context, instance, network_info, block_device_info=None,
1212                 destroy_disks=True):
1213         self._destroy(instance)
1214         self.cleanup(context, instance, network_info, block_device_info,
1215                      destroy_disks)
1216 
1217     def _undefine_domain(self, instance):
1218         try:
1219             guest = self._host.get_guest(instance)
1220             try:
1221                 support_uefi = self._has_uefi_support()
1222                 guest.delete_configuration(support_uefi)
1223             except libvirt.libvirtError as e:
1224                 with excutils.save_and_reraise_exception() as ctxt:
1225                     errcode = e.get_error_code()
1226                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1227                         LOG.debug("Called undefine, but domain already gone.",
1228                                   instance=instance)
1229                         ctxt.reraise = False
1230                     else:
1231                         LOG.error('Error from libvirt during undefine. '
1232                                   'Code=%(errcode)s Error=%(e)s',
1233                                   {'errcode': errcode,
1234                                    'e': encodeutils.exception_to_unicode(e)},
1235                                   instance=instance)
1236         except exception.InstanceNotFound:
1237             pass
1238 
1239     def cleanup(self, context, instance, network_info, block_device_info=None,
1240                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
1241         if destroy_vifs:
1242             self._unplug_vifs(instance, network_info, True)
1243 
1244         # Continue attempting to remove firewall filters for the instance
1245         # until it's done or there is a failure to remove the filters. If
1246         # unfilter fails because the instance is not yet shutdown, try to
1247         # destroy the guest again and then retry the unfilter.
1248         while True:
1249             try:
1250                 self.unfilter_instance(instance, network_info)
1251                 break
1252             except libvirt.libvirtError as e:
1253                 try:
1254                     state = self.get_info(instance).state
1255                 except exception.InstanceNotFound:
1256                     state = power_state.SHUTDOWN
1257 
1258                 if state != power_state.SHUTDOWN:
1259                     LOG.warning("Instance may be still running, destroy "
1260                                 "it again.", instance=instance)
1261                     self._destroy(instance)
1262                 else:
1263                     errcode = e.get_error_code()
1264                     LOG.exception(_('Error from libvirt during unfilter. '
1265                                     'Code=%(errcode)s Error=%(e)s'),
1266                                   {'errcode': errcode, 'e': e},
1267                                   instance=instance)
1268                     reason = _("Error unfiltering instance.")
1269                     raise exception.InstanceTerminationFailure(reason=reason)
1270             except Exception:
1271                 raise
1272 
1273         # FIXME(wangpan): if the instance is booted again here, such as the
1274         #                 soft reboot operation boot it here, it will become
1275         #                 "running deleted", should we check and destroy it
1276         #                 at the end of this method?
1277 
1278         # NOTE(vish): we disconnect from volumes regardless
1279         block_device_mapping = driver.block_device_info_get_mapping(
1280             block_device_info)
1281         for vol in block_device_mapping:
1282             connection_info = vol['connection_info']
1283             disk_dev = vol['mount_device']
1284             if disk_dev is not None:
1285                 disk_dev = disk_dev.rpartition("/")[2]
1286             try:
1287                 self._disconnect_volume(context, connection_info, instance)
1288             except Exception as exc:
1289                 with excutils.save_and_reraise_exception() as ctxt:
1290                     if destroy_disks:
1291                         # Don't block on Volume errors if we're trying to
1292                         # delete the instance as we may be partially created
1293                         # or deleted
1294                         ctxt.reraise = False
1295                         LOG.warning(
1296                             "Ignoring Volume Error on vol %(vol_id)s "
1297                             "during delete %(exc)s",
1298                             {'vol_id': vol.get('volume_id'),
1299                              'exc': encodeutils.exception_to_unicode(exc)},
1300                             instance=instance)
1301 
1302         if destroy_disks:
1303             # NOTE(haomai): destroy volumes if needed
1304             if CONF.libvirt.images_type == 'lvm':
1305                 self._cleanup_lvm(instance, block_device_info)
1306             if CONF.libvirt.images_type == 'rbd':
1307                 self._cleanup_rbd(instance)
1308 
1309         is_shared_block_storage = False
1310         if migrate_data and 'is_shared_block_storage' in migrate_data:
1311             is_shared_block_storage = migrate_data.is_shared_block_storage
1312         # NOTE(lyarwood): The following workaround allows operators to ensure
1313         # that non-shared instance directories are removed after an evacuation
1314         # or revert resize when using the shared RBD imagebackend. This
1315         # workaround is not required when cleaning up migrations that provide
1316         # migrate_data to this method as the existing is_shared_block_storage
1317         # conditional will cause the instance directory to be removed.
1318         if ((destroy_disks or is_shared_block_storage) or
1319             (CONF.workarounds.ensure_libvirt_rbd_instance_dir_cleanup and
1320              CONF.libvirt.images_type == 'rbd')):
1321 
1322             attempts = int(instance.system_metadata.get('clean_attempts',
1323                                                         '0'))
1324             success = self.delete_instance_files(instance)
1325             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1326             # task in the compute manager. The tight coupling is not great...
1327             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1328             if success:
1329                 instance.cleaned = True
1330             instance.save()
1331 
1332         self._undefine_domain(instance)
1333 
1334     def _detach_encrypted_volumes(self, instance, block_device_info):
1335         """Detaches encrypted volumes attached to instance."""
1336         disks = self._get_instance_disk_info(instance, block_device_info)
1337         encrypted_volumes = filter(dmcrypt.is_encrypted,
1338                                    [disk['path'] for disk in disks])
1339         for path in encrypted_volumes:
1340             dmcrypt.delete_volume(path)
1341 
1342     def _get_serial_ports_from_guest(self, guest, mode=None):
1343         """Returns an iterator over serial port(s) configured on guest.
1344 
1345         :param mode: Should be a value in (None, bind, connect)
1346         """
1347         xml = guest.get_xml_desc()
1348         tree = etree.fromstring(xml)
1349 
1350         # The 'serial' device is the base for x86 platforms. Other platforms
1351         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1352         xpath_mode = "[@mode='%s']" % mode if mode else ""
1353         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1354         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1355 
1356         tcp_devices = tree.findall(serial_tcp)
1357         if len(tcp_devices) == 0:
1358             tcp_devices = tree.findall(console_tcp)
1359         for source in tcp_devices:
1360             yield (source.get("host"), int(source.get("service")))
1361 
1362     def _get_scsi_controller_max_unit(self, guest):
1363         """Returns the max disk unit used by scsi controller"""
1364         xml = guest.get_xml_desc()
1365         tree = etree.fromstring(xml)
1366         addrs = "./devices/disk[@device='disk']/address[@type='drive']"
1367 
1368         ret = []
1369         for obj in tree.findall(addrs):
1370             ret.append(int(obj.get('unit', 0)))
1371         return max(ret)
1372 
1373     @staticmethod
1374     def _get_rbd_driver():
1375         return rbd_utils.RBDDriver(
1376                 pool=CONF.libvirt.images_rbd_pool,
1377                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
1378                 rbd_user=CONF.libvirt.rbd_user)
1379 
1380     def _cleanup_rbd(self, instance):
1381         # NOTE(nic): On revert_resize, the cleanup steps for the root
1382         # volume are handled with an "rbd snap rollback" command,
1383         # and none of this is needed (and is, in fact, harmful) so
1384         # filter out non-ephemerals from the list
1385         if instance.task_state == task_states.RESIZE_REVERTING:
1386             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1387                                       disk.endswith('disk.local'))
1388         else:
1389             filter_fn = lambda disk: disk.startswith(instance.uuid)
1390         LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
1391 
1392     def _cleanup_lvm(self, instance, block_device_info):
1393         """Delete all LVM disks for given instance object."""
1394         if instance.get('ephemeral_key_uuid') is not None:
1395             self._detach_encrypted_volumes(instance, block_device_info)
1396 
1397         disks = self._lvm_disks(instance)
1398         if disks:
1399             lvm.remove_volumes(disks)
1400 
1401     def _lvm_disks(self, instance):
1402         """Returns all LVM disks for given instance object."""
1403         if CONF.libvirt.images_volume_group:
1404             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1405             if not os.path.exists(vg):
1406                 return []
1407             pattern = '%s_' % instance.uuid
1408 
1409             def belongs_to_instance(disk):
1410                 return disk.startswith(pattern)
1411 
1412             def fullpath(name):
1413                 return os.path.join(vg, name)
1414 
1415             logical_volumes = lvm.list_volumes(vg)
1416 
1417             disks = [fullpath(disk) for disk in logical_volumes
1418                      if belongs_to_instance(disk)]
1419             return disks
1420         return []
1421 
1422     def get_volume_connector(self, instance):
1423         root_helper = utils.get_root_helper()
1424         return connector.get_connector_properties(
1425             root_helper, CONF.my_block_storage_ip,
1426             CONF.libvirt.volume_use_multipath,
1427             enforce_multipath=True,
1428             host=CONF.host)
1429 
1430     def _cleanup_resize(self, context, instance, network_info):
1431         inst_base = libvirt_utils.get_instance_path(instance)
1432         target = inst_base + '_resize'
1433 
1434         # Deletion can fail over NFS, so retry the deletion as required.
1435         # Set maximum attempt as 5, most test can remove the directory
1436         # for the second time.
1437         attempts = 0
1438         while(os.path.exists(target) and attempts < 5):
1439             shutil.rmtree(target, ignore_errors=True)
1440             if os.path.exists(target):
1441                 time.sleep(random.randint(20, 200) / 100.0)
1442             attempts += 1
1443 
1444         # NOTE(mriedem): Some image backends will recreate the instance path
1445         # and disk.info during init, and all we need the root disk for
1446         # here is removing cloned snapshots which is backend-specific, so
1447         # check that first before initializing the image backend object. If
1448         # there is ever an image type that supports clone *and* re-creates
1449         # the instance directory and disk.info on init, this condition will
1450         # need to be re-visited to make sure that backend doesn't re-create
1451         # the disk. Refer to bugs: 1666831 1728603 1769131
1452         if self.image_backend.backend(CONF.libvirt.images_type).SUPPORTS_CLONE:
1453             root_disk = self.image_backend.by_name(instance, 'disk')
1454             if root_disk.exists():
1455                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
1456 
1457         if instance.host != CONF.host:
1458             self._undefine_domain(instance)
1459             self.unplug_vifs(instance, network_info)
1460             self.unfilter_instance(instance, network_info)
1461 
1462     def _get_volume_driver(self, connection_info):
1463         driver_type = connection_info.get('driver_volume_type')
1464         if driver_type not in self.volume_drivers:
1465             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1466         return self.volume_drivers[driver_type]
1467 
1468     def _connect_volume(self, context, connection_info, instance,
1469                         encryption=None, allow_native_luks=True):
1470         vol_driver = self._get_volume_driver(connection_info)
1471         vol_driver.connect_volume(connection_info, instance)
1472         try:
1473             self._attach_encryptor(
1474                 context, connection_info, encryption, allow_native_luks)
1475         except Exception:
1476             # Encryption failed so rollback the volume connection.
1477             with excutils.save_and_reraise_exception(logger=LOG):
1478                 LOG.exception("Failure attaching encryptor; rolling back "
1479                               "volume connection", instance=instance)
1480                 vol_driver.disconnect_volume(connection_info, instance)
1481 
1482     def _should_disconnect_target(self, context, connection_info, instance):
1483         connection_count = 0
1484 
1485         # NOTE(jdg): Multiattach is a special case (not to be confused
1486         # with shared_targets). With multiattach we may have a single volume
1487         # attached multiple times to *this* compute node (ie Server-1 and
1488         # Server-2).  So, if we receive a call to delete the attachment for
1489         # Server-1 we need to take special care to make sure that the Volume
1490         # isn't also attached to another Server on this Node.  Otherwise we
1491         # will indiscriminantly delete the connection for all Server and that's
1492         # no good.  So check if it's attached multiple times on this node
1493         # if it is we skip the call to brick to delete the connection.
1494         if connection_info.get('multiattach', False):
1495             volume = self._volume_api.get(
1496                 context,
1497                 driver_block_device.get_volume_id(connection_info))
1498             attachments = volume.get('attachments', {})
1499             if len(attachments) > 1:
1500                 # First we get a list of all Server UUID's associated with
1501                 # this Host (Compute Node).  We're going to use this to
1502                 # determine if the Volume being detached is also in-use by
1503                 # another Server on this Host, ie just check to see if more
1504                 # than one attachment.server_id for this volume is in our
1505                 # list of Server UUID's for this Host
1506                 servers_this_host = objects.InstanceList.get_uuids_by_host(
1507                     context, instance.host)
1508 
1509                 # NOTE(jdg): nova.volume.cinder translates the
1510                 # volume['attachments'] response into a dict which includes
1511                 # the Server UUID as the key, so we're using that
1512                 # here to check against our server_this_host list
1513                 for server_id, data in attachments.items():
1514                     if server_id in servers_this_host:
1515                         connection_count += 1
1516         return (False if connection_count > 1 else True)
1517 
1518     def _disconnect_volume(self, context, connection_info, instance,
1519                            encryption=None):
1520         self._detach_encryptor(context, connection_info, encryption=encryption)
1521         if self._should_disconnect_target(context, connection_info, instance):
1522             vol_driver = self._get_volume_driver(connection_info)
1523             vol_driver.disconnect_volume(connection_info, instance)
1524         else:
1525             LOG.info("Detected multiple connections on this host for volume: "
1526                      "%s, skipping target disconnect.",
1527                      driver_block_device.get_volume_id(connection_info),
1528                      instance=instance)
1529 
1530     def _extend_volume(self, connection_info, instance, requested_size):
1531         vol_driver = self._get_volume_driver(connection_info)
1532         return vol_driver.extend_volume(connection_info, instance,
1533                                         requested_size)
1534 
1535     def _use_native_luks(self, encryption=None):
1536         """Is LUKS the required provider and native QEMU LUKS available
1537         """
1538         provider = None
1539         if encryption:
1540             provider = encryption.get('provider', None)
1541         if provider in encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP:
1542             provider = encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP[provider]
1543         return provider == encryptors.LUKS and self._is_native_luks_available()
1544 
1545     def _get_volume_config(self, connection_info, disk_info):
1546         vol_driver = self._get_volume_driver(connection_info)
1547         conf = vol_driver.get_config(connection_info, disk_info)
1548         self._set_cache_mode(conf)
1549         return conf
1550 
1551     def _get_volume_encryptor(self, connection_info, encryption):
1552         root_helper = utils.get_root_helper()
1553         return encryptors.get_volume_encryptor(root_helper=root_helper,
1554                                                keymgr=key_manager.API(CONF),
1555                                                connection_info=connection_info,
1556                                                **encryption)
1557 
1558     def _get_volume_encryption(self, context, connection_info):
1559         """Get the encryption metadata dict if it is not provided
1560         """
1561         encryption = {}
1562         volume_id = driver_block_device.get_volume_id(connection_info)
1563         if volume_id:
1564             encryption = encryptors.get_encryption_metadata(context,
1565                             self._volume_api, volume_id, connection_info)
1566         return encryption
1567 
1568     def _attach_encryptor(self, context, connection_info, encryption,
1569                           allow_native_luks):
1570         """Attach the frontend encryptor if one is required by the volume.
1571 
1572         The request context is only used when an encryption metadata dict is
1573         not provided. The encryption metadata dict being populated is then used
1574         to determine if an attempt to attach the encryptor should be made.
1575 
1576         If native LUKS decryption is enabled then create a Libvirt volume
1577         secret containing the LUKS passphrase for the volume.
1578         """
1579         if encryption is None:
1580             encryption = self._get_volume_encryption(context, connection_info)
1581 
1582         if (encryption and allow_native_luks and
1583             self._use_native_luks(encryption)):
1584             # NOTE(lyarwood): Fetch the associated key for the volume and
1585             # decode the passphrase from the key.
1586             # FIXME(lyarwood): c-vol currently creates symmetric keys for use
1587             # with volumes, leading to the binary to hex to string conversion
1588             # below.
1589             keymgr = key_manager.API(CONF)
1590             key = keymgr.get(context, encryption['encryption_key_id'])
1591             key_encoded = key.get_encoded()
1592             passphrase = binascii.hexlify(key_encoded).decode('utf-8')
1593 
1594             # NOTE(lyarwood): Retain the behaviour of the original os-brick
1595             # encryptors and format any volume that does not identify as
1596             # encrypted with LUKS.
1597             # FIXME(lyarwood): Remove this once c-vol correctly formats
1598             # encrypted volumes during their initial creation:
1599             # https://bugs.launchpad.net/cinder/+bug/1739442
1600             device_path = connection_info.get('data').get('device_path')
1601             if device_path:
1602                 root_helper = utils.get_root_helper()
1603                 if not luks_encryptor.is_luks(root_helper, device_path):
1604                     encryptor = self._get_volume_encryptor(connection_info,
1605                                                            encryption)
1606                     encryptor._format_volume(passphrase, **encryption)
1607 
1608             # NOTE(lyarwood): Store the passphrase as a libvirt secret locally
1609             # on the compute node. This secret is used later when generating
1610             # the volume config.
1611             volume_id = driver_block_device.get_volume_id(connection_info)
1612             self._host.create_secret('volume', volume_id, password=passphrase)
1613         elif encryption:
1614             encryptor = self._get_volume_encryptor(connection_info,
1615                                                    encryption)
1616             encryptor.attach_volume(context, **encryption)
1617 
1618     def _detach_encryptor(self, context, connection_info, encryption):
1619         """Detach the frontend encryptor if one is required by the volume.
1620 
1621         The request context is only used when an encryption metadata dict is
1622         not provided. The encryption metadata dict being populated is then used
1623         to determine if an attempt to detach the encryptor should be made.
1624 
1625         If native LUKS decryption is enabled then delete previously created
1626         Libvirt volume secret from the host.
1627         """
1628         volume_id = driver_block_device.get_volume_id(connection_info)
1629         if volume_id and self._host.find_secret('volume', volume_id):
1630             return self._host.delete_secret('volume', volume_id)
1631         if encryption is None:
1632             encryption = self._get_volume_encryption(context, connection_info)
1633         # NOTE(lyarwood): Handle bug #1821696 where volume secrets have been
1634         # removed manually by returning if native LUKS decryption is available
1635         # and device_path is not present in the connection_info. This avoids
1636         # VolumeEncryptionNotSupported being thrown when we incorrectly build
1637         # the encryptor below due to the secrets not being present above.
1638         if (encryption and self._use_native_luks(encryption) and
1639             not connection_info['data'].get('device_path')):
1640             return
1641         if encryption:
1642             encryptor = self._get_volume_encryptor(connection_info,
1643                                                    encryption)
1644             encryptor.detach_volume(**encryption)
1645 
1646     def _check_discard_for_attach_volume(self, conf, instance):
1647         """Perform some checks for volumes configured for discard support.
1648 
1649         If discard is configured for the volume, and the guest is using a
1650         configuration known to not work, we will log a message explaining
1651         the reason why.
1652         """
1653         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1654             LOG.debug('Attempting to attach volume %(id)s with discard '
1655                       'support enabled to an instance using an '
1656                       'unsupported configuration. target_bus = '
1657                       '%(bus)s. Trim commands will not be issued to '
1658                       'the storage device.',
1659                       {'bus': conf.target_bus,
1660                        'id': conf.serial},
1661                       instance=instance)
1662 
1663     def attach_volume(self, context, connection_info, instance, mountpoint,
1664                       disk_bus=None, device_type=None, encryption=None):
1665         guest = self._host.get_guest(instance)
1666 
1667         disk_dev = mountpoint.rpartition("/")[2]
1668         bdm = {
1669             'device_name': disk_dev,
1670             'disk_bus': disk_bus,
1671             'device_type': device_type}
1672 
1673         # Note(cfb): If the volume has a custom block size, check that
1674         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1675         #            presence of a block size is considered mandatory by
1676         #            cinder so we fail if we can't honor the request.
1677         data = {}
1678         if ('data' in connection_info):
1679             data = connection_info['data']
1680         if ('logical_block_size' in data or 'physical_block_size' in data):
1681             if ((CONF.libvirt.virt_type != "kvm" and
1682                  CONF.libvirt.virt_type != "qemu")):
1683                 msg = _("Volume sets block size, but the current "
1684                         "libvirt hypervisor '%s' does not support custom "
1685                         "block size") % CONF.libvirt.virt_type
1686                 raise exception.InvalidHypervisorType(msg)
1687 
1688         self._connect_volume(context, connection_info, instance,
1689                              encryption=encryption)
1690         disk_info = blockinfo.get_info_from_bdm(
1691             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1692         if disk_info['bus'] == 'scsi':
1693             disk_info['unit'] = self._get_scsi_controller_max_unit(guest) + 1
1694 
1695         conf = self._get_volume_config(connection_info, disk_info)
1696 
1697         self._check_discard_for_attach_volume(conf, instance)
1698 
1699         try:
1700             state = guest.get_power_state(self._host)
1701             live = state in (power_state.RUNNING, power_state.PAUSED)
1702 
1703             guest.attach_device(conf, persistent=True, live=live)
1704             # NOTE(artom) If we're attaching with a device role tag, we need to
1705             # rebuild device_metadata. If we're attaching without a role
1706             # tag, we're rebuilding it here needlessly anyways. This isn't a
1707             # massive deal, and it helps reduce code complexity by not having
1708             # to indicate to the virt driver that the attach is tagged. The
1709             # really important optimization of not calling the database unless
1710             # device_metadata has actually changed is done for us by
1711             # instance.save().
1712             instance.device_metadata = self._build_device_metadata(
1713                 context, instance)
1714             instance.save()
1715 
1716         # TODO(lyarwood) Remove the following breadcrumb once all supported
1717         # distributions provide Libvirt 3.3.0 or earlier with
1718         # https://libvirt.org/git/?p=libvirt.git;a=commit;h=7189099 applied.
1719         except libvirt.libvirtError as ex:
1720             with excutils.save_and_reraise_exception():
1721                 if 'Incorrect number of padding bytes' in six.text_type(ex):
1722                     LOG.warning(_('Failed to attach encrypted volume due to a '
1723                                   'known Libvirt issue, see the following bug '
1724                                   'for details: '
1725                                   'https://bugzilla.redhat.com/1447297'))
1726                 else:
1727                     LOG.exception(_('Failed to attach volume at mountpoint: '
1728                                     '%s'), mountpoint, instance=instance)
1729                 self._disconnect_volume(context, connection_info, instance,
1730                                         encryption=encryption)
1731         except Exception:
1732             LOG.exception(_('Failed to attach volume at mountpoint: %s'),
1733                           mountpoint, instance=instance)
1734             with excutils.save_and_reraise_exception():
1735                 self._disconnect_volume(context, connection_info, instance,
1736                                         encryption=encryption)
1737 
1738     def _swap_volume(self, guest, disk_path, conf, resize_to):
1739         """Swap existing disk with a new block device."""
1740         dev = guest.get_block_device(disk_path)
1741 
1742         # Save a copy of the domain's persistent XML file. We'll use this
1743         # to redefine the domain if anything fails during the volume swap.
1744         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1745 
1746         # Abort is an idempotent operation, so make sure any block
1747         # jobs which may have failed are ended.
1748         try:
1749             dev.abort_job()
1750         except Exception:
1751             pass
1752 
1753         try:
1754             # NOTE (rmk): blockRebase cannot be executed on persistent
1755             #             domains, so we need to temporarily undefine it.
1756             #             If any part of this block fails, the domain is
1757             #             re-defined regardless.
1758             if guest.has_persistent_configuration():
1759                 support_uefi = self._has_uefi_support()
1760                 guest.delete_configuration(support_uefi)
1761 
1762             try:
1763                 # Start copy with VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT flag to
1764                 # allow writing to existing external volume file. Use
1765                 # VIR_DOMAIN_BLOCK_REBASE_COPY_DEV if it's a block device to
1766                 # make sure XML is generated correctly (bug 1691195)
1767                 copy_dev = conf.source_type == 'block'
1768                 dev.rebase(conf.source_path, copy=True, reuse_ext=True,
1769                            copy_dev=copy_dev)
1770                 while not dev.is_job_complete():
1771                     time.sleep(0.5)
1772 
1773                 dev.abort_job(pivot=True)
1774 
1775             except Exception as exc:
1776                 LOG.exception("Failure rebasing volume %(new_path)s on "
1777                     "%(old_path)s.", {'new_path': conf.source_path,
1778                                       'old_path': disk_path})
1779                 raise exception.VolumeRebaseFailed(reason=six.text_type(exc))
1780 
1781             if resize_to:
1782                 dev.resize(resize_to * units.Gi / units.Ki)
1783 
1784             # Make sure we will redefine the domain using the updated
1785             # configuration after the volume was swapped. The dump_inactive
1786             # keyword arg controls whether we pull the inactive (persistent)
1787             # or active (live) config from the domain. We want to pull the
1788             # live config after the volume was updated to use when we redefine
1789             # the domain.
1790             xml = guest.get_xml_desc(dump_inactive=False, dump_sensitive=True)
1791         finally:
1792             self._host.write_instance_config(xml)
1793 
1794     def swap_volume(self, context, old_connection_info,
1795                     new_connection_info, instance, mountpoint, resize_to):
1796 
1797         # NOTE(lyarwood): https://bugzilla.redhat.com/show_bug.cgi?id=760547
1798         old_encrypt = self._get_volume_encryption(context, old_connection_info)
1799         new_encrypt = self._get_volume_encryption(context, new_connection_info)
1800         if ((old_encrypt and self._use_native_luks(old_encrypt)) or
1801             (new_encrypt and self._use_native_luks(new_encrypt))):
1802             raise NotImplementedError(_("Swap volume is not supported for "
1803                 "encrypted volumes when native LUKS decryption is enabled."))
1804 
1805         guest = self._host.get_guest(instance)
1806 
1807         disk_dev = mountpoint.rpartition("/")[2]
1808         if not guest.get_disk(disk_dev):
1809             raise exception.DiskNotFound(location=disk_dev)
1810         disk_info = {
1811             'dev': disk_dev,
1812             'bus': blockinfo.get_disk_bus_for_disk_dev(
1813                 CONF.libvirt.virt_type, disk_dev),
1814             'type': 'disk',
1815             }
1816         # NOTE (lyarwood): new_connection_info will be modified by the
1817         # following _connect_volume call down into the volume drivers. The
1818         # majority of the volume drivers will add a device_path that is in turn
1819         # used by _get_volume_config to set the source_path of the
1820         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
1821         # this to the BDM here as the upper compute swap_volume method will
1822         # eventually do this for us.
1823         self._connect_volume(context, new_connection_info, instance)
1824         conf = self._get_volume_config(new_connection_info, disk_info)
1825         if not conf.source_path:
1826             self._disconnect_volume(context, new_connection_info, instance)
1827             raise NotImplementedError(_("Swap only supports host devices"))
1828 
1829         try:
1830             self._swap_volume(guest, disk_dev, conf, resize_to)
1831         except exception.VolumeRebaseFailed:
1832             with excutils.save_and_reraise_exception():
1833                 self._disconnect_volume(context, new_connection_info, instance)
1834 
1835         self._disconnect_volume(context, old_connection_info, instance)
1836 
1837     def _get_existing_domain_xml(self, instance, network_info,
1838                                  block_device_info=None):
1839         try:
1840             guest = self._host.get_guest(instance)
1841             xml = guest.get_xml_desc()
1842         except exception.InstanceNotFound:
1843             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1844                                                 instance,
1845                                                 instance.image_meta,
1846                                                 block_device_info)
1847             xml = self._get_guest_xml(nova_context.get_admin_context(),
1848                                       instance, network_info, disk_info,
1849                                       instance.image_meta,
1850                                       block_device_info=block_device_info)
1851         return xml
1852 
1853     def detach_volume(self, context, connection_info, instance, mountpoint,
1854                       encryption=None):
1855         disk_dev = mountpoint.rpartition("/")[2]
1856         try:
1857             guest = self._host.get_guest(instance)
1858 
1859             state = guest.get_power_state(self._host)
1860             live = state in (power_state.RUNNING, power_state.PAUSED)
1861             # NOTE(lyarwood): The volume must be detached from the VM before
1862             # detaching any attached encryptors or disconnecting the underlying
1863             # volume in _disconnect_volume. Otherwise, the encryptor or volume
1864             # driver may report that the volume is still in use.
1865             wait_for_detach = guest.detach_device_with_retry(guest.get_disk,
1866                                                              disk_dev,
1867                                                              live=live)
1868             wait_for_detach()
1869 
1870         except exception.InstanceNotFound:
1871             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1872             #                will throw InstanceNotFound exception. Need to
1873             #                disconnect volume under this circumstance.
1874             LOG.warning("During detach_volume, instance disappeared.",
1875                         instance=instance)
1876         except exception.DeviceNotFound:
1877             # We should still try to disconnect logical device from
1878             # host, an error might have happened during a previous
1879             # call.
1880             LOG.info("Device %s not found in instance.",
1881                      disk_dev, instance=instance)
1882         except libvirt.libvirtError as ex:
1883             # NOTE(vish): This is called to cleanup volumes after live
1884             #             migration, so we should still disconnect even if
1885             #             the instance doesn't exist here anymore.
1886             error_code = ex.get_error_code()
1887             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1888                 # NOTE(vish):
1889                 LOG.warning("During detach_volume, instance disappeared.",
1890                             instance=instance)
1891             else:
1892                 raise
1893 
1894         self._disconnect_volume(context, connection_info, instance,
1895                                 encryption=encryption)
1896 
1897     def extend_volume(self, connection_info, instance, requested_size):
1898         try:
1899             new_size = self._extend_volume(connection_info, instance,
1900                                            requested_size)
1901         except NotImplementedError:
1902             raise exception.ExtendVolumeNotSupported()
1903 
1904         # Resize the device in QEMU so its size is updated and
1905         # detected by the instance without rebooting.
1906         try:
1907             guest = self._host.get_guest(instance)
1908             state = guest.get_power_state(self._host)
1909             active_state = state in (power_state.RUNNING, power_state.PAUSED)
1910             if active_state:
1911                 if 'device_path' in connection_info['data']:
1912                     disk_path = connection_info['data']['device_path']
1913                 else:
1914                     # Some drivers (eg. net) don't put the device_path
1915                     # into the connection_info. Match disks by their serial
1916                     # number instead
1917                     volume_id = driver_block_device.get_volume_id(
1918                         connection_info)
1919                     disk = next(iter([
1920                         d for d in guest.get_all_disks()
1921                         if d.serial == volume_id
1922                     ]), None)
1923                     if not disk:
1924                         raise exception.VolumeNotFound(volume_id=volume_id)
1925                     disk_path = disk.target_dev
1926 
1927                 LOG.debug('resizing block device %(dev)s to %(size)u kb',
1928                           {'dev': disk_path, 'size': new_size})
1929                 dev = guest.get_block_device(disk_path)
1930                 dev.resize(new_size // units.Ki)
1931             else:
1932                 LOG.debug('Skipping block device resize, guest is not running',
1933                           instance=instance)
1934         except exception.InstanceNotFound:
1935             with excutils.save_and_reraise_exception():
1936                 LOG.warning('During extend_volume, instance disappeared.',
1937                             instance=instance)
1938         except libvirt.libvirtError:
1939             with excutils.save_and_reraise_exception():
1940                 LOG.exception('resizing block device failed.',
1941                               instance=instance)
1942 
1943     def attach_interface(self, context, instance, image_meta, vif):
1944         guest = self._host.get_guest(instance)
1945 
1946         self.vif_driver.plug(instance, vif)
1947         self.firewall_driver.setup_basic_filtering(instance, [vif])
1948         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1949                                          instance.flavor,
1950                                          CONF.libvirt.virt_type,
1951                                          self._host)
1952         try:
1953             state = guest.get_power_state(self._host)
1954             live = state in (power_state.RUNNING, power_state.PAUSED)
1955             guest.attach_device(cfg, persistent=True, live=live)
1956         except libvirt.libvirtError:
1957             LOG.error('attaching network adapter failed.',
1958                       instance=instance, exc_info=True)
1959             self.vif_driver.unplug(instance, vif)
1960             raise exception.InterfaceAttachFailed(
1961                     instance_uuid=instance.uuid)
1962         try:
1963             # NOTE(artom) If we're attaching with a device role tag, we need to
1964             # rebuild device_metadata. If we're attaching without a role
1965             # tag, we're rebuilding it here needlessly anyways. This isn't a
1966             # massive deal, and it helps reduce code complexity by not having
1967             # to indicate to the virt driver that the attach is tagged. The
1968             # really important optimization of not calling the database unless
1969             # device_metadata has actually changed is done for us by
1970             # instance.save().
1971             instance.device_metadata = self._build_device_metadata(
1972                 context, instance)
1973             instance.save()
1974         except Exception:
1975             # NOTE(artom) If we fail here it means the interface attached
1976             # successfully but building and/or saving the device metadata
1977             # failed. Just unplugging the vif is therefore not enough cleanup,
1978             # we need to detach the interface.
1979             with excutils.save_and_reraise_exception(reraise=False):
1980                 LOG.error('Interface attached successfully but building '
1981                           'and/or saving device metadata failed.',
1982                           instance=instance, exc_info=True)
1983                 self.detach_interface(context, instance, vif)
1984                 raise exception.InterfaceAttachFailed(
1985                     instance_uuid=instance.uuid)
1986 
1987     def detach_interface(self, context, instance, vif):
1988         guest = self._host.get_guest(instance)
1989         cfg = self.vif_driver.get_config(instance, vif,
1990                                          instance.image_meta,
1991                                          instance.flavor,
1992                                          CONF.libvirt.virt_type, self._host)
1993         interface = guest.get_interface_by_cfg(cfg)
1994         try:
1995             self.vif_driver.unplug(instance, vif)
1996             # NOTE(mriedem): When deleting an instance and using Neutron,
1997             # we can be racing against Neutron deleting the port and
1998             # sending the vif-deleted event which then triggers a call to
1999             # detach the interface, so if the interface is not found then
2000             # we can just log it as a warning.
2001             if not interface:
2002                 mac = vif.get('address')
2003                 # The interface is gone so just log it as a warning.
2004                 LOG.warning('Detaching interface %(mac)s failed because '
2005                             'the device is no longer found on the guest.',
2006                             {'mac': mac}, instance=instance)
2007                 return
2008 
2009             state = guest.get_power_state(self._host)
2010             live = state in (power_state.RUNNING, power_state.PAUSED)
2011             # Now we are going to loop until the interface is detached or we
2012             # timeout.
2013             wait_for_detach = guest.detach_device_with_retry(
2014                 guest.get_interface_by_cfg, cfg, live=live,
2015                 alternative_device_name=self.vif_driver.get_vif_devname(vif))
2016             wait_for_detach()
2017         except exception.DeviceDetachFailed:
2018             # We failed to detach the device even with the retry loop, so let's
2019             # dump some debug information to the logs before raising back up.
2020             with excutils.save_and_reraise_exception():
2021                 devname = self.vif_driver.get_vif_devname(vif)
2022                 interface = guest.get_interface_by_cfg(cfg)
2023                 if interface:
2024                     LOG.warning(
2025                         'Failed to detach interface %(devname)s after '
2026                         'repeated attempts. Final interface xml:\n'
2027                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
2028                         {'devname': devname,
2029                          'interface_xml': interface.to_xml(),
2030                          'guest_xml': guest.get_xml_desc()},
2031                         instance=instance)
2032         except exception.DeviceNotFound:
2033             # The interface is gone so just log it as a warning.
2034             LOG.warning('Detaching interface %(mac)s failed because '
2035                         'the device is no longer found on the guest.',
2036                         {'mac': vif.get('address')}, instance=instance)
2037         except libvirt.libvirtError as ex:
2038             error_code = ex.get_error_code()
2039             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2040                 LOG.warning("During detach_interface, instance disappeared.",
2041                             instance=instance)
2042             else:
2043                 # NOTE(mriedem): When deleting an instance and using Neutron,
2044                 # we can be racing against Neutron deleting the port and
2045                 # sending the vif-deleted event which then triggers a call to
2046                 # detach the interface, so we might have failed because the
2047                 # network device no longer exists. Libvirt will fail with
2048                 # "operation failed: no matching network device was found"
2049                 # which unfortunately does not have a unique error code so we
2050                 # need to look up the interface by config and if it's not found
2051                 # then we can just log it as a warning rather than tracing an
2052                 # error.
2053                 mac = vif.get('address')
2054                 interface = guest.get_interface_by_cfg(cfg)
2055                 if interface:
2056                     LOG.error('detaching network adapter failed.',
2057                               instance=instance, exc_info=True)
2058                     raise exception.InterfaceDetachFailed(
2059                             instance_uuid=instance.uuid)
2060 
2061                 # The interface is gone so just log it as a warning.
2062                 LOG.warning('Detaching interface %(mac)s failed because '
2063                             'the device is no longer found on the guest.',
2064                             {'mac': mac}, instance=instance)
2065 
2066     def _create_snapshot_metadata(self, image_meta, instance,
2067                                   img_fmt, snp_name):
2068         metadata = {'status': 'active',
2069                     'name': snp_name,
2070                     'properties': {
2071                                    'kernel_id': instance.kernel_id,
2072                                    'image_location': 'snapshot',
2073                                    'image_state': 'available',
2074                                    'owner_id': instance.project_id,
2075                                    'ramdisk_id': instance.ramdisk_id,
2076                                    }
2077                     }
2078         if instance.os_type:
2079             metadata['properties']['os_type'] = instance.os_type
2080 
2081         # NOTE(vish): glance forces ami disk format to be ami
2082         if image_meta.disk_format == 'ami':
2083             metadata['disk_format'] = 'ami'
2084         else:
2085             metadata['disk_format'] = img_fmt
2086 
2087         if image_meta.obj_attr_is_set("container_format"):
2088             metadata['container_format'] = image_meta.container_format
2089         else:
2090             metadata['container_format'] = "bare"
2091 
2092         return metadata
2093 
2094     def snapshot(self, context, instance, image_id, update_task_state):
2095         """Create snapshot from a running VM instance.
2096 
2097         This command only works with qemu 0.14+
2098         """
2099         try:
2100             guest = self._host.get_guest(instance)
2101 
2102             # TODO(sahid): We are converting all calls from a
2103             # virDomain object to use nova.virt.libvirt.Guest.
2104             # We should be able to remove virt_dom at the end.
2105             virt_dom = guest._domain
2106         except exception.InstanceNotFound:
2107             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2108 
2109         snapshot = self._image_api.get(context, image_id)
2110 
2111         # source_format is an on-disk format
2112         # source_type is a backend type
2113         disk_path, source_format = libvirt_utils.find_disk(guest)
2114         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
2115 
2116         # We won't have source_type for raw or qcow2 disks, because we can't
2117         # determine that from the path. We should have it from the libvirt
2118         # xml, though.
2119         if source_type is None:
2120             source_type = source_format
2121         # For lxc instances we won't have it either from libvirt xml
2122         # (because we just gave libvirt the mounted filesystem), or the path,
2123         # so source_type is still going to be None. In this case,
2124         # root_disk is going to default to CONF.libvirt.images_type
2125         # below, which is still safe.
2126 
2127         image_format = CONF.libvirt.snapshot_image_format or source_type
2128 
2129         # NOTE(bfilippov): save lvm and rbd as raw
2130         if image_format == 'lvm' or image_format == 'rbd':
2131             image_format = 'raw'
2132 
2133         metadata = self._create_snapshot_metadata(instance.image_meta,
2134                                                   instance,
2135                                                   image_format,
2136                                                   snapshot['name'])
2137 
2138         snapshot_name = uuidutils.generate_uuid(dashed=False)
2139 
2140         state = guest.get_power_state(self._host)
2141 
2142         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
2143         #               cold snapshots. Currently, checking for encryption is
2144         #               redundant because LVM supports only cold snapshots.
2145         #               It is necessary in case this situation changes in the
2146         #               future.
2147         if (self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU) and
2148                 source_type != 'lvm' and
2149                 not CONF.ephemeral_storage_encryption.enabled and
2150                 not CONF.workarounds.disable_libvirt_livesnapshot and
2151                 # NOTE(rmk): We cannot perform live snapshots when a
2152                 # managedSave file is present, so we will use the cold/legacy
2153                 # method for instances which are shutdown or paused.
2154                 # NOTE(mriedem): Live snapshot doesn't work with paused
2155                 # instances on older versions of libvirt/qemu. We can likely
2156                 # remove the restriction on PAUSED once we require
2157                 # libvirt>=3.6.0 and qemu>=2.10 since that works with the
2158                 # Pike Ubuntu Cloud Archive testing in Queens.
2159                 state not in (power_state.SHUTDOWN, power_state.PAUSED)):
2160             live_snapshot = True
2161             # Abort is an idempotent operation, so make sure any block
2162             # jobs which may have failed are ended. This operation also
2163             # confirms the running instance, as opposed to the system as a
2164             # whole, has a new enough version of the hypervisor (bug 1193146).
2165             try:
2166                 guest.get_block_device(disk_path).abort_job()
2167             except libvirt.libvirtError as ex:
2168                 error_code = ex.get_error_code()
2169                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
2170                     live_snapshot = False
2171                 else:
2172                     pass
2173         else:
2174             live_snapshot = False
2175 
2176         self._prepare_domain_for_snapshot(context, live_snapshot, state,
2177                                           instance)
2178 
2179         root_disk = self.image_backend.by_libvirt_path(
2180             instance, disk_path, image_type=source_type)
2181 
2182         if live_snapshot:
2183             LOG.info("Beginning live snapshot process", instance=instance)
2184         else:
2185             LOG.info("Beginning cold snapshot process", instance=instance)
2186 
2187         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
2188 
2189         update_task_state(task_state=task_states.IMAGE_UPLOADING,
2190                           expected_state=task_states.IMAGE_PENDING_UPLOAD)
2191 
2192         try:
2193             metadata['location'] = root_disk.direct_snapshot(
2194                 context, snapshot_name, image_format, image_id,
2195                 instance.image_ref)
2196             self._snapshot_domain(context, live_snapshot, virt_dom, state,
2197                                   instance)
2198             self._image_api.update(context, image_id, metadata,
2199                                    purge_props=False)
2200         except (NotImplementedError, exception.ImageUnacceptable,
2201                 exception.Forbidden) as e:
2202             if type(e) != NotImplementedError:
2203                 LOG.warning('Performing standard snapshot because direct '
2204                             'snapshot failed: %(error)s',
2205                             {'error': encodeutils.exception_to_unicode(e)})
2206             failed_snap = metadata.pop('location', None)
2207             if failed_snap:
2208                 failed_snap = {'url': str(failed_snap)}
2209             root_disk.cleanup_direct_snapshot(failed_snap,
2210                                                   also_destroy_volume=True,
2211                                                   ignore_errors=True)
2212             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
2213                               expected_state=task_states.IMAGE_UPLOADING)
2214 
2215             # TODO(nic): possibly abstract this out to the root_disk
2216             if source_type == 'rbd' and live_snapshot:
2217                 # Standard snapshot uses qemu-img convert from RBD which is
2218                 # not safe to run with live_snapshot.
2219                 live_snapshot = False
2220                 # Suspend the guest, so this is no longer a live snapshot
2221                 self._prepare_domain_for_snapshot(context, live_snapshot,
2222                                                   state, instance)
2223 
2224             snapshot_directory = CONF.libvirt.snapshots_directory
2225             fileutils.ensure_tree(snapshot_directory)
2226             with utils.tempdir(dir=snapshot_directory) as tmpdir:
2227                 try:
2228                     out_path = os.path.join(tmpdir, snapshot_name)
2229                     if live_snapshot:
2230                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
2231                         os.chmod(tmpdir, 0o701)
2232                         self._live_snapshot(context, instance, guest,
2233                                             disk_path, out_path, source_format,
2234                                             image_format, instance.image_meta)
2235                     else:
2236                         root_disk.snapshot_extract(out_path, image_format)
2237                     LOG.info("Snapshot extracted, beginning image upload",
2238                              instance=instance)
2239                 except libvirt.libvirtError as ex:
2240                     error_code = ex.get_error_code()
2241                     if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2242                         LOG.info('Instance %(instance_name)s disappeared '
2243                                  'while taking snapshot of it: [Error Code '
2244                                  '%(error_code)s] %(ex)s',
2245                                  {'instance_name': instance.name,
2246                                   'error_code': error_code,
2247                                   'ex': ex},
2248                                  instance=instance)
2249                         raise exception.InstanceNotFound(
2250                             instance_id=instance.uuid)
2251                     else:
2252                         raise
2253                 finally:
2254                     self._snapshot_domain(context, live_snapshot, virt_dom,
2255                                           state, instance)
2256 
2257                 # Upload that image to the image service
2258                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
2259                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
2260                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
2261                     # execute operation with disk concurrency semaphore
2262                     with compute_utils.disk_ops_semaphore:
2263                         self._image_api.update(context,
2264                                                image_id,
2265                                                metadata,
2266                                                image_file)
2267         except Exception:
2268             with excutils.save_and_reraise_exception():
2269                 LOG.exception(_("Failed to snapshot image"))
2270                 failed_snap = metadata.pop('location', None)
2271                 if failed_snap:
2272                     failed_snap = {'url': str(failed_snap)}
2273                 root_disk.cleanup_direct_snapshot(
2274                         failed_snap, also_destroy_volume=True,
2275                         ignore_errors=True)
2276 
2277         LOG.info("Snapshot image upload complete", instance=instance)
2278 
2279     def _prepare_domain_for_snapshot(self, context, live_snapshot, state,
2280                                      instance):
2281         # NOTE(dkang): managedSave does not work for LXC
2282         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
2283             if state == power_state.RUNNING or state == power_state.PAUSED:
2284                 self.suspend(context, instance)
2285 
2286     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
2287                          instance):
2288         guest = None
2289         # NOTE(dkang): because previous managedSave is not called
2290         #              for LXC, _create_domain must not be called.
2291         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
2292             if state == power_state.RUNNING:
2293                 guest = self._create_domain(domain=virt_dom)
2294             elif state == power_state.PAUSED:
2295                 guest = self._create_domain(domain=virt_dom, pause=True)
2296 
2297             if guest is not None:
2298                 self._attach_pci_devices(
2299                     guest, pci_manager.get_instance_pci_devs(instance))
2300                 self._attach_direct_passthrough_ports(
2301                     context, instance, guest)
2302 
2303     def _can_set_admin_password(self, image_meta):
2304 
2305         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
2306             if not image_meta.properties.get('hw_qemu_guest_agent', False):
2307                 raise exception.QemuGuestAgentNotEnabled()
2308         elif not CONF.libvirt.virt_type == 'parallels':
2309             raise exception.SetAdminPasswdNotSupported()
2310 
2311     # TODO(melwitt): Combine this with the similar xenapi code at some point.
2312     def _save_instance_password_if_sshkey_present(self, instance, new_pass):
2313         sshkey = instance.key_data if 'key_data' in instance else None
2314         if sshkey and sshkey.startswith("ssh-rsa"):
2315             enc = crypto.ssh_encrypt_text(sshkey, new_pass)
2316             # NOTE(melwitt): The convert_password method doesn't actually do
2317             # anything with the context argument, so we can pass None.
2318             instance.system_metadata.update(
2319                 password.convert_password(None, base64.encode_as_text(enc)))
2320             instance.save()
2321 
2322     def set_admin_password(self, instance, new_pass):
2323         self._can_set_admin_password(instance.image_meta)
2324 
2325         guest = self._host.get_guest(instance)
2326         user = instance.image_meta.properties.get("os_admin_user")
2327         if not user:
2328             if instance.os_type == "windows":
2329                 user = "Administrator"
2330             else:
2331                 user = "root"
2332         try:
2333             guest.set_user_password(user, new_pass)
2334         except libvirt.libvirtError as ex:
2335             error_code = ex.get_error_code()
2336             if error_code == libvirt.VIR_ERR_AGENT_UNRESPONSIVE:
2337                 LOG.debug('Failed to set password: QEMU agent unresponsive',
2338                           instance_uuid=instance.uuid)
2339                 raise NotImplementedError()
2340 
2341             err_msg = encodeutils.exception_to_unicode(ex)
2342             msg = (_('Error from libvirt while set password for username '
2343                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
2344                    % {'user': user, 'error_code': error_code, 'ex': err_msg})
2345             raise exception.InternalError(msg)
2346         else:
2347             # Save the password in sysmeta so it may be retrieved from the
2348             # metadata service.
2349             self._save_instance_password_if_sshkey_present(instance, new_pass)
2350 
2351     def _can_quiesce(self, instance, image_meta):
2352         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
2353             raise exception.InstanceQuiesceNotSupported(
2354                 instance_id=instance.uuid)
2355 
2356         if not image_meta.properties.get('hw_qemu_guest_agent', False):
2357             raise exception.QemuGuestAgentNotEnabled()
2358 
2359     def _requires_quiesce(self, image_meta):
2360         return image_meta.properties.get('os_require_quiesce', False)
2361 
2362     def _set_quiesced(self, context, instance, image_meta, quiesced):
2363         self._can_quiesce(instance, image_meta)
2364         try:
2365             guest = self._host.get_guest(instance)
2366             if quiesced:
2367                 guest.freeze_filesystems()
2368             else:
2369                 guest.thaw_filesystems()
2370         except libvirt.libvirtError as ex:
2371             error_code = ex.get_error_code()
2372             err_msg = encodeutils.exception_to_unicode(ex)
2373             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
2374                      '[Error Code %(error_code)s] %(ex)s')
2375                    % {'instance_name': instance.name,
2376                       'error_code': error_code, 'ex': err_msg})
2377             raise exception.InternalError(msg)
2378 
2379     def quiesce(self, context, instance, image_meta):
2380         """Freeze the guest filesystems to prepare for snapshot.
2381 
2382         The qemu-guest-agent must be setup to execute fsfreeze.
2383         """
2384         self._set_quiesced(context, instance, image_meta, True)
2385 
2386     def unquiesce(self, context, instance, image_meta):
2387         """Thaw the guest filesystems after snapshot."""
2388         self._set_quiesced(context, instance, image_meta, False)
2389 
2390     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
2391                        source_format, image_format, image_meta):
2392         """Snapshot an instance without downtime."""
2393         dev = guest.get_block_device(disk_path)
2394 
2395         # Save a copy of the domain's persistent XML file
2396         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
2397 
2398         # Abort is an idempotent operation, so make sure any block
2399         # jobs which may have failed are ended.
2400         try:
2401             dev.abort_job()
2402         except Exception:
2403             pass
2404 
2405         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
2406         #             in QEMU 1.3. In order to do this, we need to create
2407         #             a destination image with the original backing file
2408         #             and matching size of the instance root disk.
2409         src_disk_size = libvirt_utils.get_disk_size(disk_path,
2410                                                     format=source_format)
2411         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
2412                                                         format=source_format,
2413                                                         basename=False)
2414         disk_delta = out_path + '.delta'
2415         libvirt_utils.create_cow_image(src_back_path, disk_delta,
2416                                        src_disk_size)
2417 
2418         quiesced = False
2419         try:
2420             self._set_quiesced(context, instance, image_meta, True)
2421             quiesced = True
2422         except exception.NovaException as err:
2423             if self._requires_quiesce(image_meta):
2424                 raise
2425             LOG.info('Skipping quiescing instance: %(reason)s.',
2426                      {'reason': err}, instance=instance)
2427 
2428         try:
2429             # NOTE (rmk): blockRebase cannot be executed on persistent
2430             #             domains, so we need to temporarily undefine it.
2431             #             If any part of this block fails, the domain is
2432             #             re-defined regardless.
2433             if guest.has_persistent_configuration():
2434                 support_uefi = self._has_uefi_support()
2435                 guest.delete_configuration(support_uefi)
2436 
2437             # NOTE (rmk): Establish a temporary mirror of our root disk and
2438             #             issue an abort once we have a complete copy.
2439             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
2440 
2441             while not dev.is_job_complete():
2442                 time.sleep(0.5)
2443 
2444             dev.abort_job()
2445             nova.privsep.path.chown(disk_delta, uid=os.getuid())
2446         finally:
2447             self._host.write_instance_config(xml)
2448             if quiesced:
2449                 self._set_quiesced(context, instance, image_meta, False)
2450 
2451         # Convert the delta (CoW) image with a backing file to a flat
2452         # image with no backing file.
2453         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
2454                                        out_path, image_format)
2455 
2456     def _volume_snapshot_update_status(self, context, snapshot_id, status):
2457         """Send a snapshot status update to Cinder.
2458 
2459         This method captures and logs exceptions that occur
2460         since callers cannot do anything useful with these exceptions.
2461 
2462         Operations on the Cinder side waiting for this will time out if
2463         a failure occurs sending the update.
2464 
2465         :param context: security context
2466         :param snapshot_id: id of snapshot being updated
2467         :param status: new status value
2468 
2469         """
2470 
2471         try:
2472             self._volume_api.update_snapshot_status(context,
2473                                                     snapshot_id,
2474                                                     status)
2475         except Exception:
2476             LOG.exception(_('Failed to send updated snapshot status '
2477                             'to volume service.'))
2478 
2479     def _volume_snapshot_create(self, context, instance, guest,
2480                                 volume_id, new_file):
2481         """Perform volume snapshot.
2482 
2483            :param guest: VM that volume is attached to
2484            :param volume_id: volume UUID to snapshot
2485            :param new_file: relative path to new qcow2 file present on share
2486 
2487         """
2488         xml = guest.get_xml_desc()
2489         xml_doc = etree.fromstring(xml)
2490 
2491         device_info = vconfig.LibvirtConfigGuest()
2492         device_info.parse_dom(xml_doc)
2493 
2494         disks_to_snap = []          # to be snapshotted by libvirt
2495         network_disks_to_snap = []  # network disks (netfs, etc.)
2496         disks_to_skip = []          # local disks not snapshotted
2497 
2498         for guest_disk in device_info.devices:
2499             if (guest_disk.root_name != 'disk'):
2500                 continue
2501 
2502             if (guest_disk.target_dev is None):
2503                 continue
2504 
2505             if (guest_disk.serial is None or guest_disk.serial != volume_id):
2506                 disks_to_skip.append(guest_disk.target_dev)
2507                 continue
2508 
2509             # disk is a Cinder volume with the correct volume_id
2510 
2511             disk_info = {
2512                 'dev': guest_disk.target_dev,
2513                 'serial': guest_disk.serial,
2514                 'current_file': guest_disk.source_path,
2515                 'source_protocol': guest_disk.source_protocol,
2516                 'source_name': guest_disk.source_name,
2517                 'source_hosts': guest_disk.source_hosts,
2518                 'source_ports': guest_disk.source_ports
2519             }
2520 
2521             # Determine path for new_file based on current path
2522             if disk_info['current_file'] is not None:
2523                 current_file = disk_info['current_file']
2524                 new_file_path = os.path.join(os.path.dirname(current_file),
2525                                              new_file)
2526                 disks_to_snap.append((current_file, new_file_path))
2527             # NOTE(mriedem): This used to include a check for gluster in
2528             # addition to netfs since they were added together. Support for
2529             # gluster was removed in the 16.0.0 Pike release. It is unclear,
2530             # however, if other volume drivers rely on the netfs disk source
2531             # protocol.
2532             elif disk_info['source_protocol'] == 'netfs':
2533                 network_disks_to_snap.append((disk_info, new_file))
2534 
2535         if not disks_to_snap and not network_disks_to_snap:
2536             msg = _('Found no disk to snapshot.')
2537             raise exception.InternalError(msg)
2538 
2539         snapshot = vconfig.LibvirtConfigGuestSnapshot()
2540 
2541         for current_name, new_filename in disks_to_snap:
2542             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2543             snap_disk.name = current_name
2544             snap_disk.source_path = new_filename
2545             snap_disk.source_type = 'file'
2546             snap_disk.snapshot = 'external'
2547             snap_disk.driver_name = 'qcow2'
2548 
2549             snapshot.add_disk(snap_disk)
2550 
2551         for disk_info, new_filename in network_disks_to_snap:
2552             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2553             snap_disk.name = disk_info['dev']
2554             snap_disk.source_type = 'network'
2555             snap_disk.source_protocol = disk_info['source_protocol']
2556             snap_disk.snapshot = 'external'
2557             snap_disk.source_path = new_filename
2558             old_dir = disk_info['source_name'].split('/')[0]
2559             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
2560             snap_disk.source_hosts = disk_info['source_hosts']
2561             snap_disk.source_ports = disk_info['source_ports']
2562 
2563             snapshot.add_disk(snap_disk)
2564 
2565         for dev in disks_to_skip:
2566             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2567             snap_disk.name = dev
2568             snap_disk.snapshot = 'no'
2569 
2570             snapshot.add_disk(snap_disk)
2571 
2572         snapshot_xml = snapshot.to_xml()
2573         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
2574 
2575         image_meta = instance.image_meta
2576         try:
2577             # Check to see if we can quiesce the guest before taking the
2578             # snapshot.
2579             self._can_quiesce(instance, image_meta)
2580             try:
2581                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2582                                reuse_ext=True, quiesce=True)
2583                 return
2584             except libvirt.libvirtError:
2585                 # If the image says that quiesce is required then we fail.
2586                 if self._requires_quiesce(image_meta):
2587                     raise
2588                 LOG.exception(_('Unable to create quiesced VM snapshot, '
2589                                 'attempting again with quiescing disabled.'),
2590                               instance=instance)
2591         except (exception.InstanceQuiesceNotSupported,
2592                 exception.QemuGuestAgentNotEnabled) as err:
2593             # If the image says that quiesce is required then we need to fail.
2594             if self._requires_quiesce(image_meta):
2595                 raise
2596             LOG.info('Skipping quiescing instance: %(reason)s.',
2597                      {'reason': err}, instance=instance)
2598 
2599         try:
2600             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2601                            reuse_ext=True, quiesce=False)
2602         except libvirt.libvirtError:
2603             LOG.exception(_('Unable to create VM snapshot, '
2604                             'failing volume_snapshot operation.'),
2605                           instance=instance)
2606 
2607             raise
2608 
2609     def _volume_refresh_connection_info(self, context, instance, volume_id):
2610         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
2611                   context, volume_id, instance.uuid)
2612 
2613         driver_bdm = driver_block_device.convert_volume(bdm)
2614         if driver_bdm:
2615             driver_bdm.refresh_connection_info(context, instance,
2616                                                self._volume_api, self)
2617 
2618     def volume_snapshot_create(self, context, instance, volume_id,
2619                                create_info):
2620         """Create snapshots of a Cinder volume via libvirt.
2621 
2622         :param instance: VM instance object reference
2623         :param volume_id: id of volume being snapshotted
2624         :param create_info: dict of information used to create snapshots
2625                      - snapshot_id : ID of snapshot
2626                      - type : qcow2 / <other>
2627                      - new_file : qcow2 file created by Cinder which
2628                      becomes the VM's active image after
2629                      the snapshot is complete
2630         """
2631 
2632         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
2633                   {'c_info': create_info}, instance=instance)
2634 
2635         try:
2636             guest = self._host.get_guest(instance)
2637         except exception.InstanceNotFound:
2638             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2639 
2640         if create_info['type'] != 'qcow2':
2641             msg = _('Unknown type: %s') % create_info['type']
2642             raise exception.InternalError(msg)
2643 
2644         snapshot_id = create_info.get('snapshot_id', None)
2645         if snapshot_id is None:
2646             msg = _('snapshot_id required in create_info')
2647             raise exception.InternalError(msg)
2648 
2649         try:
2650             self._volume_snapshot_create(context, instance, guest,
2651                                          volume_id, create_info['new_file'])
2652         except Exception:
2653             with excutils.save_and_reraise_exception():
2654                 LOG.exception(_('Error occurred during '
2655                                 'volume_snapshot_create, '
2656                                 'sending error status to Cinder.'),
2657                               instance=instance)
2658                 self._volume_snapshot_update_status(
2659                     context, snapshot_id, 'error')
2660 
2661         self._volume_snapshot_update_status(
2662             context, snapshot_id, 'creating')
2663 
2664         def _wait_for_snapshot():
2665             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
2666 
2667             if snapshot.get('status') != 'creating':
2668                 self._volume_refresh_connection_info(context, instance,
2669                                                      volume_id)
2670                 raise loopingcall.LoopingCallDone()
2671 
2672         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
2673         timer.start(interval=0.5).wait()
2674 
2675     @staticmethod
2676     def _rebase_with_qemu_img(guest, device, active_disk_object,
2677                               rebase_base):
2678         """Rebase a device tied to a guest using qemu-img.
2679 
2680         :param guest:the Guest which owns the device being rebased
2681         :type guest: nova.virt.libvirt.guest.Guest
2682         :param device: the guest block device to rebase
2683         :type device: nova.virt.libvirt.guest.BlockDevice
2684         :param active_disk_object: the guest block device to rebase
2685         :type active_disk_object: nova.virt.libvirt.config.\
2686                                     LibvirtConfigGuestDisk
2687         :param rebase_base: the new parent in the backing chain
2688         :type rebase_base: None or string
2689         """
2690 
2691         # It's unsure how well qemu-img handles network disks for
2692         # every protocol. So let's be safe.
2693         active_protocol = active_disk_object.source_protocol
2694         if active_protocol is not None:
2695             msg = _("Something went wrong when deleting a volume snapshot: "
2696                     "rebasing a %(protocol)s network disk using qemu-img "
2697                     "has not been fully tested") % {'protocol':
2698                     active_protocol}
2699             LOG.error(msg)
2700             raise exception.InternalError(msg)
2701 
2702         if rebase_base is None:
2703             # If backing_file is specified as "" (the empty string), then
2704             # the image is rebased onto no backing file (i.e. it will exist
2705             # independently of any backing file).
2706             backing_file = ""
2707             qemu_img_extra_arg = []
2708         else:
2709             # If the rebased image is going to have a backing file then
2710             # explicitly set the backing file format to avoid any security
2711             # concerns related to file format auto detection.
2712             backing_file = rebase_base
2713             b_file_fmt = images.qemu_img_info(backing_file).file_format
2714             qemu_img_extra_arg = ['-F', b_file_fmt]
2715 
2716         qemu_img_extra_arg.append(active_disk_object.source_path)
2717         # execute operation with disk concurrency semaphore
2718         with compute_utils.disk_ops_semaphore:
2719             processutils.execute("qemu-img", "rebase", "-b", backing_file,
2720                                  *qemu_img_extra_arg)
2721 
2722     def _volume_snapshot_delete(self, context, instance, volume_id,
2723                                 snapshot_id, delete_info=None):
2724         """Note:
2725             if file being merged into == active image:
2726                 do a blockRebase (pull) operation
2727             else:
2728                 do a blockCommit operation
2729             Files must be adjacent in snap chain.
2730 
2731         :param instance: instance object reference
2732         :param volume_id: volume UUID
2733         :param snapshot_id: snapshot UUID (unused currently)
2734         :param delete_info: {
2735             'type':              'qcow2',
2736             'file_to_merge':     'a.img',
2737             'merge_target_file': 'b.img' or None (if merging file_to_merge into
2738                                                   active image)
2739           }
2740         """
2741 
2742         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
2743                   instance=instance)
2744 
2745         if delete_info['type'] != 'qcow2':
2746             msg = _('Unknown delete_info type %s') % delete_info['type']
2747             raise exception.InternalError(msg)
2748 
2749         try:
2750             guest = self._host.get_guest(instance)
2751         except exception.InstanceNotFound:
2752             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2753 
2754         # Find dev name
2755         my_dev = None
2756         active_disk = None
2757 
2758         xml = guest.get_xml_desc()
2759         xml_doc = etree.fromstring(xml)
2760 
2761         device_info = vconfig.LibvirtConfigGuest()
2762         device_info.parse_dom(xml_doc)
2763 
2764         active_disk_object = None
2765 
2766         for guest_disk in device_info.devices:
2767             if (guest_disk.root_name != 'disk'):
2768                 continue
2769 
2770             if (guest_disk.target_dev is None or guest_disk.serial is None):
2771                 continue
2772 
2773             if guest_disk.serial == volume_id:
2774                 my_dev = guest_disk.target_dev
2775 
2776                 active_disk = guest_disk.source_path
2777                 active_protocol = guest_disk.source_protocol
2778                 active_disk_object = guest_disk
2779                 break
2780 
2781         if my_dev is None or (active_disk is None and active_protocol is None):
2782             LOG.debug('Domain XML: %s', xml, instance=instance)
2783             msg = (_('Disk with id: %s not found attached to instance.')
2784                    % volume_id)
2785             raise exception.InternalError(msg)
2786 
2787         LOG.debug("found device at %s", my_dev, instance=instance)
2788 
2789         def _get_snap_dev(filename, backing_store):
2790             if filename is None:
2791                 msg = _('filename cannot be None')
2792                 raise exception.InternalError(msg)
2793 
2794             # libgfapi delete
2795             LOG.debug("XML: %s", xml)
2796 
2797             LOG.debug("active disk object: %s", active_disk_object)
2798 
2799             # determine reference within backing store for desired image
2800             filename_to_merge = filename
2801             matched_name = None
2802             b = backing_store
2803             index = None
2804 
2805             current_filename = active_disk_object.source_name.split('/')[1]
2806             if current_filename == filename_to_merge:
2807                 return my_dev + '[0]'
2808 
2809             while b is not None:
2810                 source_filename = b.source_name.split('/')[1]
2811                 if source_filename == filename_to_merge:
2812                     LOG.debug('found match: %s', b.source_name)
2813                     matched_name = b.source_name
2814                     index = b.index
2815                     break
2816 
2817                 b = b.backing_store
2818 
2819             if matched_name is None:
2820                 msg = _('no match found for %s') % (filename_to_merge)
2821                 raise exception.InternalError(msg)
2822 
2823             LOG.debug('index of match (%s) is %s', b.source_name, index)
2824 
2825             my_snap_dev = '%s[%s]' % (my_dev, index)
2826             return my_snap_dev
2827 
2828         if delete_info['merge_target_file'] is None:
2829             # pull via blockRebase()
2830 
2831             # Merge the most recent snapshot into the active image
2832 
2833             rebase_disk = my_dev
2834             rebase_base = delete_info['file_to_merge']  # often None
2835             if (active_protocol is not None) and (rebase_base is not None):
2836                 rebase_base = _get_snap_dev(rebase_base,
2837                                             active_disk_object.backing_store)
2838 
2839             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2840             # and when available this flag _must_ be used to ensure backing
2841             # paths are maintained relative by qemu.
2842             #
2843             # If _RELATIVE flag not found, continue with old behaviour
2844             # (relative backing path seems to work for this case)
2845             try:
2846                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
2847                 relative = rebase_base is not None
2848             except AttributeError:
2849                 LOG.warning(
2850                     "Relative blockrebase support was not detected. "
2851                     "Continuing with old behaviour.")
2852                 relative = False
2853 
2854             LOG.debug(
2855                 'disk: %(disk)s, base: %(base)s, '
2856                 'bw: %(bw)s, relative: %(relative)s',
2857                 {'disk': rebase_disk,
2858                  'base': rebase_base,
2859                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
2860                  'relative': str(relative)}, instance=instance)
2861 
2862             dev = guest.get_block_device(rebase_disk)
2863             if guest.is_active():
2864                 result = dev.rebase(rebase_base, relative=relative)
2865                 if result == 0:
2866                     LOG.debug('blockRebase started successfully',
2867                               instance=instance)
2868 
2869                 while not dev.is_job_complete():
2870                     LOG.debug('waiting for blockRebase job completion',
2871                               instance=instance)
2872                     time.sleep(0.5)
2873 
2874             # If the guest is not running libvirt won't do a blockRebase.
2875             # In that case, let's ask qemu-img to rebase the disk.
2876             else:
2877                 LOG.debug('Guest is not running so doing a block rebase '
2878                           'using "qemu-img rebase"', instance=instance)
2879                 self._rebase_with_qemu_img(guest, dev, active_disk_object,
2880                                            rebase_base)
2881 
2882         else:
2883             # commit with blockCommit()
2884             my_snap_base = None
2885             my_snap_top = None
2886             commit_disk = my_dev
2887 
2888             if active_protocol is not None:
2889                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
2890                                              active_disk_object.backing_store)
2891                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
2892                                             active_disk_object.backing_store)
2893 
2894             commit_base = my_snap_base or delete_info['merge_target_file']
2895             commit_top = my_snap_top or delete_info['file_to_merge']
2896 
2897             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
2898                       'commit_base=%(commit_base)s '
2899                       'commit_top=%(commit_top)s ',
2900                       {'commit_disk': commit_disk,
2901                        'commit_base': commit_base,
2902                        'commit_top': commit_top}, instance=instance)
2903 
2904             dev = guest.get_block_device(commit_disk)
2905             result = dev.commit(commit_base, commit_top, relative=True)
2906 
2907             if result == 0:
2908                 LOG.debug('blockCommit started successfully',
2909                           instance=instance)
2910 
2911             while not dev.is_job_complete():
2912                 LOG.debug('waiting for blockCommit job completion',
2913                           instance=instance)
2914                 time.sleep(0.5)
2915 
2916     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
2917                                delete_info):
2918         try:
2919             self._volume_snapshot_delete(context, instance, volume_id,
2920                                          snapshot_id, delete_info=delete_info)
2921         except Exception:
2922             with excutils.save_and_reraise_exception():
2923                 LOG.exception(_('Error occurred during '
2924                                 'volume_snapshot_delete, '
2925                                 'sending error status to Cinder.'),
2926                               instance=instance)
2927                 self._volume_snapshot_update_status(
2928                     context, snapshot_id, 'error_deleting')
2929 
2930         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
2931         self._volume_refresh_connection_info(context, instance, volume_id)
2932 
2933     def reboot(self, context, instance, network_info, reboot_type,
2934                block_device_info=None, bad_volumes_callback=None):
2935         """Reboot a virtual machine, given an instance reference."""
2936         if reboot_type == 'SOFT':
2937             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
2938             try:
2939                 soft_reboot_success = self._soft_reboot(instance)
2940             except libvirt.libvirtError as e:
2941                 LOG.debug("Instance soft reboot failed: %s",
2942                           encodeutils.exception_to_unicode(e),
2943                           instance=instance)
2944                 soft_reboot_success = False
2945 
2946             if soft_reboot_success:
2947                 LOG.info("Instance soft rebooted successfully.",
2948                          instance=instance)
2949                 return
2950             else:
2951                 LOG.warning("Failed to soft reboot instance. "
2952                             "Trying hard reboot.",
2953                             instance=instance)
2954         return self._hard_reboot(context, instance, network_info,
2955                                  block_device_info)
2956 
2957     def _soft_reboot(self, instance):
2958         """Attempt to shutdown and restart the instance gracefully.
2959 
2960         We use shutdown and create here so we can return if the guest
2961         responded and actually rebooted. Note that this method only
2962         succeeds if the guest responds to acpi. Therefore we return
2963         success or failure so we can fall back to a hard reboot if
2964         necessary.
2965 
2966         :returns: True if the reboot succeeded
2967         """
2968         guest = self._host.get_guest(instance)
2969 
2970         state = guest.get_power_state(self._host)
2971         old_domid = guest.id
2972         # NOTE(vish): This check allows us to reboot an instance that
2973         #             is already shutdown.
2974         if state == power_state.RUNNING:
2975             guest.shutdown()
2976         # NOTE(vish): This actually could take slightly longer than the
2977         #             FLAG defines depending on how long the get_info
2978         #             call takes to return.
2979         self._prepare_pci_devices_for_use(
2980             pci_manager.get_instance_pci_devs(instance, 'all'))
2981         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
2982             guest = self._host.get_guest(instance)
2983 
2984             state = guest.get_power_state(self._host)
2985             new_domid = guest.id
2986 
2987             # NOTE(ivoks): By checking domain IDs, we make sure we are
2988             #              not recreating domain that's already running.
2989             if old_domid != new_domid:
2990                 if state in [power_state.SHUTDOWN,
2991                              power_state.CRASHED]:
2992                     LOG.info("Instance shutdown successfully.",
2993                              instance=instance)
2994                     self._create_domain(domain=guest._domain)
2995                     timer = loopingcall.FixedIntervalLoopingCall(
2996                         self._wait_for_running, instance)
2997                     timer.start(interval=0.5).wait()
2998                     return True
2999                 else:
3000                     LOG.info("Instance may have been rebooted during soft "
3001                              "reboot, so return now.", instance=instance)
3002                     return True
3003             greenthread.sleep(1)
3004         return False
3005 
3006     def _hard_reboot(self, context, instance, network_info,
3007                      block_device_info=None):
3008         """Reboot a virtual machine, given an instance reference.
3009 
3010         Performs a Libvirt reset (if supported) on the domain.
3011 
3012         If Libvirt reset is unavailable this method actually destroys and
3013         re-creates the domain to ensure the reboot happens, as the guest
3014         OS cannot ignore this action.
3015         """
3016         # NOTE(sbauza): Since we undefine the guest XML when destroying, we
3017         # need to remember the existing mdevs for reusing them.
3018         mdevs = self._get_all_assigned_mediated_devices(instance)
3019         mdevs = list(mdevs.keys())
3020         # NOTE(mdbooth): In addition to performing a hard reboot of the domain,
3021         # the hard reboot operation is relied upon by operators to be an
3022         # automated attempt to fix as many things as possible about a
3023         # non-functioning instance before resorting to manual intervention.
3024         # With this goal in mind, we tear down all the aspects of an instance
3025         # we can here without losing data. This allows us to re-initialise from
3026         # scratch, and hopefully fix, most aspects of a non-functioning guest.
3027         self.destroy(context, instance, network_info, destroy_disks=False,
3028                      block_device_info=block_device_info)
3029 
3030         # Convert the system metadata to image metadata
3031         # NOTE(mdbooth): This is a workaround for stateless Nova compute
3032         #                https://bugs.launchpad.net/nova/+bug/1349978
3033         instance_dir = libvirt_utils.get_instance_path(instance)
3034         fileutils.ensure_tree(instance_dir)
3035 
3036         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3037                                             instance,
3038                                             instance.image_meta,
3039                                             block_device_info)
3040         # NOTE(vish): This could generate the wrong device_format if we are
3041         #             using the raw backend and the images don't exist yet.
3042         #             The create_images_and_backing below doesn't properly
3043         #             regenerate raw backend images, however, so when it
3044         #             does we need to (re)generate the xml after the images
3045         #             are in place.
3046         xml = self._get_guest_xml(context, instance, network_info, disk_info,
3047                                   instance.image_meta,
3048                                   block_device_info=block_device_info,
3049                                   mdevs=mdevs)
3050 
3051         # NOTE(mdbooth): context.auth_token will not be set when we call
3052         #                _hard_reboot from resume_state_on_host_boot()
3053         if context.auth_token is not None:
3054             # NOTE (rmk): Re-populate any missing backing files.
3055             config = vconfig.LibvirtConfigGuest()
3056             config.parse_str(xml)
3057             backing_disk_info = self._get_instance_disk_info_from_config(
3058                 config, block_device_info)
3059             self._create_images_and_backing(context, instance, instance_dir,
3060                                             backing_disk_info)
3061 
3062         # Initialize all the necessary networking, block devices and
3063         # start the instance.
3064         # NOTE(melwitt): Pass vifs_already_plugged=True here even though we've
3065         # unplugged vifs earlier. The behavior of neutron plug events depends
3066         # on which vif type we're using and we are working with a stale network
3067         # info cache here, so won't rely on waiting for neutron plug events.
3068         # vifs_already_plugged=True means "do not wait for neutron plug events"
3069         self._create_domain_and_network(context, xml, instance, network_info,
3070                                         block_device_info=block_device_info,
3071                                         vifs_already_plugged=True)
3072         self._prepare_pci_devices_for_use(
3073             pci_manager.get_instance_pci_devs(instance, 'all'))
3074 
3075         def _wait_for_reboot():
3076             """Called at an interval until the VM is running again."""
3077             state = self.get_info(instance).state
3078 
3079             if state == power_state.RUNNING:
3080                 LOG.info("Instance rebooted successfully.",
3081                          instance=instance)
3082                 raise loopingcall.LoopingCallDone()
3083 
3084         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
3085         timer.start(interval=0.5).wait()
3086 
3087     def pause(self, instance):
3088         """Pause VM instance."""
3089         self._host.get_guest(instance).pause()
3090 
3091     def unpause(self, instance):
3092         """Unpause paused VM instance."""
3093         guest = self._host.get_guest(instance)
3094         guest.resume()
3095         guest.sync_guest_time()
3096 
3097     def _clean_shutdown(self, instance, timeout, retry_interval):
3098         """Attempt to shutdown the instance gracefully.
3099 
3100         :param instance: The instance to be shutdown
3101         :param timeout: How long to wait in seconds for the instance to
3102                         shutdown
3103         :param retry_interval: How often in seconds to signal the instance
3104                                to shutdown while waiting
3105 
3106         :returns: True if the shutdown succeeded
3107         """
3108 
3109         # List of states that represent a shutdown instance
3110         SHUTDOWN_STATES = [power_state.SHUTDOWN,
3111                            power_state.CRASHED]
3112 
3113         try:
3114             guest = self._host.get_guest(instance)
3115         except exception.InstanceNotFound:
3116             # If the instance has gone then we don't need to
3117             # wait for it to shutdown
3118             return True
3119 
3120         state = guest.get_power_state(self._host)
3121         if state in SHUTDOWN_STATES:
3122             LOG.info("Instance already shutdown.", instance=instance)
3123             return True
3124 
3125         LOG.debug("Shutting down instance from state %s", state,
3126                   instance=instance)
3127         guest.shutdown()
3128         retry_countdown = retry_interval
3129 
3130         for sec in range(timeout):
3131 
3132             guest = self._host.get_guest(instance)
3133             state = guest.get_power_state(self._host)
3134 
3135             if state in SHUTDOWN_STATES:
3136                 LOG.info("Instance shutdown successfully after %d seconds.",
3137                          sec, instance=instance)
3138                 return True
3139 
3140             # Note(PhilD): We can't assume that the Guest was able to process
3141             #              any previous shutdown signal (for example it may
3142             #              have still been startingup, so within the overall
3143             #              timeout we re-trigger the shutdown every
3144             #              retry_interval
3145             if retry_countdown == 0:
3146                 retry_countdown = retry_interval
3147                 # Instance could shutdown at any time, in which case we
3148                 # will get an exception when we call shutdown
3149                 try:
3150                     LOG.debug("Instance in state %s after %d seconds - "
3151                               "resending shutdown", state, sec,
3152                               instance=instance)
3153                     guest.shutdown()
3154                 except libvirt.libvirtError:
3155                     # Assume this is because its now shutdown, so loop
3156                     # one more time to clean up.
3157                     LOG.debug("Ignoring libvirt exception from shutdown "
3158                               "request.", instance=instance)
3159                     continue
3160             else:
3161                 retry_countdown -= 1
3162 
3163             time.sleep(1)
3164 
3165         LOG.info("Instance failed to shutdown in %d seconds.",
3166                  timeout, instance=instance)
3167         return False
3168 
3169     def power_off(self, instance, timeout=0, retry_interval=0):
3170         """Power off the specified instance."""
3171         if timeout:
3172             self._clean_shutdown(instance, timeout, retry_interval)
3173         self._destroy(instance)
3174 
3175     def power_on(self, context, instance, network_info,
3176                  block_device_info=None):
3177         """Power on the specified instance."""
3178         # We use _hard_reboot here to ensure that all backing files,
3179         # network, and block device connections, etc. are established
3180         # and available before we attempt to start the instance.
3181         self._hard_reboot(context, instance, network_info, block_device_info)
3182 
3183     def trigger_crash_dump(self, instance):
3184 
3185         """Trigger crash dump by injecting an NMI to the specified instance."""
3186         try:
3187             self._host.get_guest(instance).inject_nmi()
3188         except libvirt.libvirtError as ex:
3189             error_code = ex.get_error_code()
3190 
3191             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
3192                 raise exception.TriggerCrashDumpNotSupported()
3193             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
3194                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
3195 
3196             LOG.exception(_('Error from libvirt while injecting an NMI to '
3197                             '%(instance_uuid)s: '
3198                             '[Error Code %(error_code)s] %(ex)s'),
3199                           {'instance_uuid': instance.uuid,
3200                            'error_code': error_code, 'ex': ex})
3201             raise
3202 
3203     def suspend(self, context, instance):
3204         """Suspend the specified instance."""
3205         guest = self._host.get_guest(instance)
3206 
3207         self._detach_pci_devices(guest,
3208             pci_manager.get_instance_pci_devs(instance))
3209         self._detach_direct_passthrough_ports(context, instance, guest)
3210         self._detach_mediated_devices(guest)
3211         guest.save_memory_state()
3212 
3213     def resume(self, context, instance, network_info, block_device_info=None):
3214         """resume the specified instance."""
3215         xml = self._get_existing_domain_xml(instance, network_info,
3216                                             block_device_info)
3217         guest = self._create_domain_and_network(context, xml, instance,
3218                            network_info, block_device_info=block_device_info,
3219                            vifs_already_plugged=True)
3220         self._attach_pci_devices(guest,
3221             pci_manager.get_instance_pci_devs(instance))
3222         self._attach_direct_passthrough_ports(
3223             context, instance, guest, network_info)
3224         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
3225                                                      instance)
3226         timer.start(interval=0.5).wait()
3227         guest.sync_guest_time()
3228 
3229     def resume_state_on_host_boot(self, context, instance, network_info,
3230                                   block_device_info=None):
3231         """resume guest state when a host is booted."""
3232         # Check if the instance is running already and avoid doing
3233         # anything if it is.
3234         try:
3235             guest = self._host.get_guest(instance)
3236             state = guest.get_power_state(self._host)
3237 
3238             ignored_states = (power_state.RUNNING,
3239                               power_state.SUSPENDED,
3240                               power_state.NOSTATE,
3241                               power_state.PAUSED)
3242 
3243             if state in ignored_states:
3244                 return
3245         except (exception.InternalError, exception.InstanceNotFound):
3246             pass
3247 
3248         # Instance is not up and could be in an unknown state.
3249         # Be as absolute as possible about getting it back into
3250         # a known and running state.
3251         self._hard_reboot(context, instance, network_info, block_device_info)
3252 
3253     def rescue(self, context, instance, network_info, image_meta,
3254                rescue_password):
3255         """Loads a VM using rescue images.
3256 
3257         A rescue is normally performed when something goes wrong with the
3258         primary images and data needs to be corrected/recovered. Rescuing
3259         should not edit or over-ride the original image, only allow for
3260         data recovery.
3261 
3262         """
3263         instance_dir = libvirt_utils.get_instance_path(instance)
3264         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
3265         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3266         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
3267 
3268         rescue_image_id = None
3269         if image_meta.obj_attr_is_set("id"):
3270             rescue_image_id = image_meta.id
3271 
3272         rescue_images = {
3273             'image_id': (rescue_image_id or
3274                         CONF.libvirt.rescue_image_id or instance.image_ref),
3275             'kernel_id': (CONF.libvirt.rescue_kernel_id or
3276                           instance.kernel_id),
3277             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
3278                            instance.ramdisk_id),
3279         }
3280         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3281                                             instance,
3282                                             image_meta,
3283                                             rescue=True)
3284         injection_info = InjectionInfo(network_info=network_info,
3285                                        admin_pass=rescue_password,
3286                                        files=None)
3287         gen_confdrive = functools.partial(self._create_configdrive,
3288                                           context, instance, injection_info,
3289                                           rescue=True)
3290         # NOTE(sbauza): Since rescue recreates the guest XML, we need to
3291         # remember the existing mdevs for reusing them.
3292         mdevs = self._get_all_assigned_mediated_devices(instance)
3293         mdevs = list(mdevs.keys())
3294         self._create_image(context, instance, disk_info['mapping'],
3295                            injection_info=injection_info, suffix='.rescue',
3296                            disk_images=rescue_images)
3297         xml = self._get_guest_xml(context, instance, network_info, disk_info,
3298                                   image_meta, rescue=rescue_images,
3299                                   mdevs=mdevs)
3300         self._destroy(instance)
3301         self._create_domain(xml, post_xml_callback=gen_confdrive)
3302 
3303     def unrescue(self, instance, network_info):
3304         """Reboot the VM which is being rescued back into primary images.
3305         """
3306         instance_dir = libvirt_utils.get_instance_path(instance)
3307         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3308         xml = libvirt_utils.load_file(unrescue_xml_path)
3309         guest = self._host.get_guest(instance)
3310 
3311         # TODO(sahid): We are converting all calls from a
3312         # virDomain object to use nova.virt.libvirt.Guest.
3313         # We should be able to remove virt_dom at the end.
3314         virt_dom = guest._domain
3315         self._destroy(instance)
3316         self._create_domain(xml, virt_dom)
3317         os.unlink(unrescue_xml_path)
3318         rescue_files = os.path.join(instance_dir, "*.rescue")
3319         for rescue_file in glob.iglob(rescue_files):
3320             if os.path.isdir(rescue_file):
3321                 shutil.rmtree(rescue_file)
3322             else:
3323                 os.unlink(rescue_file)
3324         # cleanup rescue volume
3325         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
3326                                 if lvmdisk.endswith('.rescue')])
3327         if CONF.libvirt.images_type == 'rbd':
3328             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
3329                                       disk.endswith('.rescue'))
3330             LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
3331 
3332     def poll_rebooting_instances(self, timeout, instances):
3333         pass
3334 
3335     def spawn(self, context, instance, image_meta, injected_files,
3336               admin_password, allocations, network_info=None,
3337               block_device_info=None):
3338         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3339                                             instance,
3340                                             image_meta,
3341                                             block_device_info)
3342         injection_info = InjectionInfo(network_info=network_info,
3343                                        files=injected_files,
3344                                        admin_pass=admin_password)
3345         gen_confdrive = functools.partial(self._create_configdrive,
3346                                           context, instance,
3347                                           injection_info)
3348         self._create_image(context, instance, disk_info['mapping'],
3349                            injection_info=injection_info,
3350                            block_device_info=block_device_info)
3351 
3352         # Required by Quobyte CI
3353         self._ensure_console_log_for_instance(instance)
3354 
3355         # Does the guest need to be assigned some vGPU mediated devices ?
3356         mdevs = self._allocate_mdevs(allocations)
3357 
3358         xml = self._get_guest_xml(context, instance, network_info,
3359                                   disk_info, image_meta,
3360                                   block_device_info=block_device_info,
3361                                   mdevs=mdevs)
3362         self._create_domain_and_network(
3363             context, xml, instance, network_info,
3364             block_device_info=block_device_info,
3365             post_xml_callback=gen_confdrive,
3366             destroy_disks_on_failure=True)
3367         LOG.debug("Guest created on hypervisor", instance=instance)
3368 
3369         def _wait_for_boot():
3370             """Called at an interval until the VM is running."""
3371             state = self.get_info(instance).state
3372 
3373             if state == power_state.RUNNING:
3374                 LOG.info("Instance spawned successfully.", instance=instance)
3375                 raise loopingcall.LoopingCallDone()
3376 
3377         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
3378         timer.start(interval=0.5).wait()
3379 
3380     def _get_console_output_file(self, instance, console_log):
3381         bytes_to_read = MAX_CONSOLE_BYTES
3382         log_data = b""  # The last N read bytes
3383         i = 0  # in case there is a log rotation (like "virtlogd")
3384         path = console_log
3385 
3386         while bytes_to_read > 0 and os.path.exists(path):
3387             read_log_data, remaining = nova.privsep.path.last_bytes(
3388                                         path, bytes_to_read)
3389             # We need the log file content in chronological order,
3390             # that's why we *prepend* the log data.
3391             log_data = read_log_data + log_data
3392 
3393             # Prep to read the next file in the chain
3394             bytes_to_read -= len(read_log_data)
3395             path = console_log + "." + str(i)
3396             i += 1
3397 
3398             if remaining > 0:
3399                 LOG.info('Truncated console log returned, '
3400                          '%d bytes ignored', remaining, instance=instance)
3401         return log_data
3402 
3403     def get_console_output(self, context, instance):
3404         guest = self._host.get_guest(instance)
3405 
3406         xml = guest.get_xml_desc()
3407         tree = etree.fromstring(xml)
3408 
3409         # check for different types of consoles
3410         path_sources = [
3411             ('file', "./devices/console[@type='file']/source[@path]", 'path'),
3412             ('tcp', "./devices/console[@type='tcp']/log[@file]", 'file'),
3413             ('pty', "./devices/console[@type='pty']/source[@path]", 'path')]
3414         console_type = ""
3415         console_path = ""
3416         for c_type, epath, attrib in path_sources:
3417             node = tree.find(epath)
3418             if (node is not None) and node.get(attrib):
3419                 console_type = c_type
3420                 console_path = node.get(attrib)
3421                 break
3422 
3423         # instance has no console at all
3424         if not console_path:
3425             raise exception.ConsoleNotAvailable()
3426 
3427         # instance has a console, but file doesn't exist (yet?)
3428         if not os.path.exists(console_path):
3429             LOG.info('console logfile for instance does not exist',
3430                       instance=instance)
3431             return ""
3432 
3433         # pty consoles need special handling
3434         if console_type == 'pty':
3435             console_log = self._get_console_log_path(instance)
3436             data = nova.privsep.libvirt.readpty(console_path)
3437 
3438             # NOTE(markus_z): The virt_types kvm and qemu are the only ones
3439             # which create a dedicated file device for the console logging.
3440             # Other virt_types like xen, lxc, uml, parallels depend on the
3441             # flush of that pty device into the "console.log" file to ensure
3442             # that a series of "get_console_output" calls return the complete
3443             # content even after rebooting a guest.
3444             nova.privsep.path.writefile(console_log, 'a+', data)
3445 
3446             # set console path to logfile, not to pty device
3447             console_path = console_log
3448 
3449         # return logfile content
3450         return self._get_console_output_file(instance, console_path)
3451 
3452     def get_host_ip_addr(self):
3453         ips = compute_utils.get_machine_ips()
3454         if CONF.my_ip not in ips:
3455             LOG.warning('my_ip address (%(my_ip)s) was not found on '
3456                         'any of the interfaces: %(ifaces)s',
3457                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
3458         return CONF.my_ip
3459 
3460     def get_vnc_console(self, context, instance):
3461         def get_vnc_port_for_instance(instance_name):
3462             guest = self._host.get_guest(instance)
3463 
3464             xml = guest.get_xml_desc()
3465             xml_dom = etree.fromstring(xml)
3466 
3467             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
3468             if graphic is not None:
3469                 return graphic.get('port')
3470             # NOTE(rmk): We had VNC consoles enabled but the instance in
3471             # question is not actually listening for connections.
3472             raise exception.ConsoleTypeUnavailable(console_type='vnc')
3473 
3474         port = get_vnc_port_for_instance(instance.name)
3475         host = CONF.vnc.server_proxyclient_address
3476 
3477         return ctype.ConsoleVNC(host=host, port=port)
3478 
3479     def get_spice_console(self, context, instance):
3480         def get_spice_ports_for_instance(instance_name):
3481             guest = self._host.get_guest(instance)
3482 
3483             xml = guest.get_xml_desc()
3484             xml_dom = etree.fromstring(xml)
3485 
3486             graphic = xml_dom.find("./devices/graphics[@type='spice']")
3487             if graphic is not None:
3488                 return (graphic.get('port'), graphic.get('tlsPort'))
3489             # NOTE(rmk): We had Spice consoles enabled but the instance in
3490             # question is not actually listening for connections.
3491             raise exception.ConsoleTypeUnavailable(console_type='spice')
3492 
3493         ports = get_spice_ports_for_instance(instance.name)
3494         host = CONF.spice.server_proxyclient_address
3495 
3496         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
3497 
3498     def get_serial_console(self, context, instance):
3499         guest = self._host.get_guest(instance)
3500         for hostname, port in self._get_serial_ports_from_guest(
3501                 guest, mode='bind'):
3502             return ctype.ConsoleSerial(host=hostname, port=port)
3503         raise exception.ConsoleTypeUnavailable(console_type='serial')
3504 
3505     @staticmethod
3506     def _create_ephemeral(target, ephemeral_size,
3507                           fs_label, os_type, is_block_dev=False,
3508                           context=None, specified_fs=None,
3509                           vm_mode=None):
3510         if not is_block_dev:
3511             if (CONF.libvirt.virt_type == "parallels" and
3512                     vm_mode == fields.VMMode.EXE):
3513 
3514                 libvirt_utils.create_ploop_image('expanded', target,
3515                                                  '%dG' % ephemeral_size,
3516                                                  specified_fs)
3517                 return
3518             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
3519 
3520         # Run as root only for block devices.
3521         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
3522                       specified_fs=specified_fs)
3523 
3524     @staticmethod
3525     def _create_swap(target, swap_mb, context=None):
3526         """Create a swap file of specified size."""
3527         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
3528         nova.privsep.fs.unprivileged_mkfs('swap', target)
3529 
3530     @staticmethod
3531     def _get_console_log_path(instance):
3532         return os.path.join(libvirt_utils.get_instance_path(instance),
3533                             'console.log')
3534 
3535     def _ensure_console_log_for_instance(self, instance):
3536         # NOTE(mdbooth): Although libvirt will create this file for us
3537         # automatically when it starts, it will initially create it with
3538         # root ownership and then chown it depending on the configuration of
3539         # the domain it is launching. Quobyte CI explicitly disables the
3540         # chown by setting dynamic_ownership=0 in libvirt's config.
3541         # Consequently when the domain starts it is unable to write to its
3542         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
3543         #
3544         # To work around this, we create the file manually before starting
3545         # the domain so it has the same ownership as Nova. This works
3546         # for Quobyte CI because it is also configured to run qemu as the same
3547         # user as the Nova service. Installations which don't set
3548         # dynamic_ownership=0 are not affected because libvirt will always
3549         # correctly configure permissions regardless of initial ownership.
3550         #
3551         # Setting dynamic_ownership=0 is dubious and potentially broken in
3552         # more ways than console.log (see comment #22 on the above bug), so
3553         # Future Maintainer who finds this code problematic should check to see
3554         # if we still support it.
3555         console_file = self._get_console_log_path(instance)
3556         LOG.debug('Ensure instance console log exists: %s', console_file,
3557                   instance=instance)
3558         try:
3559             libvirt_utils.file_open(console_file, 'a').close()
3560         # NOTE(sfinucan): We can safely ignore permission issues here and
3561         # assume that it is libvirt that has taken ownership of this file.
3562         except IOError as ex:
3563             if ex.errno != errno.EACCES:
3564                 raise
3565             LOG.debug('Console file already exists: %s.', console_file)
3566 
3567     @staticmethod
3568     def _get_disk_config_image_type():
3569         # TODO(mikal): there is a bug here if images_type has
3570         # changed since creation of the instance, but I am pretty
3571         # sure that this bug already exists.
3572         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
3573 
3574     @staticmethod
3575     def _is_booted_from_volume(block_device_info):
3576         """Determines whether the VM is booting from volume
3577 
3578         Determines whether the block device info indicates that the VM
3579         is booting from a volume.
3580         """
3581         block_device_mapping = driver.block_device_info_get_mapping(
3582             block_device_info)
3583         return bool(block_device.get_root_bdm(block_device_mapping))
3584 
3585     def _inject_data(self, disk, instance, injection_info):
3586         """Injects data in a disk image
3587 
3588         Helper used for injecting data in a disk image file system.
3589 
3590         :param disk: The disk we're injecting into (an Image object)
3591         :param instance: The instance we're injecting into
3592         :param injection_info: Injection info
3593         """
3594         # Handles the partition need to be used.
3595         LOG.debug('Checking root disk injection %s',
3596                   str(injection_info), instance=instance)
3597         target_partition = None
3598         if not instance.kernel_id:
3599             target_partition = CONF.libvirt.inject_partition
3600             if target_partition == 0:
3601                 target_partition = None
3602         if CONF.libvirt.virt_type == 'lxc':
3603             target_partition = None
3604 
3605         # Handles the key injection.
3606         if CONF.libvirt.inject_key and instance.get('key_data'):
3607             key = str(instance.key_data)
3608         else:
3609             key = None
3610 
3611         # Handles the admin password injection.
3612         if not CONF.libvirt.inject_password:
3613             admin_pass = None
3614         else:
3615             admin_pass = injection_info.admin_pass
3616 
3617         # Handles the network injection.
3618         net = netutils.get_injected_network_template(
3619             injection_info.network_info,
3620             libvirt_virt_type=CONF.libvirt.virt_type)
3621 
3622         # Handles the metadata injection
3623         metadata = instance.get('metadata')
3624 
3625         if any((key, net, metadata, admin_pass, injection_info.files)):
3626             LOG.debug('Injecting %s', str(injection_info),
3627                       instance=instance)
3628             img_id = instance.image_ref
3629             try:
3630                 disk_api.inject_data(disk.get_model(self._conn),
3631                                      key, net, metadata, admin_pass,
3632                                      injection_info.files,
3633                                      partition=target_partition,
3634                                      mandatory=('files',))
3635             except Exception as e:
3636                 with excutils.save_and_reraise_exception():
3637                     LOG.error('Error injecting data into image '
3638                               '%(img_id)s (%(e)s)',
3639                               {'img_id': img_id, 'e': e},
3640                               instance=instance)
3641 
3642     # NOTE(sileht): many callers of this method assume that this
3643     # method doesn't fail if an image already exists but instead
3644     # think that it will be reused (ie: (live)-migration/resize)
3645     def _create_image(self, context, instance,
3646                       disk_mapping, injection_info=None, suffix='',
3647                       disk_images=None, block_device_info=None,
3648                       fallback_from_host=None,
3649                       ignore_bdi_for_swap=False):
3650         booted_from_volume = self._is_booted_from_volume(block_device_info)
3651 
3652         def image(fname, image_type=CONF.libvirt.images_type):
3653             return self.image_backend.by_name(instance,
3654                                               fname + suffix, image_type)
3655 
3656         def raw(fname):
3657             return image(fname, image_type='raw')
3658 
3659         # ensure directories exist and are writable
3660         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
3661 
3662         LOG.info('Creating image', instance=instance)
3663 
3664         inst_type = instance.get_flavor()
3665         swap_mb = 0
3666         if 'disk.swap' in disk_mapping:
3667             mapping = disk_mapping['disk.swap']
3668 
3669             if ignore_bdi_for_swap:
3670                 # This is a workaround to support legacy swap resizing,
3671                 # which does not touch swap size specified in bdm,
3672                 # but works with flavor specified size only.
3673                 # In this case we follow the legacy logic and ignore block
3674                 # device info completely.
3675                 # NOTE(ft): This workaround must be removed when a correct
3676                 # implementation of resize operation changing sizes in bdms is
3677                 # developed. Also at that stage we probably may get rid of
3678                 # the direct usage of flavor swap size here,
3679                 # leaving the work with bdm only.
3680                 swap_mb = inst_type['swap']
3681             else:
3682                 swap = driver.block_device_info_get_swap(block_device_info)
3683                 if driver.swap_is_usable(swap):
3684                     swap_mb = swap['swap_size']
3685                 elif (inst_type['swap'] > 0 and
3686                       not block_device.volume_in_mapping(
3687                         mapping['dev'], block_device_info)):
3688                     swap_mb = inst_type['swap']
3689 
3690             if swap_mb > 0:
3691                 if (CONF.libvirt.virt_type == "parallels" and
3692                         instance.vm_mode == fields.VMMode.EXE):
3693                     msg = _("Swap disk is not supported "
3694                             "for Virtuozzo container")
3695                     raise exception.Invalid(msg)
3696 
3697         if not disk_images:
3698             disk_images = {'image_id': instance.image_ref,
3699                            'kernel_id': instance.kernel_id,
3700                            'ramdisk_id': instance.ramdisk_id}
3701 
3702         if disk_images['kernel_id']:
3703             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
3704             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
3705                                 context=context,
3706                                 filename=fname,
3707                                 image_id=disk_images['kernel_id'])
3708             if disk_images['ramdisk_id']:
3709                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
3710                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
3711                                      context=context,
3712                                      filename=fname,
3713                                      image_id=disk_images['ramdisk_id'])
3714 
3715         if CONF.libvirt.virt_type == 'uml':
3716             # PONDERING(mikal): can I assume that root is UID zero in every
3717             # OS? Probably not.
3718             uid = pwd.getpwnam('root').pw_uid
3719             nova.privsep.path.chown(image('disk').path, uid=uid)
3720 
3721         self._create_and_inject_local_root(context, instance,
3722                                            booted_from_volume, suffix,
3723                                            disk_images, injection_info,
3724                                            fallback_from_host)
3725 
3726         # Lookup the filesystem type if required
3727         os_type_with_default = nova.privsep.fs.get_fs_type_for_os_type(
3728             instance.os_type)
3729         # Generate a file extension based on the file system
3730         # type and the mkfs commands configured if any
3731         file_extension = nova.privsep.fs.get_file_extension_for_os_type(
3732             os_type_with_default, CONF.default_ephemeral_format)
3733 
3734         vm_mode = fields.VMMode.get_from_instance(instance)
3735         ephemeral_gb = instance.flavor.ephemeral_gb
3736         if 'disk.local' in disk_mapping:
3737             disk_image = image('disk.local')
3738             fn = functools.partial(self._create_ephemeral,
3739                                    fs_label='ephemeral0',
3740                                    os_type=instance.os_type,
3741                                    is_block_dev=disk_image.is_block_dev,
3742                                    vm_mode=vm_mode)
3743             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
3744             size = ephemeral_gb * units.Gi
3745             disk_image.cache(fetch_func=fn,
3746                              context=context,
3747                              filename=fname,
3748                              size=size,
3749                              ephemeral_size=ephemeral_gb)
3750 
3751         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
3752                 block_device_info)):
3753             disk_image = image(blockinfo.get_eph_disk(idx))
3754 
3755             specified_fs = eph.get('guest_format')
3756             if specified_fs and not self.is_supported_fs_format(specified_fs):
3757                 msg = _("%s format is not supported") % specified_fs
3758                 raise exception.InvalidBDMFormat(details=msg)
3759 
3760             fn = functools.partial(self._create_ephemeral,
3761                                    fs_label='ephemeral%d' % idx,
3762                                    os_type=instance.os_type,
3763                                    is_block_dev=disk_image.is_block_dev,
3764                                    vm_mode=vm_mode)
3765             size = eph['size'] * units.Gi
3766             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
3767             disk_image.cache(fetch_func=fn,
3768                              context=context,
3769                              filename=fname,
3770                              size=size,
3771                              ephemeral_size=eph['size'],
3772                              specified_fs=specified_fs)
3773 
3774         if swap_mb > 0:
3775             size = swap_mb * units.Mi
3776             image('disk.swap').cache(fetch_func=self._create_swap,
3777                                      context=context,
3778                                      filename="swap_%s" % swap_mb,
3779                                      size=size,
3780                                      swap_mb=swap_mb)
3781 
3782     def _create_and_inject_local_root(self, context, instance,
3783                                       booted_from_volume, suffix, disk_images,
3784                                       injection_info, fallback_from_host):
3785         # File injection only if needed
3786         need_inject = (not configdrive.required_by(instance) and
3787                        injection_info is not None and
3788                        CONF.libvirt.inject_partition != -2)
3789 
3790         # NOTE(ndipanov): Even if disk_mapping was passed in, which
3791         # currently happens only on rescue - we still don't want to
3792         # create a base image.
3793         if not booted_from_volume:
3794             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
3795             size = instance.flavor.root_gb * units.Gi
3796 
3797             if size == 0 or suffix == '.rescue':
3798                 size = None
3799 
3800             backend = self.image_backend.by_name(instance, 'disk' + suffix,
3801                                                  CONF.libvirt.images_type)
3802             if instance.task_state == task_states.RESIZE_FINISH:
3803                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
3804             if backend.SUPPORTS_CLONE:
3805                 def clone_fallback_to_fetch(*args, **kwargs):
3806                     try:
3807                         backend.clone(context, disk_images['image_id'])
3808                     except exception.ImageUnacceptable:
3809                         libvirt_utils.fetch_image(*args, **kwargs)
3810                 fetch_func = clone_fallback_to_fetch
3811             else:
3812                 fetch_func = libvirt_utils.fetch_image
3813             self._try_fetch_image_cache(backend, fetch_func, context,
3814                                         root_fname, disk_images['image_id'],
3815                                         instance, size, fallback_from_host)
3816 
3817             if need_inject:
3818                 self._inject_data(backend, instance, injection_info)
3819 
3820         elif need_inject:
3821             LOG.warning('File injection into a boot from volume '
3822                         'instance is not supported', instance=instance)
3823 
3824     def _create_configdrive(self, context, instance, injection_info,
3825                             rescue=False):
3826         # As this method being called right after the definition of a
3827         # domain, but before its actual launch, device metadata will be built
3828         # and saved in the instance for it to be used by the config drive and
3829         # the metadata service.
3830         instance.device_metadata = self._build_device_metadata(context,
3831                                                                instance)
3832         if configdrive.required_by(instance):
3833             LOG.info('Using config drive', instance=instance)
3834 
3835             name = 'disk.config'
3836             if rescue:
3837                 name += '.rescue'
3838 
3839             config_disk = self.image_backend.by_name(
3840                 instance, name, self._get_disk_config_image_type())
3841 
3842             # Don't overwrite an existing config drive
3843             if not config_disk.exists():
3844                 extra_md = {}
3845                 if injection_info.admin_pass:
3846                     extra_md['admin_pass'] = injection_info.admin_pass
3847 
3848                 inst_md = instance_metadata.InstanceMetadata(
3849                     instance, content=injection_info.files, extra_md=extra_md,
3850                     network_info=injection_info.network_info,
3851                     request_context=context)
3852 
3853                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
3854                 with cdb:
3855                     # NOTE(mdbooth): We're hardcoding here the path of the
3856                     # config disk when using the flat backend. This isn't
3857                     # good, but it's required because we need a local path we
3858                     # know we can write to in case we're subsequently
3859                     # importing into rbd. This will be cleaned up when we
3860                     # replace this with a call to create_from_func, but that
3861                     # can't happen until we've updated the backends and we
3862                     # teach them not to cache config disks. This isn't
3863                     # possible while we're still using cache() under the hood.
3864                     config_disk_local_path = os.path.join(
3865                         libvirt_utils.get_instance_path(instance), name)
3866                     LOG.info('Creating config drive at %(path)s',
3867                              {'path': config_disk_local_path},
3868                              instance=instance)
3869 
3870                     try:
3871                         cdb.make_drive(config_disk_local_path)
3872                     except processutils.ProcessExecutionError as e:
3873                         with excutils.save_and_reraise_exception():
3874                             LOG.error('Creating config drive failed with '
3875                                       'error: %s', e, instance=instance)
3876 
3877                 try:
3878                     config_disk.import_file(
3879                         instance, config_disk_local_path, name)
3880                 finally:
3881                     # NOTE(mikal): if the config drive was imported into RBD,
3882                     # then we no longer need the local copy
3883                     if CONF.libvirt.images_type == 'rbd':
3884                         LOG.info('Deleting local config drive %(path)s '
3885                                  'because it was imported into RBD.',
3886                                  {'path': config_disk_local_path},
3887                                  instance=instance)
3888                         os.unlink(config_disk_local_path)
3889 
3890     def _prepare_pci_devices_for_use(self, pci_devices):
3891         # kvm , qemu support managed mode
3892         # In managed mode, the configured device will be automatically
3893         # detached from the host OS drivers when the guest is started,
3894         # and then re-attached when the guest shuts down.
3895         if CONF.libvirt.virt_type != 'xen':
3896             # we do manual detach only for xen
3897             return
3898         try:
3899             for dev in pci_devices:
3900                 libvirt_dev_addr = dev['hypervisor_name']
3901                 libvirt_dev = \
3902                         self._host.device_lookup_by_name(libvirt_dev_addr)
3903                 # Note(yjiang5) Spelling for 'dettach' is correct, see
3904                 # http://libvirt.org/html/libvirt-libvirt.html.
3905                 libvirt_dev.dettach()
3906 
3907             # Note(yjiang5): A reset of one PCI device may impact other
3908             # devices on the same bus, thus we need two separated loops
3909             # to detach and then reset it.
3910             for dev in pci_devices:
3911                 libvirt_dev_addr = dev['hypervisor_name']
3912                 libvirt_dev = \
3913                         self._host.device_lookup_by_name(libvirt_dev_addr)
3914                 libvirt_dev.reset()
3915 
3916         except libvirt.libvirtError as exc:
3917             raise exception.PciDevicePrepareFailed(id=dev['id'],
3918                                                    instance_uuid=
3919                                                    dev['instance_uuid'],
3920                                                    reason=six.text_type(exc))
3921 
3922     def _detach_pci_devices(self, guest, pci_devs):
3923         try:
3924             for dev in pci_devs:
3925                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
3926                 # after detachDeviceFlags returned, we should check the dom to
3927                 # ensure the detaching is finished
3928                 xml = guest.get_xml_desc()
3929                 xml_doc = etree.fromstring(xml)
3930                 guest_config = vconfig.LibvirtConfigGuest()
3931                 guest_config.parse_dom(xml_doc)
3932 
3933                 for hdev in [d for d in guest_config.devices
3934                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
3935                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
3936                     dbsf = pci_utils.parse_address(dev.address)
3937                     if [int(x, 16) for x in hdbsf] ==\
3938                             [int(x, 16) for x in dbsf]:
3939                         raise exception.PciDeviceDetachFailed(reason=
3940                                                               "timeout",
3941                                                               dev=dev)
3942 
3943         except libvirt.libvirtError as ex:
3944             error_code = ex.get_error_code()
3945             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
3946                 LOG.warning("Instance disappeared while detaching "
3947                             "a PCI device from it.")
3948             else:
3949                 raise
3950 
3951     def _attach_pci_devices(self, guest, pci_devs):
3952         try:
3953             for dev in pci_devs:
3954                 guest.attach_device(self._get_guest_pci_device(dev))
3955 
3956         except libvirt.libvirtError:
3957             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
3958                       {'dev': pci_devs, 'dom': guest.id})
3959             raise
3960 
3961     @staticmethod
3962     def _has_direct_passthrough_port(network_info):
3963         for vif in network_info:
3964             if (vif['vnic_type'] in
3965                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3966                 return True
3967         return False
3968 
3969     def _attach_direct_passthrough_ports(
3970         self, context, instance, guest, network_info=None):
3971         if network_info is None:
3972             network_info = instance.info_cache.network_info
3973         if network_info is None:
3974             return
3975 
3976         if self._has_direct_passthrough_port(network_info):
3977             for vif in network_info:
3978                 if (vif['vnic_type'] in
3979                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3980                     cfg = self.vif_driver.get_config(instance,
3981                                                      vif,
3982                                                      instance.image_meta,
3983                                                      instance.flavor,
3984                                                      CONF.libvirt.virt_type,
3985                                                      self._host)
3986                     LOG.debug('Attaching direct passthrough port %(port)s '
3987                               'to %(dom)s', {'port': vif, 'dom': guest.id},
3988                               instance=instance)
3989                     guest.attach_device(cfg)
3990 
3991     def _detach_direct_passthrough_ports(self, context, instance, guest):
3992         network_info = instance.info_cache.network_info
3993         if network_info is None:
3994             return
3995 
3996         if self._has_direct_passthrough_port(network_info):
3997             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
3998             # pci request per direct passthrough port. Therefore we can trust
3999             # that pci_slot value in the vif is correct.
4000             direct_passthrough_pci_addresses = [
4001                 vif['profile']['pci_slot']
4002                 for vif in network_info
4003                 if (vif['vnic_type'] in
4004                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
4005                     vif['profile'].get('pci_slot') is not None)
4006             ]
4007 
4008             # use detach_pci_devices to avoid failure in case of
4009             # multiple guest direct passthrough ports with the same MAC
4010             # (protection use-case, ports are on different physical
4011             # interfaces)
4012             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
4013             direct_passthrough_pci_addresses = (
4014                 [pci_dev for pci_dev in pci_devs
4015                  if pci_dev.address in direct_passthrough_pci_addresses])
4016             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
4017 
4018     def _set_host_enabled(self, enabled,
4019                           disable_reason=DISABLE_REASON_UNDEFINED):
4020         """Enables / Disables the compute service on this host.
4021 
4022            This doesn't override non-automatic disablement with an automatic
4023            setting; thereby permitting operators to keep otherwise
4024            healthy hosts out of rotation.
4025         """
4026 
4027         status_name = {True: 'disabled',
4028                        False: 'enabled'}
4029 
4030         disable_service = not enabled
4031 
4032         ctx = nova_context.get_admin_context()
4033         try:
4034             service = objects.Service.get_by_compute_host(ctx, CONF.host)
4035 
4036             if service.disabled != disable_service:
4037                 # Note(jang): this is a quick fix to stop operator-
4038                 # disabled compute hosts from re-enabling themselves
4039                 # automatically. We prefix any automatic reason code
4040                 # with a fixed string. We only re-enable a host
4041                 # automatically if we find that string in place.
4042                 # This should probably be replaced with a separate flag.
4043                 if not service.disabled or (
4044                         service.disabled_reason and
4045                         service.disabled_reason.startswith(DISABLE_PREFIX)):
4046                     service.disabled = disable_service
4047                     service.disabled_reason = (
4048                        DISABLE_PREFIX + disable_reason
4049                        if disable_service and disable_reason else
4050                            DISABLE_REASON_UNDEFINED)
4051                     service.save()
4052                     LOG.debug('Updating compute service status to %s',
4053                               status_name[disable_service])
4054                 else:
4055                     LOG.debug('Not overriding manual compute service '
4056                               'status with: %s',
4057                               status_name[disable_service])
4058         except exception.ComputeHostNotFound:
4059             LOG.warning('Cannot update service status on host "%s" '
4060                         'since it is not registered.', CONF.host)
4061         except Exception:
4062             LOG.warning('Cannot update service status on host "%s" '
4063                         'due to an unexpected exception.', CONF.host,
4064                         exc_info=True)
4065 
4066         if enabled:
4067             mount.get_manager().host_up(self._host)
4068         else:
4069             mount.get_manager().host_down()
4070 
4071     def _get_guest_cpu_model_config(self):
4072         mode = CONF.libvirt.cpu_mode
4073         model = CONF.libvirt.cpu_model
4074         extra_flags = set([flag.lower() for flag in
4075             CONF.libvirt.cpu_model_extra_flags])
4076 
4077         if (CONF.libvirt.virt_type == "kvm" or
4078             CONF.libvirt.virt_type == "qemu"):
4079             if mode is None:
4080                 caps = self._host.get_capabilities()
4081                 # AArch64 lacks 'host-model' support because neither libvirt
4082                 # nor QEMU are able to tell what the host CPU model exactly is.
4083                 # And there is no CPU description code for ARM(64) at this
4084                 # point.
4085 
4086                 # Also worth noting: 'host-passthrough' mode will completely
4087                 # break live migration, *unless* all the Compute nodes (running
4088                 # libvirtd) have *identical* CPUs.
4089                 if caps.host.cpu.arch == fields.Architecture.AARCH64:
4090                     mode = "host-passthrough"
4091                     LOG.info('CPU mode "host-passthrough" was chosen. Live '
4092                              'migration can break unless all compute nodes '
4093                              'have identical cpus. AArch64 does not support '
4094                              'other modes.')
4095                 else:
4096                     mode = "host-model"
4097             if mode == "none":
4098                 return vconfig.LibvirtConfigGuestCPU()
4099         else:
4100             if mode is None or mode == "none":
4101                 return None
4102 
4103         if ((CONF.libvirt.virt_type != "kvm" and
4104              CONF.libvirt.virt_type != "qemu")):
4105             msg = _("Config requested an explicit CPU model, but "
4106                     "the current libvirt hypervisor '%s' does not "
4107                     "support selecting CPU models") % CONF.libvirt.virt_type
4108             raise exception.Invalid(msg)
4109 
4110         if mode == "custom" and model is None:
4111             msg = _("Config requested a custom CPU model, but no "
4112                     "model name was provided")
4113             raise exception.Invalid(msg)
4114         elif mode != "custom" and model is not None:
4115             msg = _("A CPU model name should not be set when a "
4116                     "host CPU model is requested")
4117             raise exception.Invalid(msg)
4118 
4119         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen, "
4120                   "with extra flags: '%(extra_flags)s'",
4121                   {'mode': mode,
4122                    'model': (model or ""),
4123                    'extra_flags': (extra_flags or "")})
4124 
4125         cpu = vconfig.LibvirtConfigGuestCPU()
4126         cpu.mode = mode
4127         cpu.model = model
4128 
4129         # NOTE (kchamart): Currently there's no existing way to ask if a
4130         # given CPU model + CPU flags combination is supported by KVM &
4131         # a specific QEMU binary.  However, libvirt runs the 'CPUID'
4132         # command upfront -- before even a Nova instance (a QEMU
4133         # process) is launched -- to construct CPU models and check
4134         # their validity; so we are good there.  In the long-term,
4135         # upstream libvirt intends to add an additional new API that can
4136         # do fine-grained validation of a certain CPU model + CPU flags
4137         # against a specific QEMU binary (the libvirt RFE bug for that:
4138         # https://bugzilla.redhat.com/show_bug.cgi?id=1559832).
4139         for flag in extra_flags:
4140             cpu.add_feature(vconfig.LibvirtConfigGuestCPUFeature(flag))
4141 
4142         return cpu
4143 
4144     def _get_guest_cpu_config(self, flavor, image_meta,
4145                               guest_cpu_numa_config, instance_numa_topology):
4146         cpu = self._get_guest_cpu_model_config()
4147 
4148         if cpu is None:
4149             return None
4150 
4151         topology = hardware.get_best_cpu_topology(
4152                 flavor, image_meta, numa_topology=instance_numa_topology)
4153 
4154         cpu.sockets = topology.sockets
4155         cpu.cores = topology.cores
4156         cpu.threads = topology.threads
4157         cpu.numa = guest_cpu_numa_config
4158 
4159         return cpu
4160 
4161     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
4162                                image_type=None):
4163         disk_unit = None
4164         disk = self.image_backend.by_name(instance, name, image_type)
4165         if (name == 'disk.config' and image_type == 'rbd' and
4166                 not disk.exists()):
4167             # This is likely an older config drive that has not been migrated
4168             # to rbd yet. Try to fall back on 'flat' image type.
4169             # TODO(melwitt): Add online migration of some sort so we can
4170             # remove this fall back once we know all config drives are in rbd.
4171             # NOTE(vladikr): make sure that the flat image exist, otherwise
4172             # the image will be created after the domain definition.
4173             flat_disk = self.image_backend.by_name(instance, name, 'flat')
4174             if flat_disk.exists():
4175                 disk = flat_disk
4176                 LOG.debug('Config drive not found in RBD, falling back to the '
4177                           'instance directory', instance=instance)
4178         disk_info = disk_mapping[name]
4179         if 'unit' in disk_mapping and disk_info['bus'] == 'scsi':
4180             disk_unit = disk_mapping['unit']
4181             disk_mapping['unit'] += 1  # Increments for the next disk added
4182         conf = disk.libvirt_info(disk_info, self.disk_cachemode,
4183                                  inst_type['extra_specs'],
4184                                  self._host.get_version(),
4185                                  disk_unit=disk_unit)
4186         return conf
4187 
4188     def _get_guest_fs_config(self, instance, name, image_type=None):
4189         disk = self.image_backend.by_name(instance, name, image_type)
4190         return disk.libvirt_fs_info("/", "ploop")
4191 
4192     def _get_guest_storage_config(self, context, instance, image_meta,
4193                                   disk_info,
4194                                   rescue, block_device_info,
4195                                   inst_type, os_type):
4196         devices = []
4197         disk_mapping = disk_info['mapping']
4198 
4199         block_device_mapping = driver.block_device_info_get_mapping(
4200             block_device_info)
4201         mount_rootfs = CONF.libvirt.virt_type == "lxc"
4202         scsi_controller = self._get_scsi_controller(image_meta)
4203 
4204         if scsi_controller and scsi_controller.model == 'virtio-scsi':
4205             # The virtio-scsi can handle up to 256 devices but the
4206             # optional element "address" must be defined to describe
4207             # where the device is placed on the controller (see:
4208             # LibvirtConfigGuestDeviceAddressDrive).
4209             #
4210             # Note about why it's added in disk_mapping: It's not
4211             # possible to pass an 'int' by reference in Python, so we
4212             # use disk_mapping as container to keep reference of the
4213             # unit added and be able to increment it for each disk
4214             # added.
4215             #
4216             # NOTE(jaypipes,melwitt): If this is a boot-from-volume instance,
4217             # we need to start the disk mapping unit at 1 since we set the
4218             # bootable volume's unit to 0 for the bootable volume.
4219             disk_mapping['unit'] = 0
4220             if self._is_booted_from_volume(block_device_info):
4221                 disk_mapping['unit'] = 1
4222 
4223         def _get_ephemeral_devices():
4224             eph_devices = []
4225             for idx, eph in enumerate(
4226                 driver.block_device_info_get_ephemerals(
4227                     block_device_info)):
4228                 diskeph = self._get_guest_disk_config(
4229                     instance,
4230                     blockinfo.get_eph_disk(idx),
4231                     disk_mapping, inst_type)
4232                 eph_devices.append(diskeph)
4233             return eph_devices
4234 
4235         if mount_rootfs:
4236             fs = vconfig.LibvirtConfigGuestFilesys()
4237             fs.source_type = "mount"
4238             fs.source_dir = os.path.join(
4239                 libvirt_utils.get_instance_path(instance), 'rootfs')
4240             devices.append(fs)
4241         elif (os_type == fields.VMMode.EXE and
4242               CONF.libvirt.virt_type == "parallels"):
4243             if rescue:
4244                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
4245                 devices.append(fsrescue)
4246 
4247                 fsos = self._get_guest_fs_config(instance, "disk")
4248                 fsos.target_dir = "/mnt/rescue"
4249                 devices.append(fsos)
4250             else:
4251                 if 'disk' in disk_mapping:
4252                     fs = self._get_guest_fs_config(instance, "disk")
4253                     devices.append(fs)
4254                 devices = devices + _get_ephemeral_devices()
4255         else:
4256 
4257             if rescue:
4258                 diskrescue = self._get_guest_disk_config(instance,
4259                                                          'disk.rescue',
4260                                                          disk_mapping,
4261                                                          inst_type)
4262                 devices.append(diskrescue)
4263 
4264                 diskos = self._get_guest_disk_config(instance,
4265                                                      'disk',
4266                                                      disk_mapping,
4267                                                      inst_type)
4268                 devices.append(diskos)
4269             else:
4270                 if 'disk' in disk_mapping:
4271                     diskos = self._get_guest_disk_config(instance,
4272                                                          'disk',
4273                                                          disk_mapping,
4274                                                          inst_type)
4275                     devices.append(diskos)
4276 
4277                 if 'disk.local' in disk_mapping:
4278                     disklocal = self._get_guest_disk_config(instance,
4279                                                             'disk.local',
4280                                                             disk_mapping,
4281                                                             inst_type)
4282                     devices.append(disklocal)
4283                     instance.default_ephemeral_device = (
4284                         block_device.prepend_dev(disklocal.target_dev))
4285 
4286                 devices = devices + _get_ephemeral_devices()
4287 
4288                 if 'disk.swap' in disk_mapping:
4289                     diskswap = self._get_guest_disk_config(instance,
4290                                                            'disk.swap',
4291                                                            disk_mapping,
4292                                                            inst_type)
4293                     devices.append(diskswap)
4294                     instance.default_swap_device = (
4295                         block_device.prepend_dev(diskswap.target_dev))
4296 
4297             config_name = 'disk.config.rescue' if rescue else 'disk.config'
4298             if config_name in disk_mapping:
4299                 diskconfig = self._get_guest_disk_config(
4300                     instance, config_name, disk_mapping, inst_type,
4301                     self._get_disk_config_image_type())
4302                 devices.append(diskconfig)
4303 
4304         for vol in block_device.get_bdms_to_connect(block_device_mapping,
4305                                                    mount_rootfs):
4306             connection_info = vol['connection_info']
4307             vol_dev = block_device.prepend_dev(vol['mount_device'])
4308             info = disk_mapping[vol_dev]
4309             self._connect_volume(context, connection_info, instance)
4310             if scsi_controller and scsi_controller.model == 'virtio-scsi':
4311                 # Check if this is the bootable volume when in a
4312                 # boot-from-volume instance, and if so, ensure the unit
4313                 # attribute is 0.
4314                 if vol.get('boot_index') == 0:
4315                     info['unit'] = 0
4316                 else:
4317                     info['unit'] = disk_mapping['unit']
4318                     disk_mapping['unit'] += 1
4319             cfg = self._get_volume_config(connection_info, info)
4320             devices.append(cfg)
4321             vol['connection_info'] = connection_info
4322             vol.save()
4323 
4324         for d in devices:
4325             self._set_cache_mode(d)
4326 
4327         if scsi_controller:
4328             devices.append(scsi_controller)
4329 
4330         return devices
4331 
4332     @staticmethod
4333     def _get_scsi_controller(image_meta):
4334         """Return scsi controller or None based on image meta"""
4335         if image_meta.properties.get('hw_scsi_model'):
4336             hw_scsi_model = image_meta.properties.hw_scsi_model
4337             scsi_controller = vconfig.LibvirtConfigGuestController()
4338             scsi_controller.type = 'scsi'
4339             scsi_controller.model = hw_scsi_model
4340             scsi_controller.index = 0
4341             return scsi_controller
4342 
4343     def _get_host_sysinfo_serial_hardware(self):
4344         """Get a UUID from the host hardware
4345 
4346         Get a UUID for the host hardware reported by libvirt.
4347         This is typically from the SMBIOS data, unless it has
4348         been overridden in /etc/libvirt/libvirtd.conf
4349         """
4350         caps = self._host.get_capabilities()
4351         return caps.host.uuid
4352 
4353     def _get_host_sysinfo_serial_os(self):
4354         """Get a UUID from the host operating system
4355 
4356         Get a UUID for the host operating system. Modern Linux
4357         distros based on systemd provide a /etc/machine-id
4358         file containing a UUID. This is also provided inside
4359         systemd based containers and can be provided by other
4360         init systems too, since it is just a plain text file.
4361         """
4362         if not os.path.exists("/etc/machine-id"):
4363             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
4364             raise exception.InternalError(msg)
4365 
4366         with open("/etc/machine-id") as f:
4367             # We want to have '-' in the right place
4368             # so we parse & reformat the value
4369             lines = f.read().split()
4370             if not lines:
4371                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
4372                 raise exception.InternalError(msg)
4373 
4374             return str(uuid.UUID(lines[0]))
4375 
4376     def _get_host_sysinfo_serial_auto(self):
4377         if os.path.exists("/etc/machine-id"):
4378             return self._get_host_sysinfo_serial_os()
4379         else:
4380             return self._get_host_sysinfo_serial_hardware()
4381 
4382     def _get_guest_config_sysinfo(self, instance):
4383         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
4384 
4385         sysinfo.system_manufacturer = version.vendor_string()
4386         sysinfo.system_product = version.product_string()
4387         sysinfo.system_version = version.version_string_with_package()
4388 
4389         if CONF.libvirt.sysinfo_serial == 'unique':
4390             sysinfo.system_serial = instance.uuid
4391         else:
4392             sysinfo.system_serial = self._sysinfo_serial_func()
4393         sysinfo.system_uuid = instance.uuid
4394 
4395         sysinfo.system_family = "Virtual Machine"
4396 
4397         return sysinfo
4398 
4399     def _get_guest_pci_device(self, pci_device):
4400 
4401         dbsf = pci_utils.parse_address(pci_device.address)
4402         dev = vconfig.LibvirtConfigGuestHostdevPCI()
4403         dev.domain, dev.bus, dev.slot, dev.function = dbsf
4404 
4405         # only kvm support managed mode
4406         if CONF.libvirt.virt_type in ('xen', 'parallels',):
4407             dev.managed = 'no'
4408         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
4409             dev.managed = 'yes'
4410 
4411         return dev
4412 
4413     def _get_guest_config_meta(self, instance):
4414         """Get metadata config for guest."""
4415 
4416         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
4417         meta.package = version.version_string_with_package()
4418         meta.name = instance.display_name
4419         meta.creationTime = time.time()
4420 
4421         if instance.image_ref not in ("", None):
4422             meta.roottype = "image"
4423             meta.rootid = instance.image_ref
4424 
4425         system_meta = instance.system_metadata
4426         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
4427         ometa.userid = instance.user_id
4428         ometa.username = system_meta.get('owner_user_name', 'N/A')
4429         ometa.projectid = instance.project_id
4430         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
4431         meta.owner = ometa
4432 
4433         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
4434         flavor = instance.flavor
4435         fmeta.name = flavor.name
4436         fmeta.memory = flavor.memory_mb
4437         fmeta.vcpus = flavor.vcpus
4438         fmeta.ephemeral = flavor.ephemeral_gb
4439         fmeta.disk = flavor.root_gb
4440         fmeta.swap = flavor.swap
4441 
4442         meta.flavor = fmeta
4443 
4444         return meta
4445 
4446     @staticmethod
4447     def _create_idmaps(klass, map_strings):
4448         idmaps = []
4449         if len(map_strings) > 5:
4450             map_strings = map_strings[0:5]
4451             LOG.warning("Too many id maps, only included first five.")
4452         for map_string in map_strings:
4453             try:
4454                 idmap = klass()
4455                 values = [int(i) for i in map_string.split(":")]
4456                 idmap.start = values[0]
4457                 idmap.target = values[1]
4458                 idmap.count = values[2]
4459                 idmaps.append(idmap)
4460             except (ValueError, IndexError):
4461                 LOG.warning("Invalid value for id mapping %s", map_string)
4462         return idmaps
4463 
4464     def _get_guest_idmaps(self):
4465         id_maps = []
4466         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
4467             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
4468                                            CONF.libvirt.uid_maps)
4469             id_maps.extend(uid_maps)
4470         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
4471             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
4472                                            CONF.libvirt.gid_maps)
4473             id_maps.extend(gid_maps)
4474         return id_maps
4475 
4476     def _update_guest_cputune(self, guest, flavor, virt_type):
4477         is_able = self._host.is_cpu_control_policy_capable()
4478 
4479         cputuning = ['shares', 'period', 'quota']
4480         wants_cputune = any([k for k in cputuning
4481             if "quota:cpu_" + k in flavor.extra_specs.keys()])
4482 
4483         if wants_cputune and not is_able:
4484             raise exception.UnsupportedHostCPUControlPolicy()
4485 
4486         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
4487             return
4488 
4489         if guest.cputune is None:
4490             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
4491             # Setting the default cpu.shares value to be a value
4492             # dependent on the number of vcpus
4493         guest.cputune.shares = 1024 * guest.vcpus
4494 
4495         for name in cputuning:
4496             key = "quota:cpu_" + name
4497             if key in flavor.extra_specs:
4498                 setattr(guest.cputune, name,
4499                         int(flavor.extra_specs[key]))
4500 
4501     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
4502                                            wants_hugepages):
4503         if instance_numa_topology:
4504             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
4505             for instance_cell in instance_numa_topology.cells:
4506                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
4507                 guest_cell.id = instance_cell.id
4508                 guest_cell.cpus = instance_cell.cpuset
4509                 guest_cell.memory = instance_cell.memory * units.Ki
4510 
4511                 # The vhost-user network backend requires file backed
4512                 # guest memory (ie huge pages) to be marked as shared
4513                 # access, not private, so an external process can read
4514                 # and write the pages.
4515                 #
4516                 # You can't change the shared vs private flag for an
4517                 # already running guest, and since we can't predict what
4518                 # types of NIC may be hotplugged, we have no choice but
4519                 # to unconditionally turn on the shared flag. This has
4520                 # no real negative functional effect on the guest, so
4521                 # is a reasonable approach to take
4522                 if wants_hugepages:
4523                     guest_cell.memAccess = "shared"
4524                 guest_cpu_numa.cells.append(guest_cell)
4525             return guest_cpu_numa
4526 
4527     def _wants_hugepages(self, host_topology, instance_topology):
4528         """Determine if the guest / host topology implies the
4529            use of huge pages for guest RAM backing
4530         """
4531 
4532         if host_topology is None or instance_topology is None:
4533             return False
4534 
4535         avail_pagesize = [page.size_kb
4536                           for page in host_topology.cells[0].mempages]
4537         avail_pagesize.sort()
4538         # Remove smallest page size as that's not classed as a largepage
4539         avail_pagesize = avail_pagesize[1:]
4540 
4541         # See if we have page size set
4542         for cell in instance_topology.cells:
4543             if (cell.pagesize is not None and
4544                 cell.pagesize in avail_pagesize):
4545                 return True
4546 
4547         return False
4548 
4549     def _get_cell_pairs(self, guest_cpu_numa_config, host_topology):
4550         """Returns the lists of pairs(tuple) of an instance cell and
4551         corresponding host cell:
4552             [(LibvirtConfigGuestCPUNUMACell, NUMACell), ...]
4553         """
4554         cell_pairs = []
4555         for guest_config_cell in guest_cpu_numa_config.cells:
4556             for host_cell in host_topology.cells:
4557                 if guest_config_cell.id == host_cell.id:
4558                     cell_pairs.append((guest_config_cell, host_cell))
4559         return cell_pairs
4560 
4561     def _get_pin_cpuset(self, vcpu, object_numa_cell, host_cell):
4562         """Returns the config object of LibvirtConfigGuestCPUTuneVCPUPin.
4563         Prepares vcpupin config for the guest with the following caveats:
4564 
4565             a) If there is pinning information in the cell, we pin vcpus to
4566                individual CPUs
4567             b) Otherwise we float over the whole host NUMA node
4568         """
4569         pin_cpuset = vconfig.LibvirtConfigGuestCPUTuneVCPUPin()
4570         pin_cpuset.id = vcpu
4571 
4572         if object_numa_cell.cpu_pinning:
4573             pin_cpuset.cpuset = set([object_numa_cell.cpu_pinning[vcpu]])
4574         else:
4575             pin_cpuset.cpuset = host_cell.cpuset
4576 
4577         return pin_cpuset
4578 
4579     def _get_emulatorpin_cpuset(self, vcpu, object_numa_cell, vcpus_rt,
4580                                 emulator_threads_policy, wants_realtime,
4581                                 pin_cpuset):
4582         """Returns a set of cpu_ids to add to the cpuset for emulator threads
4583            with the following caveats:
4584 
4585             a) If emulator threads policy is isolated, we pin emulator threads
4586                to one cpu we have reserved for it.
4587             b) If emulator threads policy is shared and CONF.cpu_shared_set is
4588                defined, we pin emulator threads on the set of pCPUs defined by
4589                CONF.cpu_shared_set
4590             c) Otherwise;
4591                 c1) If realtime IS NOT enabled, the emulator threads are
4592                     allowed to float cross all the pCPUs associated with
4593                     the guest vCPUs.
4594                 c2) If realtime IS enabled, at least 1 vCPU is required
4595                     to be set aside for non-realtime usage. The emulator
4596                     threads are allowed to float across the pCPUs that
4597                     are associated with the non-realtime VCPUs.
4598         """
4599         emulatorpin_cpuset = set([])
4600         shared_ids = hardware.get_cpu_shared_set()
4601 
4602         if emulator_threads_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
4603             if object_numa_cell.cpuset_reserved:
4604                 emulatorpin_cpuset = object_numa_cell.cpuset_reserved
4605         elif ((emulator_threads_policy ==
4606               fields.CPUEmulatorThreadsPolicy.SHARE) and
4607               shared_ids):
4608             online_pcpus = self._host.get_online_cpus()
4609             cpuset = shared_ids & online_pcpus
4610             if not cpuset:
4611                 msg = (_("Invalid cpu_shared_set config, one or more of the "
4612                          "specified cpuset is not online. Online cpuset(s): "
4613                          "%(online)s, requested cpuset(s): %(req)s"),
4614                        {'online': sorted(online_pcpus),
4615                         'req': sorted(shared_ids)})
4616                 raise exception.Invalid(msg)
4617             emulatorpin_cpuset = cpuset
4618         elif not wants_realtime or vcpu not in vcpus_rt:
4619             emulatorpin_cpuset = pin_cpuset.cpuset
4620 
4621         return emulatorpin_cpuset
4622 
4623     def _get_guest_numa_config(self, instance_numa_topology, flavor,
4624                                allowed_cpus=None, image_meta=None):
4625         """Returns the config objects for the guest NUMA specs.
4626 
4627         Determines the CPUs that the guest can be pinned to if the guest
4628         specifies a cell topology and the host supports it. Constructs the
4629         libvirt XML config object representing the NUMA topology selected
4630         for the guest. Returns a tuple of:
4631 
4632             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
4633 
4634         With the following caveats:
4635 
4636             a) If there is no specified guest NUMA topology, then
4637                all tuple elements except cpu_set shall be None. cpu_set
4638                will be populated with the chosen CPUs that the guest
4639                allowed CPUs fit within, which could be the supplied
4640                allowed_cpus value if the host doesn't support NUMA
4641                topologies.
4642 
4643             b) If there is a specified guest NUMA topology, then
4644                cpu_set will be None and guest_cpu_numa will be the
4645                LibvirtConfigGuestCPUNUMA object representing the guest's
4646                NUMA topology. If the host supports NUMA, then guest_cpu_tune
4647                will contain a LibvirtConfigGuestCPUTune object representing
4648                the optimized chosen cells that match the host capabilities
4649                with the instance's requested topology. If the host does
4650                not support NUMA, then guest_cpu_tune and guest_numa_tune
4651                will be None.
4652         """
4653 
4654         if (not self._has_numa_support() and
4655                 instance_numa_topology is not None):
4656             # We should not get here, since we should have avoided
4657             # reporting NUMA topology from _get_host_numa_topology
4658             # in the first place. Just in case of a scheduler
4659             # mess up though, raise an exception
4660             raise exception.NUMATopologyUnsupported()
4661 
4662         topology = self._get_host_numa_topology()
4663 
4664         # We have instance NUMA so translate it to the config class
4665         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
4666                 instance_numa_topology,
4667                 self._wants_hugepages(topology, instance_numa_topology))
4668 
4669         if not guest_cpu_numa_config:
4670             # No NUMA topology defined for instance - let the host kernel deal
4671             # with the NUMA effects.
4672             # TODO(ndipanov): Attempt to spread the instance
4673             # across NUMA nodes and expose the topology to the
4674             # instance as an optimisation
4675             return GuestNumaConfig(allowed_cpus, None, None, None)
4676 
4677         if not topology:
4678             # No NUMA topology defined for host - This will only happen with
4679             # some libvirt versions and certain platforms.
4680             return GuestNumaConfig(allowed_cpus, None,
4681                                    guest_cpu_numa_config, None)
4682 
4683         # Now get configuration from the numa_topology
4684         # Init CPUTune configuration
4685         guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
4686         guest_cpu_tune.emulatorpin = (
4687             vconfig.LibvirtConfigGuestCPUTuneEmulatorPin())
4688         guest_cpu_tune.emulatorpin.cpuset = set([])
4689 
4690         # Init NUMATune configuration
4691         guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
4692         guest_numa_tune.memory = vconfig.LibvirtConfigGuestNUMATuneMemory()
4693         guest_numa_tune.memnodes = []
4694 
4695         emulator_threads_policy = None
4696         if 'emulator_threads_policy' in instance_numa_topology:
4697             emulator_threads_policy = (
4698                 instance_numa_topology.emulator_threads_policy)
4699 
4700         # Set realtime scheduler for CPUTune
4701         vcpus_rt = set([])
4702         wants_realtime = hardware.is_realtime_enabled(flavor)
4703         if wants_realtime:
4704             vcpus_rt = hardware.vcpus_realtime_topology(flavor, image_meta)
4705             vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
4706             designer.set_vcpu_realtime_scheduler(
4707                 vcpusched, vcpus_rt, CONF.libvirt.realtime_scheduler_priority)
4708             guest_cpu_tune.vcpusched.append(vcpusched)
4709 
4710         cell_pairs = self._get_cell_pairs(guest_cpu_numa_config, topology)
4711         for guest_node_id, (guest_config_cell, host_cell) in enumerate(
4712                 cell_pairs):
4713             # set NUMATune for the cell
4714             tnode = vconfig.LibvirtConfigGuestNUMATuneMemNode()
4715             designer.set_numa_memnode(tnode, guest_node_id, host_cell.id)
4716             guest_numa_tune.memnodes.append(tnode)
4717             guest_numa_tune.memory.nodeset.append(host_cell.id)
4718 
4719             # set CPUTune for the cell
4720             object_numa_cell = instance_numa_topology.cells[guest_node_id]
4721             for cpu in guest_config_cell.cpus:
4722                 pin_cpuset = self._get_pin_cpuset(cpu, object_numa_cell,
4723                                                   host_cell)
4724                 guest_cpu_tune.vcpupin.append(pin_cpuset)
4725 
4726                 emu_pin_cpuset = self._get_emulatorpin_cpuset(
4727                     cpu, object_numa_cell, vcpus_rt,
4728                     emulator_threads_policy, wants_realtime, pin_cpuset)
4729                 guest_cpu_tune.emulatorpin.cpuset.update(emu_pin_cpuset)
4730 
4731         # TODO(berrange) When the guest has >1 NUMA node, it will
4732         # span multiple host NUMA nodes. By pinning emulator threads
4733         # to the union of all nodes, we guarantee there will be
4734         # cross-node memory access by the emulator threads when
4735         # responding to guest I/O operations. The only way to avoid
4736         # this would be to pin emulator threads to a single node and
4737         # tell the guest OS to only do I/O from one of its virtual
4738         # NUMA nodes. This is not even remotely practical.
4739         #
4740         # The long term solution is to make use of a new QEMU feature
4741         # called "I/O Threads" which will let us configure an explicit
4742         # I/O thread for each guest vCPU or guest NUMA node. It is
4743         # still TBD how to make use of this feature though, especially
4744         # how to associate IO threads with guest devices to eliminate
4745         # cross NUMA node traffic. This is an area of investigation
4746         # for QEMU community devs.
4747 
4748         # Sort the vcpupin list per vCPU id for human-friendlier XML
4749         guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
4750 
4751         # normalize cell.id
4752         for i, (cell, memnode) in enumerate(zip(guest_cpu_numa_config.cells,
4753                                                 guest_numa_tune.memnodes)):
4754             cell.id = i
4755             memnode.cellid = i
4756 
4757         return GuestNumaConfig(None, guest_cpu_tune, guest_cpu_numa_config,
4758                                guest_numa_tune)
4759 
4760     def _get_guest_os_type(self, virt_type):
4761         """Returns the guest OS type based on virt type."""
4762         if virt_type == "lxc":
4763             ret = fields.VMMode.EXE
4764         elif virt_type == "uml":
4765             ret = fields.VMMode.UML
4766         elif virt_type == "xen":
4767             ret = fields.VMMode.XEN
4768         else:
4769             ret = fields.VMMode.HVM
4770         return ret
4771 
4772     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
4773                               root_device_name):
4774         if rescue.get('kernel_id'):
4775             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
4776             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4777             if virt_type == "qemu":
4778                 guest.os_cmdline += " no_timer_check"
4779         if rescue.get('ramdisk_id'):
4780             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
4781 
4782     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
4783                                 root_device_name, image_meta):
4784         guest.os_kernel = os.path.join(inst_path, "kernel")
4785         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4786         if virt_type == "qemu":
4787             guest.os_cmdline += " no_timer_check"
4788         if instance.ramdisk_id:
4789             guest.os_initrd = os.path.join(inst_path, "ramdisk")
4790         # we only support os_command_line with images with an explicit
4791         # kernel set and don't want to break nova if there's an
4792         # os_command_line property without a specified kernel_id param
4793         if image_meta.properties.get("os_command_line"):
4794             guest.os_cmdline = image_meta.properties.os_command_line
4795 
4796     def _set_clock(self, guest, os_type, image_meta, virt_type):
4797         # NOTE(mikal): Microsoft Windows expects the clock to be in
4798         # "localtime". If the clock is set to UTC, then you can use a
4799         # registry key to let windows know, but Microsoft says this is
4800         # buggy in http://support.microsoft.com/kb/2687252
4801         clk = vconfig.LibvirtConfigGuestClock()
4802         if os_type == 'windows':
4803             LOG.info('Configuring timezone for windows instance to localtime')
4804             clk.offset = 'localtime'
4805         else:
4806             clk.offset = 'utc'
4807         guest.set_clock(clk)
4808 
4809         if virt_type == "kvm":
4810             self._set_kvm_timers(clk, os_type, image_meta)
4811 
4812     def _set_kvm_timers(self, clk, os_type, image_meta):
4813         # TODO(berrange) One day this should be per-guest
4814         # OS type configurable
4815         tmpit = vconfig.LibvirtConfigGuestTimer()
4816         tmpit.name = "pit"
4817         tmpit.tickpolicy = "delay"
4818 
4819         tmrtc = vconfig.LibvirtConfigGuestTimer()
4820         tmrtc.name = "rtc"
4821         tmrtc.tickpolicy = "catchup"
4822 
4823         clk.add_timer(tmpit)
4824         clk.add_timer(tmrtc)
4825 
4826         hpet = image_meta.properties.get('hw_time_hpet', False)
4827         guestarch = libvirt_utils.get_arch(image_meta)
4828         if guestarch in (fields.Architecture.I686,
4829                          fields.Architecture.X86_64):
4830             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
4831             # qemu -no-hpet is not supported on non-x86 targets.
4832             tmhpet = vconfig.LibvirtConfigGuestTimer()
4833             tmhpet.name = "hpet"
4834             tmhpet.present = hpet
4835             clk.add_timer(tmhpet)
4836         else:
4837             if hpet:
4838                 LOG.warning('HPET is not turned on for non-x86 guests in image'
4839                             ' %s.', image_meta.id)
4840 
4841         # Provide Windows guests with the paravirtualized hyperv timer source.
4842         # This is the windows equiv of kvm-clock, allowing Windows
4843         # guests to accurately keep time.
4844         if os_type == 'windows':
4845             tmhyperv = vconfig.LibvirtConfigGuestTimer()
4846             tmhyperv.name = "hypervclock"
4847             tmhyperv.present = True
4848             clk.add_timer(tmhyperv)
4849 
4850     def _set_features(self, guest, os_type, caps, virt_type, image_meta,
4851             flavor):
4852         hide_hypervisor_id = (strutils.bool_from_string(
4853                 flavor.extra_specs.get('hide_hypervisor_id')) or
4854             image_meta.properties.get('img_hide_hypervisor_id'))
4855 
4856         if virt_type == "xen":
4857             # PAE only makes sense in X86
4858             if caps.host.cpu.arch in (fields.Architecture.I686,
4859                                       fields.Architecture.X86_64):
4860                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
4861 
4862         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
4863                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
4864             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
4865             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
4866 
4867         if (virt_type in ("qemu", "kvm") and
4868                 os_type == 'windows'):
4869             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
4870             hv.relaxed = True
4871 
4872             hv.spinlocks = True
4873             # Increase spinlock retries - value recommended by
4874             # KVM maintainers who certify Windows guests
4875             # with Microsoft
4876             hv.spinlock_retries = 8191
4877             hv.vapic = True
4878 
4879             # NOTE(kosamara): Spoofing the vendor_id aims to allow the nvidia
4880             # driver to work on windows VMs. At the moment, the nvidia driver
4881             # checks for the hyperv vendorid, and if it doesn't find that, it
4882             # works. In the future, its behaviour could become more strict,
4883             # checking for the presence of other hyperv feature flags to
4884             # determine that it's loaded in a VM. If that happens, this
4885             # workaround will not be enough, and we'll need to drop the whole
4886             # hyperv element.
4887             # That would disable some optimizations, reducing the guest's
4888             # performance.
4889             if hide_hypervisor_id:
4890                 hv.vendorid_spoof = True
4891 
4892             guest.features.append(hv)
4893 
4894         if (virt_type in ("qemu", "kvm") and hide_hypervisor_id):
4895             guest.features.append(vconfig.LibvirtConfigGuestFeatureKvmHidden())
4896 
4897     def _check_number_of_serial_console(self, num_ports):
4898         virt_type = CONF.libvirt.virt_type
4899         if (virt_type in ("kvm", "qemu") and
4900             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
4901             raise exception.SerialPortNumberLimitExceeded(
4902                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
4903 
4904     def _video_model_supported(self, model):
4905         if model not in fields.VideoModel.ALL:
4906             return False
4907         min_ver = MIN_LIBVIRT_VIDEO_MODEL_VERSIONS.get(model)
4908         if min_ver and not self._host.has_min_version(lv_ver=min_ver):
4909             return False
4910         return True
4911 
4912     def _add_video_driver(self, guest, image_meta, flavor):
4913         video = vconfig.LibvirtConfigGuestVideo()
4914         # NOTE(ldbragst): The following logic sets the video.type
4915         # depending on supported defaults given the architecture,
4916         # virtualization type, and features. The video.type attribute can
4917         # be overridden by the user with image_meta.properties, which
4918         # is carried out in the next if statement below this one.
4919         guestarch = libvirt_utils.get_arch(image_meta)
4920         if guest.os_type == fields.VMMode.XEN:
4921             video.type = 'xen'
4922         elif CONF.libvirt.virt_type == 'parallels':
4923             video.type = 'vga'
4924         elif guestarch in (fields.Architecture.PPC,
4925                            fields.Architecture.PPC64,
4926                            fields.Architecture.PPC64LE):
4927             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
4928             # so use 'vga' instead when running on Power hardware.
4929             video.type = 'vga'
4930         elif guestarch == fields.Architecture.AARCH64:
4931             # NOTE(kevinz): Only virtio device type is supported by AARCH64
4932             # so use 'virtio' instead when running on AArch64 hardware.
4933             video.type = 'virtio'
4934         elif CONF.spice.enabled:
4935             video.type = 'qxl'
4936         if image_meta.properties.get('hw_video_model'):
4937             video.type = image_meta.properties.hw_video_model
4938             if not self._video_model_supported(video.type):
4939                 raise exception.InvalidVideoMode(model=video.type)
4940 
4941         # Set video memory, only if the flavor's limit is set
4942         video_ram = image_meta.properties.get('hw_video_ram', 0)
4943         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
4944         if video_ram > max_vram:
4945             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
4946                                                  max_vram=max_vram)
4947         if max_vram and video_ram:
4948             video.vram = video_ram * units.Mi / units.Ki
4949         guest.add_device(video)
4950 
4951         # NOTE(sean-k-mooney): return the video device we added
4952         # for simpler testing.
4953         return video
4954 
4955     def _add_qga_device(self, guest, instance):
4956         qga = vconfig.LibvirtConfigGuestChannel()
4957         qga.type = "unix"
4958         qga.target_name = "org.qemu.guest_agent.0"
4959         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
4960                           ("org.qemu.guest_agent.0", instance.name))
4961         guest.add_device(qga)
4962 
4963     def _add_rng_device(self, guest, flavor):
4964         rng_device = vconfig.LibvirtConfigGuestRng()
4965         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
4966         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
4967         if rate_bytes:
4968             rng_device.rate_bytes = int(rate_bytes)
4969             rng_device.rate_period = int(period)
4970         rng_path = CONF.libvirt.rng_dev_path
4971         if (rng_path and not os.path.exists(rng_path)):
4972             raise exception.RngDeviceNotExist(path=rng_path)
4973         rng_device.backend = rng_path
4974         guest.add_device(rng_device)
4975 
4976     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
4977         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
4978         if image_meta.properties.get('hw_qemu_guest_agent', False):
4979             LOG.debug("Qemu guest agent is enabled through image "
4980                       "metadata", instance=instance)
4981             self._add_qga_device(guest, instance)
4982         rng_is_virtio = image_meta.properties.get('hw_rng_model') == 'virtio'
4983         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
4984         rng_allowed = strutils.bool_from_string(rng_allowed_str)
4985         if rng_is_virtio and rng_allowed:
4986             self._add_rng_device(guest, flavor)
4987 
4988     def _get_guest_memory_backing_config(
4989             self, inst_topology, numatune, flavor):
4990         wantsmempages = False
4991         if inst_topology:
4992             for cell in inst_topology.cells:
4993                 if cell.pagesize:
4994                     wantsmempages = True
4995                     break
4996 
4997         wantsrealtime = hardware.is_realtime_enabled(flavor)
4998 
4999         wantsfilebacked = CONF.libvirt.file_backed_memory > 0
5000 
5001         if wantsmempages and wantsfilebacked:
5002             # Can't use file-backed memory with hugepages
5003             LOG.warning("Instance requested huge pages, but file-backed "
5004                     "memory is enabled, and incompatible with huge pages")
5005             raise exception.MemoryPagesUnsupported()
5006 
5007         membacking = None
5008         if wantsmempages:
5009             pages = self._get_memory_backing_hugepages_support(
5010                 inst_topology, numatune)
5011             if pages:
5012                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
5013                 membacking.hugepages = pages
5014         if wantsrealtime:
5015             if not membacking:
5016                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
5017             membacking.locked = True
5018             membacking.sharedpages = False
5019         if wantsfilebacked:
5020             if not membacking:
5021                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
5022             membacking.filesource = True
5023             membacking.sharedaccess = True
5024             membacking.allocateimmediate = True
5025             if self._host.has_min_version(
5026                     MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION,
5027                     MIN_QEMU_FILE_BACKED_DISCARD_VERSION):
5028                 membacking.discard = True
5029 
5030         return membacking
5031 
5032     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
5033         if not self._has_numa_support():
5034             # We should not get here, since we should have avoided
5035             # reporting NUMA topology from _get_host_numa_topology
5036             # in the first place. Just in case of a scheduler
5037             # mess up though, raise an exception
5038             raise exception.MemoryPagesUnsupported()
5039 
5040         host_topology = self._get_host_numa_topology()
5041 
5042         if host_topology is None:
5043             # As above, we should not get here but just in case...
5044             raise exception.MemoryPagesUnsupported()
5045 
5046         # Currently libvirt does not support the smallest
5047         # pagesize set as a backend memory.
5048         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
5049         avail_pagesize = [page.size_kb
5050                           for page in host_topology.cells[0].mempages]
5051         avail_pagesize.sort()
5052         smallest = avail_pagesize[0]
5053 
5054         pages = []
5055         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
5056             if inst_cell.pagesize and inst_cell.pagesize > smallest:
5057                 for memnode in numatune.memnodes:
5058                     if guest_cellid == memnode.cellid:
5059                         page = (
5060                             vconfig.LibvirtConfigGuestMemoryBackingPage())
5061                         page.nodeset = [guest_cellid]
5062                         page.size_kb = inst_cell.pagesize
5063                         pages.append(page)
5064                         break  # Quit early...
5065         return pages
5066 
5067     def _get_flavor(self, ctxt, instance, flavor):
5068         if flavor is not None:
5069             return flavor
5070         return instance.flavor
5071 
5072     def _has_uefi_support(self):
5073         # This means that the host can support uefi booting for guests
5074         supported_archs = [fields.Architecture.X86_64,
5075                            fields.Architecture.AARCH64]
5076         caps = self._host.get_capabilities()
5077         return ((caps.host.cpu.arch in supported_archs) and
5078                 os.path.exists(DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]))
5079 
5080     def _get_supported_perf_events(self):
5081 
5082         if (len(CONF.libvirt.enabled_perf_events) == 0 or
5083              not self._host.has_min_version(MIN_LIBVIRT_PERF_VERSION)):
5084             return []
5085 
5086         supported_events = []
5087         host_cpu_info = self._get_cpu_info()
5088         for event in CONF.libvirt.enabled_perf_events:
5089             if self._supported_perf_event(event, host_cpu_info['features']):
5090                 supported_events.append(event)
5091         return supported_events
5092 
5093     def _supported_perf_event(self, event, cpu_features):
5094 
5095         libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
5096 
5097         if not hasattr(libvirt, libvirt_perf_event_name):
5098             LOG.warning("Libvirt doesn't support event type %s.", event)
5099             return False
5100 
5101         if event in PERF_EVENTS_CPU_FLAG_MAPPING:
5102             LOG.warning('Monitoring Intel CMT `perf` event(s) %s is '
5103                         'deprecated and will be removed in the "Stein" '
5104                         'release.  It was broken by design in the '
5105                         'Linux kernel, so support for Intel CMT was '
5106                         'removed from Linux 4.14 onwards. Therefore '
5107                         'it is recommended to not enable them.',
5108                         event)
5109             if PERF_EVENTS_CPU_FLAG_MAPPING[event] not in cpu_features:
5110                 LOG.warning("Host does not support event type %s.", event)
5111                 return False
5112         return True
5113 
5114     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
5115                                       image_meta, flavor, root_device_name):
5116         if virt_type == "xen":
5117             if guest.os_type == fields.VMMode.HVM:
5118                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
5119             else:
5120                 guest.os_cmdline = CONSOLE
5121         elif virt_type in ("kvm", "qemu"):
5122             if caps.host.cpu.arch in (fields.Architecture.I686,
5123                                       fields.Architecture.X86_64):
5124                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
5125                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
5126             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
5127             if caps.host.cpu.arch == fields.Architecture.AARCH64:
5128                 if not hw_firmware_type:
5129                     hw_firmware_type = fields.FirmwareType.UEFI
5130             if hw_firmware_type == fields.FirmwareType.UEFI:
5131                 if self._has_uefi_support():
5132                     global uefi_logged
5133                     if not uefi_logged:
5134                         LOG.warning("uefi support is without some kind of "
5135                                     "functional testing and therefore "
5136                                     "considered experimental.")
5137                         uefi_logged = True
5138                     guest.os_loader = DEFAULT_UEFI_LOADER_PATH[
5139                         caps.host.cpu.arch]
5140                     guest.os_loader_type = "pflash"
5141                 else:
5142                     raise exception.UEFINotSupported()
5143             guest.os_mach_type = libvirt_utils.get_machine_type(image_meta)
5144             if image_meta.properties.get('hw_boot_menu') is None:
5145                 guest.os_bootmenu = strutils.bool_from_string(
5146                     flavor.extra_specs.get('hw:boot_menu', 'no'))
5147             else:
5148                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
5149 
5150         elif virt_type == "lxc":
5151             guest.os_init_path = "/sbin/init"
5152             guest.os_cmdline = CONSOLE
5153         elif virt_type == "uml":
5154             guest.os_kernel = "/usr/bin/linux"
5155             guest.os_root = root_device_name
5156         elif virt_type == "parallels":
5157             if guest.os_type == fields.VMMode.EXE:
5158                 guest.os_init_path = "/sbin/init"
5159 
5160     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
5161                     instance, inst_path, image_meta, disk_info):
5162         if rescue:
5163             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
5164                                        root_device_name)
5165         elif instance.kernel_id:
5166             self._set_guest_for_inst_kernel(instance, guest, inst_path,
5167                                             virt_type, root_device_name,
5168                                             image_meta)
5169         else:
5170             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
5171 
5172     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
5173                          image_meta):
5174         # NOTE(markus_z): Beware! Below are so many conditionals that it is
5175         # easy to lose track. Use this chart to figure out your case:
5176         #
5177         # case | is serial | is qemu | resulting
5178         #      | enabled?  | or kvm? | devices
5179         # -------------------------------------------
5180         #    1 |        no |     no  | pty*
5181         #    2 |        no |     yes | pty with logd
5182         #    3 |       yes |      no | see case 1
5183         #    4 |       yes |     yes | tcp with logd
5184         #
5185         #    * exception: `virt_type=parallels` doesn't create a device
5186         if virt_type == 'parallels':
5187             pass
5188         elif virt_type not in ("qemu", "kvm"):
5189             log_path = self._get_console_log_path(instance)
5190             self._create_pty_device(guest_cfg,
5191                                     vconfig.LibvirtConfigGuestConsole,
5192                                     log_path=log_path)
5193         elif (virt_type in ("qemu", "kvm") and
5194                   self._is_s390x_guest(image_meta)):
5195             self._create_consoles_s390x(guest_cfg, instance,
5196                                         flavor, image_meta)
5197         elif virt_type in ("qemu", "kvm"):
5198             self._create_consoles_qemu_kvm(guest_cfg, instance,
5199                                         flavor, image_meta)
5200 
5201     def _is_s390x_guest(self, image_meta):
5202         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
5203         return libvirt_utils.get_arch(image_meta) in s390x_archs
5204 
5205     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
5206                                   image_meta):
5207         char_dev_cls = vconfig.LibvirtConfigGuestSerial
5208         log_path = self._get_console_log_path(instance)
5209         if CONF.serial_console.enabled:
5210             if not self._serial_ports_already_defined(instance):
5211                 num_ports = hardware.get_number_of_serial_ports(flavor,
5212                                                                 image_meta)
5213                 self._check_number_of_serial_console(num_ports)
5214                 self._create_serial_consoles(guest_cfg, num_ports,
5215                                              char_dev_cls, log_path)
5216         else:
5217             self._create_pty_device(guest_cfg, char_dev_cls,
5218                                     log_path=log_path)
5219 
5220     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
5221         char_dev_cls = vconfig.LibvirtConfigGuestConsole
5222         log_path = self._get_console_log_path(instance)
5223         if CONF.serial_console.enabled:
5224             if not self._serial_ports_already_defined(instance):
5225                 num_ports = hardware.get_number_of_serial_ports(flavor,
5226                                                                 image_meta)
5227                 self._create_serial_consoles(guest_cfg, num_ports,
5228                                              char_dev_cls, log_path)
5229         else:
5230             self._create_pty_device(guest_cfg, char_dev_cls,
5231                                     "sclp", log_path)
5232 
5233     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
5234                            log_path=None):
5235 
5236         consolepty = char_dev_cls()
5237         consolepty.target_type = target_type
5238         consolepty.type = "pty"
5239 
5240         log = vconfig.LibvirtConfigGuestCharDeviceLog()
5241         log.file = log_path
5242         consolepty.log = log
5243 
5244         guest_cfg.add_device(consolepty)
5245 
5246     def _serial_ports_already_defined(self, instance):
5247         try:
5248             guest = self._host.get_guest(instance)
5249             if list(self._get_serial_ports_from_guest(guest)):
5250                 # Serial port are already configured for instance that
5251                 # means we are in a context of migration.
5252                 return True
5253         except exception.InstanceNotFound:
5254             LOG.debug(
5255                 "Instance does not exist yet on libvirt, we can "
5256                 "safely pass on looking for already defined serial "
5257                 "ports in its domain XML", instance=instance)
5258         return False
5259 
5260     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
5261                                 log_path):
5262         for port in six.moves.range(num_ports):
5263             console = char_dev_cls()
5264             console.port = port
5265             console.type = "tcp"
5266             console.listen_host = CONF.serial_console.proxyclient_address
5267             listen_port = serial_console.acquire_port(console.listen_host)
5268             console.listen_port = listen_port
5269             # NOTE: only the first serial console gets the boot messages,
5270             # that's why we attach the logd subdevice only to that.
5271             if port == 0:
5272                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
5273                 log.file = log_path
5274                 console.log = log
5275             guest_cfg.add_device(console)
5276 
5277     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
5278         """Update VirtCPUModel object according to libvirt CPU config.
5279 
5280         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
5281                            instance's virtual cpu configuration.
5282         :param:vcpu_model: VirtCPUModel object. A new object will be created
5283                            if None.
5284 
5285         :return: Updated VirtCPUModel object, or None if cpu_config is None
5286 
5287         """
5288 
5289         if not cpu_config:
5290             return
5291         if not vcpu_model:
5292             vcpu_model = objects.VirtCPUModel()
5293 
5294         vcpu_model.arch = cpu_config.arch
5295         vcpu_model.vendor = cpu_config.vendor
5296         vcpu_model.model = cpu_config.model
5297         vcpu_model.mode = cpu_config.mode
5298         vcpu_model.match = cpu_config.match
5299 
5300         if cpu_config.sockets:
5301             vcpu_model.topology = objects.VirtCPUTopology(
5302                 sockets=cpu_config.sockets,
5303                 cores=cpu_config.cores,
5304                 threads=cpu_config.threads)
5305         else:
5306             vcpu_model.topology = None
5307 
5308         features = [objects.VirtCPUFeature(
5309             name=f.name,
5310             policy=f.policy) for f in cpu_config.features]
5311         vcpu_model.features = features
5312 
5313         return vcpu_model
5314 
5315     def _vcpu_model_to_cpu_config(self, vcpu_model):
5316         """Create libvirt CPU config according to VirtCPUModel object.
5317 
5318         :param:vcpu_model: VirtCPUModel object.
5319 
5320         :return: vconfig.LibvirtConfigGuestCPU.
5321 
5322         """
5323 
5324         cpu_config = vconfig.LibvirtConfigGuestCPU()
5325         cpu_config.arch = vcpu_model.arch
5326         cpu_config.model = vcpu_model.model
5327         cpu_config.mode = vcpu_model.mode
5328         cpu_config.match = vcpu_model.match
5329         cpu_config.vendor = vcpu_model.vendor
5330         if vcpu_model.topology:
5331             cpu_config.sockets = vcpu_model.topology.sockets
5332             cpu_config.cores = vcpu_model.topology.cores
5333             cpu_config.threads = vcpu_model.topology.threads
5334         if vcpu_model.features:
5335             for f in vcpu_model.features:
5336                 xf = vconfig.LibvirtConfigGuestCPUFeature()
5337                 xf.name = f.name
5338                 xf.policy = f.policy
5339                 cpu_config.features.add(xf)
5340         return cpu_config
5341 
5342     def _guest_add_pcie_root_ports(self, guest):
5343         """Add PCI Express root ports.
5344 
5345         PCI Express machine can have as many PCIe devices as it has
5346         pcie-root-port controllers (slots in virtual motherboard).
5347 
5348         If we want to have more PCIe slots for hotplug then we need to create
5349         whole PCIe structure (libvirt limitation).
5350         """
5351 
5352         pcieroot = vconfig.LibvirtConfigGuestPCIeRootController()
5353         guest.add_device(pcieroot)
5354 
5355         for x in range(0, CONF.libvirt.num_pcie_ports):
5356             pcierootport = vconfig.LibvirtConfigGuestPCIeRootPortController()
5357             guest.add_device(pcierootport)
5358 
5359     def _guest_needs_pcie(self, guest, caps):
5360         """Check for prerequisites for adding PCIe root port
5361         controllers
5362         """
5363 
5364         # TODO(kchamart) In the third 'if' conditional below, for 'x86'
5365         # arch, we're assuming: when 'os_mach_type' is 'None', you'll
5366         # have "pc" machine type.  That assumption, although it is
5367         # correct for the "forseeable future", it will be invalid when
5368         # libvirt / QEMU changes the default machine types.
5369         #
5370         # From libvirt 4.7.0 onwards (September 2018), it will ensure
5371         # that *if* 'pc' is available, it will be used as the default --
5372         # to not break existing applications.  (Refer:
5373         # https://libvirt.org/git/?p=libvirt.git;a=commit;h=26cfb1a3
5374         # --"qemu: ensure default machine types don't change if QEMU
5375         # changes").
5376         #
5377         # But even if libvirt (>=v4.7.0) handled the default case,
5378         # relying on such assumptions is not robust.  Instead we should
5379         # get the default machine type for a given architecture reliably
5380         # -- by Nova setting it explicitly (we already do it for Arm /
5381         # AArch64 & s390x).  A part of this bug is being tracked here:
5382         # https://bugs.launchpad.net/nova/+bug/1780138).
5383 
5384         # Add PCIe root port controllers for PCI Express machines
5385         # but only if their amount is configured
5386 
5387         if not CONF.libvirt.num_pcie_ports:
5388             return False
5389         if (caps.host.cpu.arch == fields.Architecture.AARCH64 and
5390                 guest.os_mach_type.startswith('virt')):
5391             return True
5392         if (caps.host.cpu.arch == fields.Architecture.X86_64 and
5393                 guest.os_mach_type is not None and
5394                 'q35' in guest.os_mach_type):
5395             return True
5396         return False
5397 
5398     def _guest_add_usb_host_keyboard(self, guest):
5399         """Add USB Host controller and keyboard for graphical console use.
5400 
5401         Add USB keyboard as PS/2 support may not be present on non-x86
5402         architectures.
5403         """
5404         keyboard = vconfig.LibvirtConfigGuestInput()
5405         keyboard.type = "keyboard"
5406         keyboard.bus = "usb"
5407         guest.add_device(keyboard)
5408 
5409         usbhost = vconfig.LibvirtConfigGuestUSBHostController()
5410         usbhost.index = 0
5411         guest.add_device(usbhost)
5412 
5413     def _get_guest_config(self, instance, network_info, image_meta,
5414                           disk_info, rescue=None, block_device_info=None,
5415                           context=None, mdevs=None):
5416         """Get config data for parameters.
5417 
5418         :param rescue: optional dictionary that should contain the key
5419             'ramdisk_id' if a ramdisk is needed for the rescue image and
5420             'kernel_id' if a kernel is needed for the rescue image.
5421 
5422         :param mdevs: optional list of mediated devices to assign to the guest.
5423         """
5424         flavor = instance.flavor
5425         inst_path = libvirt_utils.get_instance_path(instance)
5426         disk_mapping = disk_info['mapping']
5427 
5428         virt_type = CONF.libvirt.virt_type
5429         guest = vconfig.LibvirtConfigGuest()
5430         guest.virt_type = virt_type
5431         guest.name = instance.name
5432         guest.uuid = instance.uuid
5433         # We are using default unit for memory: KiB
5434         guest.memory = flavor.memory_mb * units.Ki
5435         guest.vcpus = flavor.vcpus
5436         allowed_cpus = hardware.get_vcpu_pin_set()
5437 
5438         guest_numa_config = self._get_guest_numa_config(
5439             instance.numa_topology, flavor, allowed_cpus, image_meta)
5440 
5441         guest.cpuset = guest_numa_config.cpuset
5442         guest.cputune = guest_numa_config.cputune
5443         guest.numatune = guest_numa_config.numatune
5444 
5445         guest.membacking = self._get_guest_memory_backing_config(
5446             instance.numa_topology,
5447             guest_numa_config.numatune,
5448             flavor)
5449 
5450         guest.metadata.append(self._get_guest_config_meta(instance))
5451         guest.idmaps = self._get_guest_idmaps()
5452 
5453         for event in self._supported_perf_events:
5454             guest.add_perf_event(event)
5455 
5456         self._update_guest_cputune(guest, flavor, virt_type)
5457 
5458         guest.cpu = self._get_guest_cpu_config(
5459             flavor, image_meta, guest_numa_config.numaconfig,
5460             instance.numa_topology)
5461 
5462         # Notes(yjiang5): we always sync the instance's vcpu model with
5463         # the corresponding config file.
5464         instance.vcpu_model = self._cpu_config_to_vcpu_model(
5465             guest.cpu, instance.vcpu_model)
5466 
5467         if 'root' in disk_mapping:
5468             root_device_name = block_device.prepend_dev(
5469                 disk_mapping['root']['dev'])
5470         else:
5471             root_device_name = None
5472 
5473         if root_device_name:
5474             instance.root_device_name = root_device_name
5475 
5476         guest.os_type = (fields.VMMode.get_from_instance(instance) or
5477                 self._get_guest_os_type(virt_type))
5478         caps = self._host.get_capabilities()
5479 
5480         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
5481                                            image_meta, flavor,
5482                                            root_device_name)
5483         if virt_type not in ('lxc', 'uml'):
5484             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
5485                     instance, inst_path, image_meta, disk_info)
5486 
5487         self._set_features(guest, instance.os_type, caps, virt_type,
5488                            image_meta, flavor)
5489         self._set_clock(guest, instance.os_type, image_meta, virt_type)
5490 
5491         storage_configs = self._get_guest_storage_config(context,
5492                 instance, image_meta, disk_info, rescue, block_device_info,
5493                 flavor, guest.os_type)
5494         for config in storage_configs:
5495             guest.add_device(config)
5496 
5497         for vif in network_info:
5498             config = self.vif_driver.get_config(
5499                 instance, vif, image_meta,
5500                 flavor, virt_type, self._host)
5501             guest.add_device(config)
5502 
5503         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
5504 
5505         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
5506         if pointer:
5507             guest.add_device(pointer)
5508 
5509         self._guest_add_spice_channel(guest)
5510 
5511         if self._guest_add_video_device(guest):
5512             self._add_video_driver(guest, image_meta, flavor)
5513 
5514             # We want video == we want graphical console. Some architectures
5515             # do not have input devices attached in default configuration.
5516             # Let then add USB Host controller and USB keyboard.
5517             # x86(-64) and ppc64 have usb host controller and keyboard
5518             # s390x does not support USB
5519             if caps.host.cpu.arch == fields.Architecture.AARCH64:
5520                 self._guest_add_usb_host_keyboard(guest)
5521 
5522         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
5523         if virt_type in ('qemu', 'kvm'):
5524             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
5525 
5526         if self._guest_needs_pcie(guest, caps):
5527             self._guest_add_pcie_root_ports(guest)
5528 
5529         self._guest_add_pci_devices(guest, instance)
5530 
5531         self._guest_add_watchdog_action(guest, flavor, image_meta)
5532 
5533         self._guest_add_memory_balloon(guest)
5534 
5535         if mdevs:
5536             self._guest_add_mdevs(guest, mdevs)
5537 
5538         return guest
5539 
5540     def _guest_add_mdevs(self, guest, chosen_mdevs):
5541         for chosen_mdev in chosen_mdevs:
5542             mdev = vconfig.LibvirtConfigGuestHostdevMDEV()
5543             mdev.uuid = chosen_mdev
5544             guest.add_device(mdev)
5545 
5546     @staticmethod
5547     def _guest_add_spice_channel(guest):
5548         if (CONF.spice.enabled and CONF.spice.agent_enabled and
5549                 guest.virt_type not in ('lxc', 'uml', 'xen')):
5550             channel = vconfig.LibvirtConfigGuestChannel()
5551             channel.type = 'spicevmc'
5552             channel.target_name = "com.redhat.spice.0"
5553             guest.add_device(channel)
5554 
5555     @staticmethod
5556     def _guest_add_memory_balloon(guest):
5557         virt_type = guest.virt_type
5558         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
5559         if (virt_type in ('xen', 'qemu', 'kvm') and
5560                     CONF.libvirt.mem_stats_period_seconds > 0):
5561             balloon = vconfig.LibvirtConfigMemoryBalloon()
5562             if virt_type in ('qemu', 'kvm'):
5563                 balloon.model = 'virtio'
5564             else:
5565                 balloon.model = 'xen'
5566             balloon.period = CONF.libvirt.mem_stats_period_seconds
5567             guest.add_device(balloon)
5568 
5569     @staticmethod
5570     def _guest_add_watchdog_action(guest, flavor, image_meta):
5571         # image meta takes precedence over flavor extra specs; disable the
5572         # watchdog action by default
5573         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action') or
5574                            'disabled')
5575         watchdog_action = image_meta.properties.get('hw_watchdog_action',
5576                                                     watchdog_action)
5577         # NB(sross): currently only actually supported by KVM/QEmu
5578         if watchdog_action != 'disabled':
5579             if watchdog_action in fields.WatchdogAction.ALL:
5580                 bark = vconfig.LibvirtConfigGuestWatchdog()
5581                 bark.action = watchdog_action
5582                 guest.add_device(bark)
5583             else:
5584                 raise exception.InvalidWatchdogAction(action=watchdog_action)
5585 
5586     def _guest_add_pci_devices(self, guest, instance):
5587         virt_type = guest.virt_type
5588         if virt_type in ('xen', 'qemu', 'kvm'):
5589             # Get all generic PCI devices (non-SR-IOV).
5590             for pci_dev in pci_manager.get_instance_pci_devs(instance):
5591                 guest.add_device(self._get_guest_pci_device(pci_dev))
5592         else:
5593             # PCI devices is only supported for hypervisors
5594             #  'xen', 'qemu' and 'kvm'.
5595             if pci_manager.get_instance_pci_devs(instance, 'all'):
5596                 raise exception.PciDeviceUnsupportedHypervisor(type=virt_type)
5597 
5598     @staticmethod
5599     def _guest_add_video_device(guest):
5600         # NB some versions of libvirt support both SPICE and VNC
5601         # at the same time. We're not trying to second guess which
5602         # those versions are. We'll just let libvirt report the
5603         # errors appropriately if the user enables both.
5604         add_video_driver = False
5605         if CONF.vnc.enabled and guest.virt_type not in ('lxc', 'uml'):
5606             graphics = vconfig.LibvirtConfigGuestGraphics()
5607             graphics.type = "vnc"
5608             if CONF.vnc.keymap:
5609                 graphics.keymap = CONF.vnc.keymap
5610             graphics.listen = CONF.vnc.server_listen
5611             guest.add_device(graphics)
5612             add_video_driver = True
5613         if CONF.spice.enabled and guest.virt_type not in ('lxc', 'uml', 'xen'):
5614             graphics = vconfig.LibvirtConfigGuestGraphics()
5615             graphics.type = "spice"
5616             if CONF.spice.keymap:
5617                 graphics.keymap = CONF.spice.keymap
5618             graphics.listen = CONF.spice.server_listen
5619             guest.add_device(graphics)
5620             add_video_driver = True
5621         return add_video_driver
5622 
5623     def _get_guest_pointer_model(self, os_type, image_meta):
5624         pointer_model = image_meta.properties.get(
5625             'hw_pointer_model', CONF.pointer_model)
5626         if pointer_model is None and CONF.libvirt.use_usb_tablet:
5627             # TODO(sahid): We set pointer_model to keep compatibility
5628             # until the next release O*. It means operators can continue
5629             # to use the deprecated option "use_usb_tablet" or set a
5630             # specific device to use
5631             pointer_model = "usbtablet"
5632             LOG.warning('The option "use_usb_tablet" has been '
5633                         'deprecated for Newton in favor of the more '
5634                         'generic "pointer_model". Please update '
5635                         'nova.conf to address this change.')
5636 
5637         if pointer_model == "usbtablet":
5638             # We want a tablet if VNC is enabled, or SPICE is enabled and
5639             # the SPICE agent is disabled. If the SPICE agent is enabled
5640             # it provides a paravirt mouse which drastically reduces
5641             # overhead (by eliminating USB polling).
5642             if CONF.vnc.enabled or (
5643                     CONF.spice.enabled and not CONF.spice.agent_enabled):
5644                 return self._get_guest_usb_tablet(os_type)
5645             else:
5646                 if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5647                     # For backward compatibility We don't want to break
5648                     # process of booting an instance if host is configured
5649                     # to use USB tablet without VNC or SPICE and SPICE
5650                     # agent disable.
5651                     LOG.warning('USB tablet requested for guests by host '
5652                                 'configuration. In order to accept this '
5653                                 'request VNC should be enabled or SPICE '
5654                                 'and SPICE agent disabled on host.')
5655                 else:
5656                     raise exception.UnsupportedPointerModelRequested(
5657                         model="usbtablet")
5658 
5659     def _get_guest_usb_tablet(self, os_type):
5660         tablet = None
5661         if os_type == fields.VMMode.HVM:
5662             tablet = vconfig.LibvirtConfigGuestInput()
5663             tablet.type = "tablet"
5664             tablet.bus = "usb"
5665         else:
5666             if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5667                 # For backward compatibility We don't want to break
5668                 # process of booting an instance if virtual machine mode
5669                 # is not configured as HVM.
5670                 LOG.warning('USB tablet requested for guests by host '
5671                             'configuration. In order to accept this '
5672                             'request the machine mode should be '
5673                             'configured as HVM.')
5674             else:
5675                 raise exception.UnsupportedPointerModelRequested(
5676                     model="usbtablet")
5677         return tablet
5678 
5679     def _get_guest_xml(self, context, instance, network_info, disk_info,
5680                        image_meta, rescue=None,
5681                        block_device_info=None,
5682                        mdevs=None):
5683         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
5684         # this ahead of time so that we don't acquire it while also
5685         # holding the logging lock.
5686         network_info_str = str(network_info)
5687         msg = ('Start _get_guest_xml '
5688                'network_info=%(network_info)s '
5689                'disk_info=%(disk_info)s '
5690                'image_meta=%(image_meta)s rescue=%(rescue)s '
5691                'block_device_info=%(block_device_info)s' %
5692                {'network_info': network_info_str, 'disk_info': disk_info,
5693                 'image_meta': image_meta, 'rescue': rescue,
5694                 'block_device_info': block_device_info})
5695         # NOTE(mriedem): block_device_info can contain auth_password so we
5696         # need to sanitize the password in the message.
5697         LOG.debug(strutils.mask_password(msg), instance=instance)
5698         conf = self._get_guest_config(instance, network_info, image_meta,
5699                                       disk_info, rescue, block_device_info,
5700                                       context, mdevs)
5701         xml = conf.to_xml()
5702 
5703         LOG.debug('End _get_guest_xml xml=%(xml)s',
5704                   {'xml': xml}, instance=instance)
5705         return xml
5706 
5707     def get_info(self, instance, use_cache=True):
5708         """Retrieve information from libvirt for a specific instance.
5709 
5710         If a libvirt error is encountered during lookup, we might raise a
5711         NotFound exception or Error exception depending on how severe the
5712         libvirt error is.
5713 
5714         :param instance: nova.objects.instance.Instance object
5715         :param use_cache: unused in this driver
5716         :returns: An InstanceInfo object
5717         """
5718         guest = self._host.get_guest(instance)
5719         # Kind of ugly but we need to pass host to get_info as for a
5720         # workaround, see libvirt/compat.py
5721         return guest.get_info(self._host)
5722 
5723     def _create_domain_setup_lxc(self, context, instance, image_meta,
5724                                  block_device_info):
5725         inst_path = libvirt_utils.get_instance_path(instance)
5726         block_device_mapping = driver.block_device_info_get_mapping(
5727             block_device_info)
5728         root_disk = block_device.get_root_bdm(block_device_mapping)
5729         if root_disk:
5730             self._connect_volume(context, root_disk['connection_info'],
5731                                  instance)
5732             disk_path = root_disk['connection_info']['data']['device_path']
5733 
5734             # NOTE(apmelton) - Even though the instance is being booted from a
5735             # cinder volume, it is still presented as a local block device.
5736             # LocalBlockImage is used here to indicate that the instance's
5737             # disk is backed by a local block device.
5738             image_model = imgmodel.LocalBlockImage(disk_path)
5739         else:
5740             root_disk = self.image_backend.by_name(instance, 'disk')
5741             image_model = root_disk.get_model(self._conn)
5742 
5743         container_dir = os.path.join(inst_path, 'rootfs')
5744         fileutils.ensure_tree(container_dir)
5745         rootfs_dev = disk_api.setup_container(image_model,
5746                                               container_dir=container_dir)
5747 
5748         try:
5749             # Save rootfs device to disconnect it when deleting the instance
5750             if rootfs_dev:
5751                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
5752             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
5753                 id_maps = self._get_guest_idmaps()
5754                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
5755         except Exception:
5756             with excutils.save_and_reraise_exception():
5757                 self._create_domain_cleanup_lxc(instance)
5758 
5759     def _create_domain_cleanup_lxc(self, instance):
5760         inst_path = libvirt_utils.get_instance_path(instance)
5761         container_dir = os.path.join(inst_path, 'rootfs')
5762 
5763         try:
5764             state = self.get_info(instance).state
5765         except exception.InstanceNotFound:
5766             # The domain may not be present if the instance failed to start
5767             state = None
5768 
5769         if state == power_state.RUNNING:
5770             # NOTE(uni): Now the container is running with its own private
5771             # mount namespace and so there is no need to keep the container
5772             # rootfs mounted in the host namespace
5773             LOG.debug('Attempting to unmount container filesystem: %s',
5774                       container_dir, instance=instance)
5775             disk_api.clean_lxc_namespace(container_dir=container_dir)
5776         else:
5777             disk_api.teardown_container(container_dir=container_dir)
5778 
5779     @contextlib.contextmanager
5780     def _lxc_disk_handler(self, context, instance, image_meta,
5781                           block_device_info):
5782         """Context manager to handle the pre and post instance boot,
5783            LXC specific disk operations.
5784 
5785            An image or a volume path will be prepared and setup to be
5786            used by the container, prior to starting it.
5787            The disk will be disconnected and unmounted if a container has
5788            failed to start.
5789         """
5790 
5791         if CONF.libvirt.virt_type != 'lxc':
5792             yield
5793             return
5794 
5795         self._create_domain_setup_lxc(context, instance, image_meta,
5796                                       block_device_info)
5797 
5798         try:
5799             yield
5800         finally:
5801             self._create_domain_cleanup_lxc(instance)
5802 
5803     # TODO(sahid): Consider renaming this to _create_guest.
5804     def _create_domain(self, xml=None, domain=None,
5805                        power_on=True, pause=False, post_xml_callback=None):
5806         """Create a domain.
5807 
5808         Either domain or xml must be passed in. If both are passed, then
5809         the domain definition is overwritten from the xml.
5810 
5811         :returns guest.Guest: Guest just created
5812         """
5813         if xml:
5814             guest = libvirt_guest.Guest.create(xml, self._host)
5815             if post_xml_callback is not None:
5816                 post_xml_callback()
5817         else:
5818             guest = libvirt_guest.Guest(domain)
5819 
5820         if power_on or pause:
5821             guest.launch(pause=pause)
5822 
5823         if not utils.is_neutron():
5824             guest.enable_hairpin()
5825 
5826         return guest
5827 
5828     def _neutron_failed_callback(self, event_name, instance):
5829         LOG.error('Neutron Reported failure on event '
5830                   '%(event)s for instance %(uuid)s',
5831                   {'event': event_name, 'uuid': instance.uuid},
5832                   instance=instance)
5833         if CONF.vif_plugging_is_fatal:
5834             raise exception.VirtualInterfaceCreateException()
5835 
5836     def _get_neutron_events(self, network_info):
5837         # NOTE(danms): We need to collect any VIFs that are currently
5838         # down that we expect a down->up event for. Anything that is
5839         # already up will not undergo that transition, and for
5840         # anything that might be stale (cache-wise) assume it's
5841         # already up so we don't block on it.
5842         return [('network-vif-plugged', vif['id'])
5843                 for vif in network_info if vif.get('active', True) is False]
5844 
5845     def _cleanup_failed_start(self, context, instance, network_info,
5846                               block_device_info, guest, destroy_disks):
5847         try:
5848             if guest and guest.is_active():
5849                 guest.poweroff()
5850         finally:
5851             self.cleanup(context, instance, network_info=network_info,
5852                          block_device_info=block_device_info,
5853                          destroy_disks=destroy_disks)
5854 
5855     def _create_domain_and_network(self, context, xml, instance, network_info,
5856                                    block_device_info=None, power_on=True,
5857                                    vifs_already_plugged=False,
5858                                    post_xml_callback=None,
5859                                    destroy_disks_on_failure=False):
5860 
5861         """Do required network setup and create domain."""
5862         timeout = CONF.vif_plugging_timeout
5863         if (self._conn_supports_start_paused and
5864             utils.is_neutron() and not
5865             vifs_already_plugged and power_on and timeout):
5866             events = self._get_neutron_events(network_info)
5867         else:
5868             events = []
5869 
5870         pause = bool(events)
5871         guest = None
5872         try:
5873             with self.virtapi.wait_for_instance_event(
5874                     instance, events, deadline=timeout,
5875                     error_callback=self._neutron_failed_callback):
5876                 self.plug_vifs(instance, network_info)
5877                 self.firewall_driver.setup_basic_filtering(instance,
5878                                                            network_info)
5879                 self.firewall_driver.prepare_instance_filter(instance,
5880                                                              network_info)
5881                 with self._lxc_disk_handler(context, instance,
5882                                             instance.image_meta,
5883                                             block_device_info):
5884                     guest = self._create_domain(
5885                         xml, pause=pause, power_on=power_on,
5886                         post_xml_callback=post_xml_callback)
5887 
5888                 self.firewall_driver.apply_instance_filter(instance,
5889                                                            network_info)
5890         except exception.VirtualInterfaceCreateException:
5891             # Neutron reported failure and we didn't swallow it, so
5892             # bail here
5893             with excutils.save_and_reraise_exception():
5894                 self._cleanup_failed_start(context, instance, network_info,
5895                                            block_device_info, guest,
5896                                            destroy_disks_on_failure)
5897         except eventlet.timeout.Timeout:
5898             # We never heard from Neutron
5899             LOG.warning('Timeout waiting for %(events)s for '
5900                         'instance with vm_state %(vm_state)s and '
5901                         'task_state %(task_state)s.',
5902                         {'events': events,
5903                          'vm_state': instance.vm_state,
5904                          'task_state': instance.task_state},
5905                         instance=instance)
5906             if CONF.vif_plugging_is_fatal:
5907                 self._cleanup_failed_start(context, instance, network_info,
5908                                            block_device_info, guest,
5909                                            destroy_disks_on_failure)
5910                 raise exception.VirtualInterfaceCreateException()
5911         except Exception:
5912             # Any other error, be sure to clean up
5913             LOG.error('Failed to start libvirt guest', instance=instance)
5914             with excutils.save_and_reraise_exception():
5915                 self._cleanup_failed_start(context, instance, network_info,
5916                                            block_device_info, guest,
5917                                            destroy_disks_on_failure)
5918 
5919         # Resume only if domain has been paused
5920         if pause:
5921             guest.resume()
5922         return guest
5923 
5924     def _get_vcpu_total(self):
5925         """Get available vcpu number of physical computer.
5926 
5927         :returns: the number of cpu core instances can be used.
5928 
5929         """
5930         try:
5931             total_pcpus = self._host.get_cpu_count()
5932         except libvirt.libvirtError:
5933             LOG.warning("Cannot get the number of cpu, because this "
5934                         "function is not implemented for this platform.")
5935             return 0
5936 
5937         if not CONF.vcpu_pin_set:
5938             return total_pcpus
5939 
5940         available_ids = hardware.get_vcpu_pin_set()
5941         # We get the list of online CPUs on the host and see if the requested
5942         # set falls under these. If not, we retain the old behavior.
5943         online_pcpus = None
5944         try:
5945             online_pcpus = self._host.get_online_cpus()
5946         except libvirt.libvirtError as ex:
5947             error_code = ex.get_error_code()
5948             err_msg = encodeutils.exception_to_unicode(ex)
5949             LOG.warning(
5950                 "Couldn't retrieve the online CPUs due to a Libvirt "
5951                 "error: %(error)s with error code: %(error_code)s",
5952                 {'error': err_msg, 'error_code': error_code})
5953         if online_pcpus:
5954             if not (available_ids <= online_pcpus):
5955                 msg = (_("Invalid vcpu_pin_set config, one or more of the "
5956                          "specified cpuset is not online. Online cpuset(s): "
5957                          "%(online)s, requested cpuset(s): %(req)s"),
5958                        {'online': sorted(online_pcpus),
5959                         'req': sorted(available_ids)})
5960                 raise exception.Invalid(msg)
5961         elif sorted(available_ids)[-1] >= total_pcpus:
5962             raise exception.Invalid(_("Invalid vcpu_pin_set config, "
5963                                       "out of hypervisor cpu range."))
5964         return len(available_ids)
5965 
5966     @staticmethod
5967     def _get_local_gb_info():
5968         """Get local storage info of the compute node in GB.
5969 
5970         :returns: A dict containing:
5971              :total: How big the overall usable filesystem is (in gigabytes)
5972              :free: How much space is free (in gigabytes)
5973              :used: How much space is used (in gigabytes)
5974         """
5975 
5976         if CONF.libvirt.images_type == 'lvm':
5977             info = lvm.get_volume_group_info(
5978                                CONF.libvirt.images_volume_group)
5979         elif CONF.libvirt.images_type == 'rbd':
5980             info = LibvirtDriver._get_rbd_driver().get_pool_info()
5981         else:
5982             info = libvirt_utils.get_fs_info(CONF.instances_path)
5983 
5984         for (k, v) in info.items():
5985             info[k] = v / units.Gi
5986 
5987         return info
5988 
5989     def _get_vcpu_used(self):
5990         """Get vcpu usage number of physical computer.
5991 
5992         :returns: The total number of vcpu(s) that are currently being used.
5993 
5994         """
5995 
5996         total = 0
5997 
5998         # Not all libvirt drivers will support the get_vcpus_info()
5999         #
6000         # For example, LXC does not have a concept of vCPUs, while
6001         # QEMU (TCG) traditionally handles all vCPUs in a single
6002         # thread. So both will report an exception when the vcpus()
6003         # API call is made. In such a case we should report the
6004         # guest as having 1 vCPU, since that lets us still do
6005         # CPU over commit calculations that apply as the total
6006         # guest count scales.
6007         #
6008         # It is also possible that we might see an exception if
6009         # the guest is just in middle of shutting down. Technically
6010         # we should report 0 for vCPU usage in this case, but we
6011         # we can't reliably distinguish the vcpu not supported
6012         # case from the just shutting down case. Thus we don't know
6013         # whether to report 1 or 0 for vCPU count.
6014         #
6015         # Under-reporting vCPUs is bad because it could conceivably
6016         # let the scheduler place too many guests on the host. Over-
6017         # reporting vCPUs is not a problem as it'll auto-correct on
6018         # the next refresh of usage data.
6019         #
6020         # Thus when getting an exception we always report 1 as the
6021         # vCPU count, as the least worst value.
6022         for guest in self._host.list_guests():
6023             try:
6024                 vcpus = guest.get_vcpus_info()
6025                 total += len(list(vcpus))
6026             except libvirt.libvirtError:
6027                 total += 1
6028             # NOTE(gtt116): give other tasks a chance.
6029             greenthread.sleep(0)
6030         return total
6031 
6032     def _get_supported_vgpu_types(self):
6033         if not CONF.devices.enabled_vgpu_types:
6034             return []
6035         # TODO(sbauza): Move this check up to compute_manager.init_host
6036         if len(CONF.devices.enabled_vgpu_types) > 1:
6037             LOG.warning('libvirt only supports one GPU type per compute node,'
6038                         ' only first type will be used.')
6039         requested_types = CONF.devices.enabled_vgpu_types[:1]
6040         return requested_types
6041 
6042     def _count_mediated_devices(self, enabled_vgpu_types):
6043         """Counts the sysfs objects (handles) that represent a mediated device
6044         and filtered by $enabled_vgpu_types.
6045 
6046         Those handles can be in use by a libvirt guest or not.
6047 
6048         :param enabled_vgpu_types: list of enabled VGPU types on this host
6049         :returns: dict, keyed by parent GPU libvirt PCI device ID, of number of
6050         mdev device handles for that GPU
6051         """
6052 
6053         counts_per_parent = collections.defaultdict(int)
6054         mediated_devices = self._get_mediated_devices(types=enabled_vgpu_types)
6055         for mdev in mediated_devices:
6056             counts_per_parent[mdev['parent']] += 1
6057         return counts_per_parent
6058 
6059     def _count_mdev_capable_devices(self, enabled_vgpu_types):
6060         """Counts the mdev-capable devices on this host filtered by
6061         $enabled_vgpu_types.
6062 
6063         :param enabled_vgpu_types: list of enabled VGPU types on this host
6064         :returns: dict, keyed by device name, to an integer count of available
6065             instances of each type per device
6066         """
6067         mdev_capable_devices = self._get_mdev_capable_devices(
6068             types=enabled_vgpu_types)
6069         counts_per_dev = collections.defaultdict(int)
6070         for dev in mdev_capable_devices:
6071             # dev_id is the libvirt name for the PCI device,
6072             # eg. pci_0000_84_00_0 which matches a PCI address of 0000:84:00.0
6073             dev_name = dev['dev_id']
6074             for _type in dev['types']:
6075                 available = dev['types'][_type]['availableInstances']
6076                 # TODO(sbauza): Once we support multiple types, check which
6077                 # PCI devices are set for this type
6078                 # NOTE(sbauza): Even if we support multiple types, Nova will
6079                 # only use one per physical GPU.
6080                 counts_per_dev[dev_name] += available
6081         return counts_per_dev
6082 
6083     def _get_gpu_inventories(self):
6084         """Returns the inventories for each physical GPU for a specific type
6085         supported by the enabled_vgpu_types CONF option.
6086 
6087         :returns: dict, keyed by libvirt PCI name, of dicts like:
6088                 {'pci_0000_84_00_0':
6089                     {'total': $TOTAL,
6090                      'min_unit': 1,
6091                      'max_unit': $TOTAL,
6092                      'step_size': 1,
6093                      'reserved': 0,
6094                      'allocation_ratio': 1.0,
6095                     }
6096                 }
6097         """
6098 
6099         # Bail out early if operator doesn't care about providing vGPUs
6100         enabled_vgpu_types = self._get_supported_vgpu_types()
6101         if not enabled_vgpu_types:
6102             return {}
6103         inventories = {}
6104         count_per_parent = self._count_mediated_devices(enabled_vgpu_types)
6105         for dev_name, count in count_per_parent.items():
6106             inventories[dev_name] = {'total': count}
6107         # Filter how many available mdevs we can create for all the supported
6108         # types.
6109         count_per_dev = self._count_mdev_capable_devices(enabled_vgpu_types)
6110         # Combine the counts into the dict that we return to the caller.
6111         for dev_name, count in count_per_dev.items():
6112             inv_per_parent = inventories.setdefault(
6113                 dev_name, {'total': 0})
6114             inv_per_parent['total'] += count
6115             inv_per_parent.update({
6116                 'min_unit': 1,
6117                 'step_size': 1,
6118                 'reserved': 0,
6119                 # NOTE(sbauza): There is no sense to have a ratio but 1.0
6120                 # since we can't overallocate vGPU resources
6121                 'allocation_ratio': 1.0,
6122                 # FIXME(sbauza): Some vendors could support only one
6123                 'max_unit': inv_per_parent['total'],
6124             })
6125 
6126         return inventories
6127 
6128     def _get_instance_capabilities(self):
6129         """Get hypervisor instance capabilities
6130 
6131         Returns a list of tuples that describe instances the
6132         hypervisor is capable of hosting.  Each tuple consists
6133         of the triplet (arch, hypervisor_type, vm_mode).
6134 
6135         Supported hypervisor_type is filtered by virt_type,
6136         a parameter set by operators via `nova.conf`.
6137 
6138         :returns: List of tuples describing instance capabilities
6139         """
6140         caps = self._host.get_capabilities()
6141         instance_caps = list()
6142         for g in caps.guests:
6143             for dt in g.domtype:
6144                 if dt != CONF.libvirt.virt_type:
6145                     continue
6146                 try:
6147                     instance_cap = (
6148                         fields.Architecture.canonicalize(g.arch),
6149                         fields.HVType.canonicalize(dt),
6150                         fields.VMMode.canonicalize(g.ostype))
6151                     instance_caps.append(instance_cap)
6152                 except exception.InvalidArchitectureName:
6153                     # NOTE(danms): Libvirt is exposing a guest arch that nova
6154                     # does not even know about. Avoid aborting here and
6155                     # continue to process the rest.
6156                     pass
6157 
6158         return instance_caps
6159 
6160     def _get_cpu_info(self):
6161         """Get cpuinfo information.
6162 
6163         Obtains cpu feature from virConnect.getCapabilities.
6164 
6165         :return: see above description
6166 
6167         """
6168 
6169         caps = self._host.get_capabilities()
6170         cpu_info = dict()
6171 
6172         cpu_info['arch'] = caps.host.cpu.arch
6173         cpu_info['model'] = caps.host.cpu.model
6174         cpu_info['vendor'] = caps.host.cpu.vendor
6175 
6176         topology = dict()
6177         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
6178         topology['sockets'] = caps.host.cpu.sockets
6179         topology['cores'] = caps.host.cpu.cores
6180         topology['threads'] = caps.host.cpu.threads
6181         cpu_info['topology'] = topology
6182 
6183         features = set()
6184         for f in caps.host.cpu.features:
6185             features.add(f.name)
6186         cpu_info['features'] = features
6187         return cpu_info
6188 
6189     def _get_pcinet_info(self, vf_address):
6190         """Returns a dict of NET device."""
6191         devname = pci_utils.get_net_name_by_vf_pci_address(vf_address)
6192         if not devname:
6193             return
6194 
6195         virtdev = self._host.device_lookup_by_name(devname)
6196         xmlstr = virtdev.XMLDesc(0)
6197         cfgdev = vconfig.LibvirtConfigNodeDevice()
6198         cfgdev.parse_str(xmlstr)
6199         return {'name': cfgdev.name,
6200                 'capabilities': cfgdev.pci_capability.features}
6201 
6202     def _get_pcidev_info(self, devname):
6203         """Returns a dict of PCI device."""
6204 
6205         def _get_device_type(cfgdev, pci_address):
6206             """Get a PCI device's device type.
6207 
6208             An assignable PCI device can be a normal PCI device,
6209             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
6210             Function (VF). Only normal PCI devices or SR-IOV VFs
6211             are assignable, while SR-IOV PFs are always owned by
6212             hypervisor.
6213             """
6214             for fun_cap in cfgdev.pci_capability.fun_capability:
6215                 if fun_cap.type == 'virt_functions':
6216                     return {
6217                         'dev_type': fields.PciDeviceType.SRIOV_PF,
6218                     }
6219                 if (fun_cap.type == 'phys_function' and
6220                     len(fun_cap.device_addrs) != 0):
6221                     phys_address = "%04x:%02x:%02x.%01x" % (
6222                         fun_cap.device_addrs[0][0],
6223                         fun_cap.device_addrs[0][1],
6224                         fun_cap.device_addrs[0][2],
6225                         fun_cap.device_addrs[0][3])
6226                     result = {
6227                         'dev_type': fields.PciDeviceType.SRIOV_VF,
6228                         'parent_addr': phys_address,
6229                     }
6230                     parent_ifname = None
6231                     try:
6232                         parent_ifname = pci_utils.get_ifname_by_pci_address(
6233                             pci_address, pf_interface=True)
6234                     except exception.PciDeviceNotFoundById:
6235                         # NOTE(sean-k-mooney): we ignore this error as it
6236                         # is expected when the virtual function is not a NIC.
6237                         pass
6238                     if parent_ifname:
6239                         result['parent_ifname'] = parent_ifname
6240                     return result
6241 
6242             return {'dev_type': fields.PciDeviceType.STANDARD}
6243 
6244         def _get_device_capabilities(device, address):
6245             """Get PCI VF device's additional capabilities.
6246 
6247             If a PCI device is a virtual function, this function reads the PCI
6248             parent's network capabilities (must be always a NIC device) and
6249             appends this information to the device's dictionary.
6250             """
6251             if device.get('dev_type') == fields.PciDeviceType.SRIOV_VF:
6252                 pcinet_info = self._get_pcinet_info(address)
6253                 if pcinet_info:
6254                     return {'capabilities':
6255                                 {'network': pcinet_info.get('capabilities')}}
6256             return {}
6257 
6258         virtdev = self._host.device_lookup_by_name(devname)
6259         xmlstr = virtdev.XMLDesc(0)
6260         cfgdev = vconfig.LibvirtConfigNodeDevice()
6261         cfgdev.parse_str(xmlstr)
6262 
6263         address = "%04x:%02x:%02x.%1x" % (
6264             cfgdev.pci_capability.domain,
6265             cfgdev.pci_capability.bus,
6266             cfgdev.pci_capability.slot,
6267             cfgdev.pci_capability.function)
6268 
6269         device = {
6270             "dev_id": cfgdev.name,
6271             "address": address,
6272             "product_id": "%04x" % cfgdev.pci_capability.product_id,
6273             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
6274             }
6275 
6276         device["numa_node"] = cfgdev.pci_capability.numa_node
6277 
6278         # requirement by DataBase Model
6279         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
6280         device.update(_get_device_type(cfgdev, address))
6281         device.update(_get_device_capabilities(device, address))
6282         return device
6283 
6284     def _get_pci_passthrough_devices(self):
6285         """Get host PCI devices information.
6286 
6287         Obtains pci devices information from libvirt, and returns
6288         as a JSON string.
6289 
6290         Each device information is a dictionary, with mandatory keys
6291         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
6292         'label' and other optional device specific information.
6293 
6294         Refer to the objects/pci_device.py for more idea of these keys.
6295 
6296         :returns: a JSON string containing a list of the assignable PCI
6297                   devices information
6298         """
6299         # Bail early if we know we can't support `listDevices` to avoid
6300         # repeated warnings within a periodic task
6301         if not getattr(self, '_list_devices_supported', True):
6302             return jsonutils.dumps([])
6303 
6304         try:
6305             dev_names = self._host.list_pci_devices() or []
6306         except libvirt.libvirtError as ex:
6307             error_code = ex.get_error_code()
6308             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
6309                 self._list_devices_supported = False
6310                 LOG.warning("URI %(uri)s does not support "
6311                             "listDevices: %(error)s",
6312                             {'uri': self._uri(),
6313                              'error': encodeutils.exception_to_unicode(ex)})
6314                 return jsonutils.dumps([])
6315             else:
6316                 raise
6317 
6318         pci_info = []
6319         for name in dev_names:
6320             pci_info.append(self._get_pcidev_info(name))
6321 
6322         return jsonutils.dumps(pci_info)
6323 
6324     def _get_mdev_capabilities_for_dev(self, devname, types=None):
6325         """Returns a dict of MDEV capable device with the ID as first key
6326         and then a list of supported types, each of them being a dict.
6327 
6328         :param types: Only return those specific types.
6329         """
6330         virtdev = self._host.device_lookup_by_name(devname)
6331         xmlstr = virtdev.XMLDesc(0)
6332         cfgdev = vconfig.LibvirtConfigNodeDevice()
6333         cfgdev.parse_str(xmlstr)
6334 
6335         device = {
6336             "dev_id": cfgdev.name,
6337             "types": {},
6338             "vendor_id": cfgdev.pci_capability.vendor_id,
6339         }
6340         for mdev_cap in cfgdev.pci_capability.mdev_capability:
6341             for cap in mdev_cap.mdev_types:
6342                 if not types or cap['type'] in types:
6343                     device["types"].update({cap['type']: {
6344                         'availableInstances': cap['availableInstances'],
6345                         'name': cap['name'],
6346                         'deviceAPI': cap['deviceAPI']}})
6347         return device
6348 
6349     def _get_mdev_capable_devices(self, types=None):
6350         """Get host devices supporting mdev types.
6351 
6352         Obtain devices information from libvirt and returns a list of
6353         dictionaries.
6354 
6355         :param types: Filter only devices supporting those types.
6356         """
6357         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
6358             return []
6359         dev_names = self._host.list_mdev_capable_devices() or []
6360         mdev_capable_devices = []
6361         for name in dev_names:
6362             device = self._get_mdev_capabilities_for_dev(name, types)
6363             if not device["types"]:
6364                 continue
6365             mdev_capable_devices.append(device)
6366         return mdev_capable_devices
6367 
6368     def _get_mediated_device_information(self, devname):
6369         """Returns a dict of a mediated device."""
6370         virtdev = self._host.device_lookup_by_name(devname)
6371         xmlstr = virtdev.XMLDesc(0)
6372         cfgdev = vconfig.LibvirtConfigNodeDevice()
6373         cfgdev.parse_str(xmlstr)
6374 
6375         device = {
6376             "dev_id": cfgdev.name,
6377             # name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
6378             "uuid": libvirt_utils.mdev_name2uuid(cfgdev.name),
6379             # the physical GPU PCI device
6380             "parent": cfgdev.parent,
6381             "type": cfgdev.mdev_information.type,
6382             "iommu_group": cfgdev.mdev_information.iommu_group,
6383         }
6384         return device
6385 
6386     def _get_mediated_devices(self, types=None):
6387         """Get host mediated devices.
6388 
6389         Obtain devices information from libvirt and returns a list of
6390         dictionaries.
6391 
6392         :param types: Filter only devices supporting those types.
6393         """
6394         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
6395             return []
6396         dev_names = self._host.list_mediated_devices() or []
6397         mediated_devices = []
6398         for name in dev_names:
6399             device = self._get_mediated_device_information(name)
6400             if not types or device["type"] in types:
6401                 mediated_devices.append(device)
6402         return mediated_devices
6403 
6404     def _get_all_assigned_mediated_devices(self, instance=None):
6405         """Lookup all instances from the host and return all the mediated
6406         devices that are assigned to a guest.
6407 
6408         :param instance: Only return mediated devices for that instance.
6409 
6410         :returns: A dictionary of keys being mediated device UUIDs and their
6411                   respective values the instance UUID of the guest using it.
6412                   Returns an empty dict if an instance is provided but not
6413                   found in the hypervisor.
6414         """
6415         allocated_mdevs = {}
6416         if instance:
6417             # NOTE(sbauza): In some cases (like a migration issue), the
6418             # instance can exist in the Nova database but libvirt doesn't know
6419             # about it. For such cases, the way to fix that is to hard reboot
6420             # the instance, which will recreate the libvirt guest.
6421             # For that reason, we need to support that case by making sure
6422             # we don't raise an exception if the libvirt guest doesn't exist.
6423             try:
6424                 guest = self._host.get_guest(instance)
6425             except exception.InstanceNotFound:
6426                 # Bail out early if libvirt doesn't know about it since we
6427                 # can't know the existing mediated devices
6428                 return {}
6429             guests = [guest]
6430         else:
6431             guests = self._host.list_guests(only_running=False)
6432         for guest in guests:
6433             cfg = guest.get_config()
6434             for device in cfg.devices:
6435                 if isinstance(device, vconfig.LibvirtConfigGuestHostdevMDEV):
6436                     allocated_mdevs[device.uuid] = guest.uuid
6437         return allocated_mdevs
6438 
6439     @staticmethod
6440     def _vgpu_allocations(allocations):
6441         """Filtering only the VGPU allocations from a list of allocations.
6442 
6443         :param allocations: Information about resources allocated to the
6444                             instance via placement, of the form returned by
6445                             SchedulerReportClient.get_allocations_for_consumer.
6446         """
6447         if not allocations:
6448             # If no allocations, there is no vGPU request.
6449             return {}
6450         RC_VGPU = orc.VGPU
6451         vgpu_allocations = {}
6452         for rp in allocations:
6453             res = allocations[rp]['resources']
6454             if RC_VGPU in res and res[RC_VGPU] > 0:
6455                 vgpu_allocations[rp] = {'resources': {RC_VGPU: res[RC_VGPU]}}
6456         return vgpu_allocations
6457 
6458     def _get_existing_mdevs_not_assigned(self, requested_types=None,
6459                                          parent=None):
6460         """Returns the already created mediated devices that are not assigned
6461         to a guest yet.
6462 
6463         :param requested_types: Filter out the result for only mediated devices
6464                                 having those types.
6465         :param parent: Filter out result for only mdevs from the parent device.
6466         """
6467         allocated_mdevs = self._get_all_assigned_mediated_devices()
6468         mdevs = self._get_mediated_devices(requested_types)
6469         available_mdevs = set()
6470         for mdev in mdevs:
6471             if parent is None or mdev['parent'] == parent:
6472                 available_mdevs.add(mdev["uuid"])
6473 
6474         available_mdevs -= set(allocated_mdevs)
6475         return available_mdevs
6476 
6477     def _create_new_mediated_device(self, requested_types, uuid=None,
6478                                     parent=None):
6479         """Find a physical device that can support a new mediated device and
6480         create it.
6481 
6482         :param requested_types: Filter only capable devices supporting those
6483                                 types.
6484         :param uuid: The possible mdev UUID we want to create again
6485         :param parent: Only create a mdev for this device
6486 
6487         :returns: the newly created mdev UUID or None if not possible
6488         """
6489         # Try to see if we can still create a new mediated device
6490         devices = self._get_mdev_capable_devices(requested_types)
6491         for device in devices:
6492             # For the moment, the libvirt driver only supports one
6493             # type per host
6494             # TODO(sbauza): Once we support more than one type, make
6495             # sure we look at the flavor/trait for the asked type.
6496             asked_type = requested_types[0]
6497             if device['types'][asked_type]['availableInstances'] > 0:
6498                 # That physical GPU has enough room for a new mdev
6499                 dev_name = device['dev_id']
6500                 # the parent attribute can be None
6501                 if parent is not None and dev_name != parent:
6502                     # The device is not the one that was called, not creating
6503                     # the mdev
6504                     continue
6505                 # We need the PCI address, not the libvirt name
6506                 # The libvirt name is like 'pci_0000_84_00_0'
6507                 pci_addr = "{}:{}:{}.{}".format(*dev_name[4:].split('_'))
6508                 chosen_mdev = nova.privsep.libvirt.create_mdev(pci_addr,
6509                                                                asked_type,
6510                                                                uuid=uuid)
6511                 return chosen_mdev
6512 
6513     @utils.synchronized(VGPU_RESOURCE_SEMAPHORE)
6514     def _allocate_mdevs(self, allocations):
6515         """Returns a list of mediated device UUIDs corresponding to available
6516         resources we can assign to the guest(s) corresponding to the allocation
6517         requests passed as argument.
6518 
6519         That method can either find an existing but unassigned mediated device
6520         it can allocate, or create a new mediated device from a capable
6521         physical device if the latter has enough left capacity.
6522 
6523         :param allocations: Information about resources allocated to the
6524                             instance via placement, of the form returned by
6525                             SchedulerReportClient.get_allocations_for_consumer.
6526                             That code is supporting Placement API version 1.12
6527         """
6528         vgpu_allocations = self._vgpu_allocations(allocations)
6529         if not vgpu_allocations:
6530             return
6531         # TODO(sbauza): Once we have nested resource providers, find which one
6532         # is having the related allocation for the specific VGPU type.
6533         # For the moment, we should only have one allocation for
6534         # ResourceProvider.
6535         # TODO(sbauza): Iterate over all the allocations once we have
6536         # nested Resource Providers. For the moment, just take the first.
6537         if len(vgpu_allocations) > 1:
6538             LOG.warning('More than one allocation was passed over to libvirt '
6539                         'while at the moment libvirt only supports one. Only '
6540                         'the first allocation will be looked up.')
6541         rp_uuid, alloc = six.next(six.iteritems(vgpu_allocations))
6542         vgpus_asked = alloc['resources'][orc.VGPU]
6543 
6544         # Find if we allocated against a specific pGPU (and then the allocation
6545         # is made against a child RP) or any pGPU (in case the VGPU inventory
6546         # is still on the root RP)
6547         try:
6548             allocated_rp = self.provider_tree.data(rp_uuid)
6549         except ValueError:
6550             # The provider doesn't exist, return a better understandable
6551             # exception
6552             raise exception.ComputeResourcesUnavailable(
6553                 reason='vGPU resource is not available')
6554         # TODO(sbauza): Remove this conditional in Train once all VGPU
6555         # inventories are related to a child RP
6556         if allocated_rp.parent_uuid is None:
6557             # We are on a root RP
6558             parent_device = None
6559         else:
6560             rp_name = allocated_rp.name
6561             # There can be multiple roots, we need to find the root name
6562             # to guess the physical device name
6563             roots = self.provider_tree.roots
6564             for root in roots:
6565                 if rp_name.startswith(root.name + '_'):
6566                     # The RP name convention is :
6567                     #    root_name + '_' + parent_device
6568                     parent_device = rp_name[len(root.name) + 1:]
6569                     break
6570             else:
6571                 LOG.warning("pGPU device name %(name)s can't be guessed from "
6572                             "the ProviderTree "
6573                             "roots %(roots)s", {'name': rp_name,
6574                                                  'roots': roots})
6575                 # We f... have no idea what was the parent device
6576                 # If we can't find devices having available VGPUs, just raise
6577                 raise exception.ComputeResourcesUnavailable(
6578                     reason='vGPU resource is not available')
6579 
6580         requested_types = self._get_supported_vgpu_types()
6581         # Which mediated devices are created but not assigned to a guest ?
6582         mdevs_available = self._get_existing_mdevs_not_assigned(
6583             requested_types, parent_device)
6584 
6585         chosen_mdevs = []
6586         for c in six.moves.range(vgpus_asked):
6587             chosen_mdev = None
6588             if mdevs_available:
6589                 # Take the first available mdev
6590                 chosen_mdev = mdevs_available.pop()
6591             else:
6592                 chosen_mdev = self._create_new_mediated_device(
6593                     requested_types, parent=parent_device)
6594             if not chosen_mdev:
6595                 # If we can't find devices having available VGPUs, just raise
6596                 raise exception.ComputeResourcesUnavailable(
6597                     reason='vGPU resource is not available')
6598             else:
6599                 chosen_mdevs.append(chosen_mdev)
6600         return chosen_mdevs
6601 
6602     def _detach_mediated_devices(self, guest):
6603         mdevs = guest.get_all_devices(
6604             devtype=vconfig.LibvirtConfigGuestHostdevMDEV)
6605         for mdev_cfg in mdevs:
6606             try:
6607                 guest.detach_device(mdev_cfg, live=True)
6608             except libvirt.libvirtError as ex:
6609                 error_code = ex.get_error_code()
6610                 # NOTE(sbauza): There is a pending issue with libvirt that
6611                 # doesn't allow to hot-unplug mediated devices. Let's
6612                 # short-circuit the suspend action and set the instance back
6613                 # to ACTIVE.
6614                 # TODO(sbauza): Once libvirt supports this, amend the resume()
6615                 # operation to support reallocating mediated devices.
6616                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
6617                     reason = _("Suspend is not supported for instances having "
6618                                "attached vGPUs.")
6619                     raise exception.InstanceFaultRollback(
6620                         exception.InstanceSuspendFailure(reason=reason))
6621                 else:
6622                     raise
6623 
6624     def _has_numa_support(self):
6625         # This means that the host can support LibvirtConfigGuestNUMATune
6626         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
6627         caps = self._host.get_capabilities()
6628 
6629         if (caps.host.cpu.arch in (fields.Architecture.I686,
6630                                    fields.Architecture.X86_64,
6631                                    fields.Architecture.AARCH64) and
6632                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
6633             return True
6634         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
6635                                      fields.Architecture.PPC64LE)):
6636             return True
6637 
6638         return False
6639 
6640     def _get_host_numa_topology(self):
6641         if not self._has_numa_support():
6642             return
6643 
6644         caps = self._host.get_capabilities()
6645         topology = caps.host.topology
6646 
6647         if topology is None or not topology.cells:
6648             return
6649 
6650         cells = []
6651         allowed_cpus = hardware.get_vcpu_pin_set()
6652         online_cpus = self._host.get_online_cpus()
6653         if allowed_cpus:
6654             allowed_cpus &= online_cpus
6655         else:
6656             allowed_cpus = online_cpus
6657 
6658         def _get_reserved_memory_for_cell(self, cell_id, page_size):
6659             cell = self._reserved_hugepages.get(cell_id, {})
6660             return cell.get(page_size, 0)
6661 
6662         def _get_physnet_numa_affinity():
6663             affinities = {cell.id: set() for cell in topology.cells}
6664             for physnet in CONF.neutron.physnets:
6665                 # This will error out if the group is not registered, which is
6666                 # exactly what we want as that would be a bug
6667                 group = getattr(CONF, 'neutron_physnet_%s' % physnet)
6668 
6669                 if not group.numa_nodes:
6670                     msg = ("the physnet '%s' was listed in '[neutron] "
6671                            "physnets' but no corresponding "
6672                            "'[neutron_physnet_%s] numa_nodes' option was "
6673                            "defined." % (physnet, physnet))
6674                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6675 
6676                 for node in group.numa_nodes:
6677                     if node not in affinities:
6678                         msg = ("node %d for physnet %s is not present in host "
6679                                "affinity set %r" % (node, physnet, affinities))
6680                         # The config option referenced an invalid node
6681                         raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6682                     affinities[node].add(physnet)
6683 
6684             return affinities
6685 
6686         def _get_tunnel_numa_affinity():
6687             affinities = {cell.id: False for cell in topology.cells}
6688 
6689             for node in CONF.neutron_tunnel.numa_nodes:
6690                 if node not in affinities:
6691                     msg = ("node %d for tunneled networks is not present "
6692                            "in host affinity set %r" % (node, affinities))
6693                     # The config option referenced an invalid node
6694                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6695                 affinities[node] = True
6696 
6697             return affinities
6698 
6699         physnet_affinities = _get_physnet_numa_affinity()
6700         tunnel_affinities = _get_tunnel_numa_affinity()
6701 
6702         for cell in topology.cells:
6703             cpuset = set(cpu.id for cpu in cell.cpus)
6704             siblings = sorted(map(set,
6705                                   set(tuple(cpu.siblings)
6706                                         if cpu.siblings else ()
6707                                       for cpu in cell.cpus)
6708                                   ))
6709             cpuset &= allowed_cpus
6710             siblings = [sib & allowed_cpus for sib in siblings]
6711             # Filter out empty sibling sets that may be left
6712             siblings = [sib for sib in siblings if len(sib) > 0]
6713 
6714             mempages = [
6715                 objects.NUMAPagesTopology(
6716                     size_kb=pages.size,
6717                     total=pages.total,
6718                     used=0,
6719                     reserved=_get_reserved_memory_for_cell(
6720                         self, cell.id, pages.size))
6721                 for pages in cell.mempages]
6722 
6723             network_metadata = objects.NetworkMetadata(
6724                 physnets=physnet_affinities[cell.id],
6725                 tunneled=tunnel_affinities[cell.id])
6726 
6727             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
6728                                     memory=cell.memory / units.Ki,
6729                                     cpu_usage=0, memory_usage=0,
6730                                     siblings=siblings,
6731                                     pinned_cpus=set([]),
6732                                     mempages=mempages,
6733                                     network_metadata=network_metadata)
6734             cells.append(cell)
6735 
6736         return objects.NUMATopology(cells=cells)
6737 
6738     def get_all_volume_usage(self, context, compute_host_bdms):
6739         """Return usage info for volumes attached to vms on
6740            a given host.
6741         """
6742         vol_usage = []
6743 
6744         for instance_bdms in compute_host_bdms:
6745             instance = instance_bdms['instance']
6746 
6747             for bdm in instance_bdms['instance_bdms']:
6748                 mountpoint = bdm['device_name']
6749                 if mountpoint.startswith('/dev/'):
6750                     mountpoint = mountpoint[5:]
6751                 volume_id = bdm['volume_id']
6752 
6753                 LOG.debug("Trying to get stats for the volume %s",
6754                           volume_id, instance=instance)
6755                 vol_stats = self.block_stats(instance, mountpoint)
6756 
6757                 if vol_stats:
6758                     stats = dict(volume=volume_id,
6759                                  instance=instance,
6760                                  rd_req=vol_stats[0],
6761                                  rd_bytes=vol_stats[1],
6762                                  wr_req=vol_stats[2],
6763                                  wr_bytes=vol_stats[3])
6764                     LOG.debug(
6765                         "Got volume usage stats for the volume=%(volume)s,"
6766                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
6767                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
6768                         stats, instance=instance)
6769                     vol_usage.append(stats)
6770 
6771         return vol_usage
6772 
6773     def block_stats(self, instance, disk_id):
6774         """Note that this function takes an instance name."""
6775         try:
6776             guest = self._host.get_guest(instance)
6777             dev = guest.get_block_device(disk_id)
6778             return dev.blockStats()
6779         except libvirt.libvirtError as e:
6780             errcode = e.get_error_code()
6781             LOG.info('Getting block stats failed, device might have '
6782                      'been detached. Instance=%(instance_name)s '
6783                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
6784                      {'instance_name': instance.name, 'disk': disk_id,
6785                       'errcode': errcode, 'e': e},
6786                      instance=instance)
6787         except exception.InstanceNotFound:
6788             LOG.info('Could not find domain in libvirt for instance %s. '
6789                      'Cannot get block stats for device', instance.name,
6790                      instance=instance)
6791 
6792     def get_console_pool_info(self, console_type):
6793         # TODO(mdragon): console proxy should be implemented for libvirt,
6794         #                in case someone wants to use it with kvm or
6795         #                such. For now return fake data.
6796         return {'address': '127.0.0.1',
6797                 'username': 'fakeuser',
6798                 'password': 'fakepassword'}
6799 
6800     def refresh_security_group_rules(self, security_group_id):
6801         self.firewall_driver.refresh_security_group_rules(security_group_id)
6802 
6803     def refresh_instance_security_rules(self, instance):
6804         self.firewall_driver.refresh_instance_security_rules(instance)
6805 
6806     def update_provider_tree(self, provider_tree, nodename, allocations=None):
6807         """Update a ProviderTree object with current resource provider,
6808         inventory information and CPU traits.
6809 
6810         :param nova.compute.provider_tree.ProviderTree provider_tree:
6811             A nova.compute.provider_tree.ProviderTree object representing all
6812             the providers in the tree associated with the compute node, and any
6813             sharing providers (those with the ``MISC_SHARES_VIA_AGGREGATE``
6814             trait) associated via aggregate with any of those providers (but
6815             not *their* tree- or aggregate-associated providers), as currently
6816             known by placement.
6817         :param nodename:
6818             String name of the compute node (i.e.
6819             ComputeNode.hypervisor_hostname) for which the caller is requesting
6820             updated provider information.
6821         :param allocations:
6822             Dict of allocation data of the form:
6823               { $CONSUMER_UUID: {
6824                     # The shape of each "allocations" dict below is identical
6825                     # to the return from GET /allocations/{consumer_uuid}
6826                     "allocations": {
6827                         $RP_UUID: {
6828                             "generation": $RP_GEN,
6829                             "resources": {
6830                                 $RESOURCE_CLASS: $AMOUNT,
6831                                 ...
6832                             },
6833                         },
6834                         ...
6835                     },
6836                     "project_id": $PROJ_ID,
6837                     "user_id": $USER_ID,
6838                     "consumer_generation": $CONSUMER_GEN,
6839                 },
6840                 ...
6841               }
6842             If None, and the method determines that any inventory needs to be
6843             moved (from one provider to another and/or to a different resource
6844             class), the ReshapeNeeded exception must be raised. Otherwise, this
6845             dict must be edited in place to indicate the desired final state of
6846             allocations.
6847         :raises ReshapeNeeded: If allocations is None and any inventory needs
6848             to be moved from one provider to another and/or to a different
6849             resource class.
6850         :raises: ReshapeFailed if the requested tree reshape fails for
6851             whatever reason.
6852         """
6853         disk_gb = int(self._get_local_gb_info()['total'])
6854         memory_mb = int(self._host.get_memory_mb_total())
6855         vcpus = self._get_vcpu_total()
6856 
6857         # NOTE(yikun): If the inv record does not exists, the allocation_ratio
6858         # will use the CONF.xxx_allocation_ratio value if xxx_allocation_ratio
6859         # is set, and fallback to use the initial_xxx_allocation_ratio
6860         # otherwise.
6861         inv = provider_tree.data(nodename).inventory
6862         ratios = self._get_allocation_ratios(inv)
6863         result = {
6864             orc.VCPU: {
6865                 'total': vcpus,
6866                 'min_unit': 1,
6867                 'max_unit': vcpus,
6868                 'step_size': 1,
6869                 'allocation_ratio': ratios[orc.VCPU],
6870                 'reserved': CONF.reserved_host_cpus,
6871             },
6872             orc.MEMORY_MB: {
6873                 'total': memory_mb,
6874                 'min_unit': 1,
6875                 'max_unit': memory_mb,
6876                 'step_size': 1,
6877                 'allocation_ratio': ratios[orc.MEMORY_MB],
6878                 'reserved': CONF.reserved_host_memory_mb,
6879             },
6880         }
6881 
6882         # If a sharing DISK_GB provider exists in the provider tree, then our
6883         # storage is shared, and we should not report the DISK_GB inventory in
6884         # the compute node provider.
6885         # TODO(efried): Reinstate non-reporting of shared resource by the
6886         # compute RP once the issues from bug #1784020 have been resolved.
6887         if provider_tree.has_sharing_provider(orc.DISK_GB):
6888             LOG.debug('Ignoring sharing provider - see bug #1784020')
6889         result[orc.DISK_GB] = {
6890             'total': disk_gb,
6891             'min_unit': 1,
6892             'max_unit': disk_gb,
6893             'step_size': 1,
6894             'allocation_ratio': ratios[orc.DISK_GB],
6895             'reserved': self._get_reserved_host_disk_gb_from_config(),
6896         }
6897 
6898         # NOTE(sbauza): For the moment, the libvirt driver only supports
6899         # providing the total number of virtual GPUs for a single GPU type. If
6900         # you have multiple physical GPUs, each of them providing multiple GPU
6901         # types, only one type will be used for each of the physical GPUs.
6902         # If one of the pGPUs doesn't support this type, it won't be used.
6903         # TODO(sbauza): Use traits to make a better world.
6904         inventories_dict = self._get_gpu_inventories()
6905         if inventories_dict:
6906             self._update_provider_tree_for_vgpu(
6907                 inventories_dict, provider_tree, nodename,
6908                 allocations=allocations)
6909 
6910         if self._pmem_namespaces:
6911             pmems_by_label = self._pmem_namespaces[1]
6912             if pmems_by_label:
6913                 for label in pmems_by_label.keys():
6914                     rc_name = "CUSTOM_PMEM_NAMESPACE_%(label)s" % \
6915                             {'label': label}
6916                     result[rc_name] = {
6917                         'total': len(pmems_by_label[label]),
6918                         'max_unit': len(pmems_by_label[label]),
6919                         'min_unit': 1,
6920                         'step_size': 1,
6921                         'allocation_ratio': 1.0,
6922                         'reserved': 0
6923                     }
6924 
6925         provider_tree.update_inventory(nodename, result)
6926 
6927         traits = self._get_cpu_traits()
6928         if traits is not None:
6929             # _get_cpu_traits returns a dict of trait names mapped to boolean
6930             # values. Add traits equal to True to provider tree, remove
6931             # those False traits from provider tree.
6932             traits_to_add = [t for t in traits if traits[t]]
6933             traits_to_remove = set(traits) - set(traits_to_add)
6934             provider_tree.add_traits(nodename, *traits_to_add)
6935             provider_tree.remove_traits(nodename, *traits_to_remove)
6936 
6937         # Now that we updated the ProviderTree, we want to store it locally
6938         # so that spawn() or other methods can access it thru a getter
6939         self.provider_tree = copy.deepcopy(provider_tree)
6940 
6941     @staticmethod
6942     def _is_reshape_needed_vgpu_on_root(provider_tree, nodename):
6943         """Determine if root RP has VGPU inventories.
6944 
6945         Check to see if the root compute node provider in the tree for
6946         this host already has VGPU inventory because if it does, we either
6947         need to signal for a reshape (if _update_provider_tree_for_vgpu()
6948         has no allocations) or move the allocations within the ProviderTree if
6949         passed.
6950 
6951         :param provider_tree: The ProviderTree object for this host.
6952         :param nodename: The ComputeNode.hypervisor_hostname, also known as
6953             the name of the root node provider in the tree for this host.
6954         :returns: boolean, whether we have VGPU root inventory.
6955         """
6956         root_node = provider_tree.data(nodename)
6957         return orc.VGPU in root_node.inventory
6958 
6959     @staticmethod
6960     def _ensure_pgpu_providers(inventories_dict, provider_tree, nodename):
6961         """Ensures GPU inventory providers exist in the tree for $nodename.
6962 
6963         GPU providers are named $nodename_$gpu-device-id, e.g.
6964         ``somehost.foo.bar.com_pci_0000_84_00_0``.
6965 
6966         :param inventories_dict: Dictionary of inventories for VGPU class
6967             directly provided by _get_gpu_inventories() and which looks like:
6968                 {'pci_0000_84_00_0':
6969                     {'total': $TOTAL,
6970                      'min_unit': 1,
6971                      'max_unit': $MAX_UNIT, # defaults to $TOTAL
6972                      'step_size': 1,
6973                      'reserved': 0,
6974                      'allocation_ratio': 1.0,
6975                     }
6976                 }
6977         :param provider_tree: The ProviderTree to update.
6978         :param nodename: The ComputeNode.hypervisor_hostname, also known as
6979             the name of the root node provider in the tree for this host.
6980         :returns: dict, keyed by GPU device ID, to ProviderData object
6981             representing that resource provider in the tree
6982         """
6983         # Create the VGPU child providers if they do not already exist.
6984         # TODO(mriedem): For the moment, _get_supported_vgpu_types() only
6985         # returns one single type but that will be changed once we support
6986         # multiple types.
6987         # Note that we can't support multiple vgpu types until a reshape has
6988         # been performed on the vgpu resources provided by the root provider,
6989         # if any.
6990 
6991         # Dict of PGPU RPs keyed by their libvirt PCI name
6992         pgpu_rps = {}
6993         for pgpu_dev_id, inventory in inventories_dict.items():
6994             # For each physical GPU, we make sure to have a child provider
6995             pgpu_rp_name = '%s_%s' % (nodename, pgpu_dev_id)
6996             if not provider_tree.exists(pgpu_rp_name):
6997                 # This is the first time creating the child provider so add
6998                 # it to the tree under the root node provider.
6999                 provider_tree.new_child(pgpu_rp_name, nodename)
7000             # We want to idempotently return the resource providers with VGPUs
7001             pgpu_rp = provider_tree.data(pgpu_rp_name)
7002             pgpu_rps[pgpu_dev_id] = pgpu_rp
7003 
7004             # The VGPU inventory goes on a child provider of the given root
7005             # node, identified by $nodename.
7006             pgpu_inventory = {orc.VGPU: inventory}
7007             provider_tree.update_inventory(pgpu_rp_name, pgpu_inventory)
7008         return pgpu_rps
7009 
7010     @staticmethod
7011     def _assert_is_root_provider(
7012             rp_uuid, root_node, consumer_uuid, alloc_data):
7013         """Asserts during a reshape that rp_uuid is for the root node provider.
7014 
7015         When reshaping, inventory and allocations should be on the root node
7016         provider and then moved to child providers.
7017 
7018         :param rp_uuid: UUID of the provider that holds inventory/allocations.
7019         :param root_node: ProviderData object representing the root node in a
7020             provider tree.
7021         :param consumer_uuid: UUID of the consumer (instance) holding resource
7022             allocations against the given rp_uuid provider.
7023         :param alloc_data: dict of allocation data for the consumer.
7024         :raises: ReshapeFailed if rp_uuid is not the root node indicating a
7025             reshape was needed but the inventory/allocation structure is not
7026             expected.
7027         """
7028         if rp_uuid != root_node.uuid:
7029             # Something is wrong - VGPU inventory should
7030             # only be on the root node provider if we are
7031             # reshaping the tree.
7032             msg = (_('Unexpected VGPU resource allocation '
7033                      'on provider %(rp_uuid)s for consumer '
7034                      '%(consumer_uuid)s: %(alloc_data)s. '
7035                      'Expected VGPU allocation to be on root '
7036                      'compute node provider %(root_uuid)s.')
7037                    % {'rp_uuid': rp_uuid,
7038                       'consumer_uuid': consumer_uuid,
7039                       'alloc_data': alloc_data,
7040                       'root_uuid': root_node.uuid})
7041             raise exception.ReshapeFailed(error=msg)
7042 
7043     def _get_assigned_mdevs_for_reshape(
7044             self, instance_uuid, rp_uuid, alloc_data):
7045         """Gets the mediated devices assigned to the instance during a reshape.
7046 
7047         :param instance_uuid: UUID of the instance consuming VGPU resources
7048             on this host.
7049         :param rp_uuid: UUID of the resource provider with VGPU inventory being
7050             consumed by the instance.
7051         :param alloc_data: dict of allocation data for the instance consumer.
7052         :return: list of mediated device UUIDs assigned to the instance
7053         :raises: ReshapeFailed if the instance is not found in the hypervisor
7054             or no mediated devices were found to be assigned to the instance
7055             indicating VGPU allocations are out of sync with the hypervisor
7056         """
7057         # FIXME(sbauza): We don't really need an Instance
7058         # object, but given some libvirt.host logs needs
7059         # to have an instance name, just provide a fake one
7060         Instance = collections.namedtuple('Instance', ['uuid', 'name'])
7061         instance = Instance(uuid=instance_uuid, name=instance_uuid)
7062         mdevs = self._get_all_assigned_mediated_devices(instance)
7063         # _get_all_assigned_mediated_devices returns {} if the instance is
7064         # not found in the hypervisor
7065         if not mdevs:
7066             # If we found a VGPU allocation against a consumer
7067             # which is not an instance, the only left case for
7068             # Nova would be a migration but we don't support
7069             # this at the moment.
7070             msg = (_('Unexpected VGPU resource allocation on provider '
7071                      '%(rp_uuid)s for consumer %(consumer_uuid)s: '
7072                      '%(alloc_data)s. The allocation is made against a '
7073                      'non-existing instance or there are no devices assigned.')
7074                    % {'rp_uuid': rp_uuid, 'consumer_uuid': instance_uuid,
7075                       'alloc_data': alloc_data})
7076             raise exception.ReshapeFailed(error=msg)
7077         return mdevs
7078 
7079     def _count_vgpus_per_pgpu(self, mdev_uuids):
7080         """Count the number of VGPUs per physical GPU mediated device.
7081 
7082         :param mdev_uuids: List of physical GPU mediated device UUIDs.
7083         :return: dict, keyed by PGPU device ID, to count of VGPUs on that
7084             device
7085         """
7086         vgpu_count_per_pgpu = collections.defaultdict(int)
7087         for mdev_uuid in mdev_uuids:
7088             # libvirt name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
7089             dev_name = libvirt_utils.mdev_uuid2name(mdev_uuid)
7090             # Count how many vGPUs are in use for this instance
7091             dev_info = self._get_mediated_device_information(dev_name)
7092             pgpu_dev_id = dev_info['parent']
7093             vgpu_count_per_pgpu[pgpu_dev_id] += 1
7094         return vgpu_count_per_pgpu
7095 
7096     @staticmethod
7097     def _check_vgpu_allocations_match_real_use(
7098             vgpu_count_per_pgpu, expected_usage, rp_uuid, consumer_uuid,
7099             alloc_data):
7100         """Checks that the number of GPU devices assigned to the consumer
7101         matches what is expected from the allocations in the placement service
7102         and logs a warning if there is a mismatch.
7103 
7104         :param vgpu_count_per_pgpu: dict, keyed by PGPU device ID, to count of
7105             VGPUs on that device where each device is assigned to the consumer
7106             (guest instance on this hypervisor)
7107         :param expected_usage: The expected usage from placement for the
7108             given resource provider and consumer
7109         :param rp_uuid: UUID of the resource provider with VGPU inventory being
7110             consumed by the instance
7111         :param consumer_uuid: UUID of the consumer (instance) holding resource
7112             allocations against the given rp_uuid provider
7113         :param alloc_data: dict of allocation data for the instance consumer
7114         """
7115         actual_usage = sum(vgpu_count_per_pgpu.values())
7116         if actual_usage != expected_usage:
7117             # Don't make it blocking, just make sure you actually correctly
7118             # allocate the existing resources
7119             LOG.warning(
7120                 'Unexpected VGPU resource allocation on provider %(rp_uuid)s '
7121                 'for consumer %(consumer_uuid)s: %(alloc_data)s. Allocations '
7122                 '(%(expected_usage)s) differ from actual use '
7123                 '(%(actual_usage)s).',
7124                 {'rp_uuid': rp_uuid, 'consumer_uuid': consumer_uuid,
7125                  'alloc_data': alloc_data, 'expected_usage': expected_usage,
7126                  'actual_usage': actual_usage})
7127 
7128     def _reshape_vgpu_allocations(
7129             self, rp_uuid, root_node, consumer_uuid, alloc_data, resources,
7130             pgpu_rps):
7131         """Update existing VGPU allocations by moving them from the root node
7132         provider to the child provider for the given VGPU provider.
7133 
7134         :param rp_uuid: UUID of the VGPU resource provider with allocations
7135             from consumer_uuid (should be the root node provider before
7136             reshaping occurs)
7137         :param root_node: ProviderData object for the root compute node
7138             resource provider in the provider tree
7139         :param consumer_uuid: UUID of the consumer (instance) with VGPU
7140             allocations against the resource provider represented by rp_uuid
7141         :param alloc_data: dict of allocation information for consumer_uuid
7142         :param resources: dict, keyed by resource class, of resources allocated
7143             to consumer_uuid from rp_uuid
7144         :param pgpu_rps: dict, keyed by GPU device ID, to ProviderData object
7145             representing that resource provider in the tree
7146         :raises: ReshapeFailed if the reshape fails for whatever reason
7147         """
7148         # We've found VGPU allocations on a provider. It should be the root
7149         # node provider.
7150         self._assert_is_root_provider(
7151             rp_uuid, root_node, consumer_uuid, alloc_data)
7152 
7153         # Find which physical GPU corresponds to this allocation.
7154         mdev_uuids = self._get_assigned_mdevs_for_reshape(
7155             consumer_uuid, rp_uuid, alloc_data)
7156 
7157         vgpu_count_per_pgpu = self._count_vgpus_per_pgpu(mdev_uuids)
7158 
7159         # We need to make sure we found all the mediated devices that
7160         # correspond to an allocation.
7161         self._check_vgpu_allocations_match_real_use(
7162             vgpu_count_per_pgpu, resources[orc.VGPU],
7163             rp_uuid, consumer_uuid, alloc_data)
7164 
7165         # Add the VGPU allocation for each VGPU provider.
7166         allocs = alloc_data['allocations']
7167         for pgpu_dev_id, pgpu_rp in pgpu_rps.items():
7168             vgpu_count = vgpu_count_per_pgpu[pgpu_dev_id]
7169             if vgpu_count:
7170                 allocs[pgpu_rp.uuid] = {
7171                     'resources': {
7172                         orc.VGPU: vgpu_count
7173                     }
7174                 }
7175         # And remove the VGPU allocation from the root node provider.
7176         del resources[orc.VGPU]
7177 
7178     def _reshape_gpu_resources(
7179             self, allocations, root_node, pgpu_rps):
7180         """Reshapes the provider tree moving VGPU inventory from root to child
7181 
7182         :param allocations:
7183             Dict of allocation data of the form:
7184               { $CONSUMER_UUID: {
7185                     # The shape of each "allocations" dict below is identical
7186                     # to the return from GET /allocations/{consumer_uuid}
7187                     "allocations": {
7188                         $RP_UUID: {
7189                             "generation": $RP_GEN,
7190                             "resources": {
7191                                 $RESOURCE_CLASS: $AMOUNT,
7192                                 ...
7193                             },
7194                         },
7195                         ...
7196                     },
7197                     "project_id": $PROJ_ID,
7198                     "user_id": $USER_ID,
7199                     "consumer_generation": $CONSUMER_GEN,
7200                 },
7201                 ...
7202               }
7203         :params root_node: The root node in the provider tree
7204         :params pgpu_rps: dict, keyed by GPU device ID, to ProviderData object
7205             representing that resource provider in the tree
7206         """
7207         LOG.info('Reshaping tree; moving VGPU allocations from root '
7208                  'provider %s to child providers %s.', root_node.uuid,
7209                  pgpu_rps.values())
7210         # For each consumer in the allocations dict, look for VGPU
7211         # allocations and move them to the VGPU provider.
7212         for consumer_uuid, alloc_data in allocations.items():
7213             # Copy and iterate over the current set of providers to avoid
7214             # modifying keys while iterating.
7215             allocs = alloc_data['allocations']
7216             for rp_uuid in list(allocs):
7217                 resources = allocs[rp_uuid]['resources']
7218                 if orc.VGPU in resources:
7219                     self._reshape_vgpu_allocations(
7220                         rp_uuid, root_node, consumer_uuid, alloc_data,
7221                         resources, pgpu_rps)
7222 
7223     def _update_provider_tree_for_vgpu(self, inventories_dict, provider_tree,
7224                                        nodename, allocations=None):
7225         """Updates the provider tree for VGPU inventory.
7226 
7227         Before Stein, VGPU inventory and allocations were on the root compute
7228         node provider in the tree. Starting in Stein, the VGPU inventory is
7229         on a child provider in the tree. As a result, this method will
7230         "reshape" the tree if necessary on first start of this compute service
7231         in Stein.
7232 
7233         :param inventories_dict: Dictionary of inventories for VGPU class
7234             directly provided by _get_gpu_inventories() and which looks like:
7235                 {'pci_0000_84_00_0':
7236                     {'total': $TOTAL,
7237                      'min_unit': 1,
7238                      'max_unit': $MAX_UNIT, # defaults to $TOTAL
7239                      'step_size': 1,
7240                      'reserved': 0,
7241                      'allocation_ratio': 1.0,
7242                     }
7243                 }
7244         :param provider_tree: The ProviderTree to update.
7245         :param nodename: The ComputeNode.hypervisor_hostname, also known as
7246             the name of the root node provider in the tree for this host.
7247         :param allocations: If not None, indicates a reshape was requested and
7248             should be performed.
7249         :raises: nova.exception.ReshapeNeeded if ``allocations`` is None and
7250             the method determines a reshape of the tree is needed, i.e. VGPU
7251             inventory and allocations must be migrated from the root node
7252             provider to a child provider of VGPU resources in the tree.
7253         :raises: nova.exception.ReshapeFailed if the requested tree reshape
7254             fails for whatever reason.
7255         """
7256         # Check to see if the root compute node provider in the tree for
7257         # this host already has VGPU inventory because if it does, and
7258         # we're not currently reshaping (allocations is None), we need
7259         # to indicate that a reshape is needed to move the VGPU inventory
7260         # onto a child provider in the tree.
7261 
7262         # Ensure GPU providers are in the ProviderTree for the given inventory.
7263         pgpu_rps = self._ensure_pgpu_providers(
7264             inventories_dict, provider_tree, nodename)
7265 
7266         if self._is_reshape_needed_vgpu_on_root(provider_tree, nodename):
7267             if allocations is None:
7268                 # We have old VGPU inventory on root RP, but we haven't yet
7269                 # allocations. That means we need to ask for a reshape.
7270                 LOG.info('Requesting provider tree reshape in order to move '
7271                          'VGPU inventory from the root compute node provider '
7272                          '%s to a child provider.', nodename)
7273                 raise exception.ReshapeNeeded()
7274             # We have allocations, that means we already asked for a reshape
7275             # and the Placement API returned us them. We now need to move
7276             # those from the root RP to the needed children RPs.
7277             root_node = provider_tree.data(nodename)
7278             # Reshape VGPU provider inventory and allocations, moving them
7279             # from the root node provider to the child providers.
7280             self._reshape_gpu_resources(allocations, root_node, pgpu_rps)
7281             # Only delete the root inventory once the reshape is done
7282             if orc.VGPU in root_node.inventory:
7283                 del root_node.inventory[orc.VGPU]
7284                 provider_tree.update_inventory(nodename, root_node.inventory)
7285 
7286     def get_available_resource(self, nodename):
7287         """Retrieve resource information.
7288 
7289         This method is called when nova-compute launches, and
7290         as part of a periodic task that records the results in the DB.
7291 
7292         :param nodename: unused in this driver
7293         :returns: dictionary containing resource info
7294         """
7295 
7296         disk_info_dict = self._get_local_gb_info()
7297         data = {}
7298 
7299         # NOTE(dprince): calling capabilities before getVersion works around
7300         # an initialization issue with some versions of Libvirt (1.0.5.5).
7301         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
7302         # See: https://bugs.launchpad.net/nova/+bug/1215593
7303         data["supported_instances"] = self._get_instance_capabilities()
7304 
7305         data["vcpus"] = self._get_vcpu_total()
7306         data["memory_mb"] = self._host.get_memory_mb_total()
7307         data["local_gb"] = disk_info_dict['total']
7308         data["vcpus_used"] = self._get_vcpu_used()
7309         data["memory_mb_used"] = self._host.get_memory_mb_used()
7310         data["local_gb_used"] = disk_info_dict['used']
7311         data["hypervisor_type"] = self._host.get_driver_type()
7312         data["hypervisor_version"] = self._host.get_version()
7313         data["hypervisor_hostname"] = self._host.get_hostname()
7314         # TODO(berrange): why do we bother converting the
7315         # libvirt capabilities XML into a special JSON format ?
7316         # The data format is different across all the drivers
7317         # so we could just return the raw capabilities XML
7318         # which 'compare_cpu' could use directly
7319         #
7320         # That said, arch_filter.py now seems to rely on
7321         # the libvirt drivers format which suggests this
7322         # data format needs to be standardized across drivers
7323         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
7324 
7325         disk_free_gb = disk_info_dict['free']
7326         disk_over_committed = self._get_disk_over_committed_size_total()
7327         available_least = disk_free_gb * units.Gi - disk_over_committed
7328         data['disk_available_least'] = available_least / units.Gi
7329 
7330         data['pci_passthrough_devices'] = self._get_pci_passthrough_devices()
7331 
7332         numa_topology = self._get_host_numa_topology()
7333         if numa_topology:
7334             data['numa_topology'] = numa_topology._to_json()
7335         else:
7336             data['numa_topology'] = None
7337 
7338         return data
7339 
7340     def check_instance_shared_storage_local(self, context, instance):
7341         """Check if instance files located on shared storage.
7342 
7343         This runs check on the destination host, and then calls
7344         back to the source host to check the results.
7345 
7346         :param context: security context
7347         :param instance: nova.objects.instance.Instance object
7348         :returns:
7349          - tempfile: A dict containing the tempfile info on the destination
7350                      host
7351          - None:
7352 
7353             1. If the instance path is not existing.
7354             2. If the image backend is shared block storage type.
7355         """
7356         if self.image_backend.backend().is_shared_block_storage():
7357             return None
7358 
7359         dirpath = libvirt_utils.get_instance_path(instance)
7360 
7361         if not os.path.exists(dirpath):
7362             return None
7363 
7364         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
7365         LOG.debug("Creating tmpfile %s to verify with other "
7366                   "compute node that the instance is on "
7367                   "the same shared storage.",
7368                   tmp_file, instance=instance)
7369         os.close(fd)
7370         return {"filename": tmp_file}
7371 
7372     def check_instance_shared_storage_remote(self, context, data):
7373         return os.path.exists(data['filename'])
7374 
7375     def check_instance_shared_storage_cleanup(self, context, data):
7376         fileutils.delete_if_exists(data["filename"])
7377 
7378     def check_can_live_migrate_destination(self, context, instance,
7379                                            src_compute_info, dst_compute_info,
7380                                            block_migration=False,
7381                                            disk_over_commit=False):
7382         """Check if it is possible to execute live migration.
7383 
7384         This runs checks on the destination host, and then calls
7385         back to the source host to check the results.
7386 
7387         :param context: security context
7388         :param instance: nova.db.sqlalchemy.models.Instance
7389         :param block_migration: if true, prepare for block migration
7390         :param disk_over_commit: if true, allow disk over commit
7391         :returns: a LibvirtLiveMigrateData object
7392         """
7393         if disk_over_commit:
7394             disk_available_gb = dst_compute_info['free_disk_gb']
7395         else:
7396             disk_available_gb = dst_compute_info['disk_available_least']
7397         disk_available_mb = (
7398             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
7399 
7400         # Compare CPU
7401         if not instance.vcpu_model or not instance.vcpu_model.model:
7402             source_cpu_info = src_compute_info['cpu_info']
7403             self._compare_cpu(None, source_cpu_info, instance)
7404         else:
7405             self._compare_cpu(instance.vcpu_model, None, instance)
7406 
7407         # Create file on storage, to be checked on source host
7408         filename = self._create_shared_storage_test_file(instance)
7409 
7410         data = objects.LibvirtLiveMigrateData()
7411         data.filename = filename
7412         data.image_type = CONF.libvirt.images_type
7413         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
7414         data.graphics_listen_addr_spice = CONF.spice.server_listen
7415         if CONF.serial_console.enabled:
7416             data.serial_listen_addr = CONF.serial_console.proxyclient_address
7417         else:
7418             data.serial_listen_addr = None
7419         # Notes(eliqiao): block_migration and disk_over_commit are not
7420         # nullable, so just don't set them if they are None
7421         if block_migration is not None:
7422             data.block_migration = block_migration
7423         if disk_over_commit is not None:
7424             data.disk_over_commit = disk_over_commit
7425         data.disk_available_mb = disk_available_mb
7426         data.dst_wants_file_backed_memory = CONF.libvirt.file_backed_memory > 0
7427         data.file_backed_memory_discard = (CONF.libvirt.file_backed_memory and
7428             self._host.has_min_version(MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION,
7429                                        MIN_QEMU_FILE_BACKED_DISCARD_VERSION))
7430 
7431         return data
7432 
7433     def cleanup_live_migration_destination_check(self, context,
7434                                                  dest_check_data):
7435         """Do required cleanup on dest host after check_can_live_migrate calls
7436 
7437         :param context: security context
7438         """
7439         filename = dest_check_data.filename
7440         self._cleanup_shared_storage_test_file(filename)
7441 
7442     def check_can_live_migrate_source(self, context, instance,
7443                                       dest_check_data,
7444                                       block_device_info=None):
7445         """Check if it is possible to execute live migration.
7446 
7447         This checks if the live migration can succeed, based on the
7448         results from check_can_live_migrate_destination.
7449 
7450         :param context: security context
7451         :param instance: nova.db.sqlalchemy.models.Instance
7452         :param dest_check_data: result of check_can_live_migrate_destination
7453         :param block_device_info: result of _get_instance_block_device_info
7454         :returns: a LibvirtLiveMigrateData object
7455         """
7456         # Checking shared storage connectivity
7457         # if block migration, instances_path should not be on shared storage.
7458         source = CONF.host
7459 
7460         dest_check_data.is_shared_instance_path = (
7461             self._check_shared_storage_test_file(
7462                 dest_check_data.filename, instance))
7463 
7464         dest_check_data.is_shared_block_storage = (
7465             self._is_shared_block_storage(instance, dest_check_data,
7466                                           block_device_info))
7467 
7468         if 'block_migration' not in dest_check_data:
7469             dest_check_data.block_migration = (
7470                 not dest_check_data.is_on_shared_storage())
7471 
7472         if dest_check_data.block_migration:
7473             # TODO(eliqiao): Once block_migration flag is removed from the API
7474             # we can safely remove the if condition
7475             if dest_check_data.is_on_shared_storage():
7476                 reason = _("Block migration can not be used "
7477                            "with shared storage.")
7478                 raise exception.InvalidLocalStorage(reason=reason, path=source)
7479             if 'disk_over_commit' in dest_check_data:
7480                 self._assert_dest_node_has_enough_disk(context, instance,
7481                                         dest_check_data.disk_available_mb,
7482                                         dest_check_data.disk_over_commit,
7483                                         block_device_info)
7484             if block_device_info:
7485                 bdm = block_device_info.get('block_device_mapping')
7486                 # NOTE(eliqiao): Selective disk migrations are not supported
7487                 # with tunnelled block migrations so we can block them early.
7488                 if (bdm and
7489                     (self._block_migration_flags &
7490                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
7491                     msg = (_('Cannot block migrate instance %(uuid)s with'
7492                              ' mapped volumes. Selective block device'
7493                              ' migration is not supported with tunnelled'
7494                              ' block migrations.') % {'uuid': instance.uuid})
7495                     LOG.error(msg, instance=instance)
7496                     raise exception.MigrationPreCheckError(reason=msg)
7497         elif not (dest_check_data.is_shared_block_storage or
7498                   dest_check_data.is_shared_instance_path):
7499             reason = _("Shared storage live-migration requires either shared "
7500                        "storage or boot-from-volume with no local disks.")
7501             raise exception.InvalidSharedStorage(reason=reason, path=source)
7502 
7503         # NOTE(mikal): include the instance directory name here because it
7504         # doesn't yet exist on the destination but we want to force that
7505         # same name to be used
7506         instance_path = libvirt_utils.get_instance_path(instance,
7507                                                         relative=True)
7508         dest_check_data.instance_relative_path = instance_path
7509 
7510         # NOTE(lyarwood): Used to indicate to the dest that the src is capable
7511         # of wiring up the encrypted disk configuration for the domain.
7512         # Note that this does not require the QEMU and Libvirt versions to
7513         # decrypt LUKS to be installed on the source node. Only the Nova
7514         # utility code to generate the correct XML is required, so we can
7515         # default to True here for all computes >= Queens.
7516         dest_check_data.src_supports_native_luks = True
7517 
7518         return dest_check_data
7519 
7520     def _is_shared_block_storage(self, instance, dest_check_data,
7521                                  block_device_info=None):
7522         """Check if all block storage of an instance can be shared
7523         between source and destination of a live migration.
7524 
7525         Returns true if the instance is volume backed and has no local disks,
7526         or if the image backend is the same on source and destination and the
7527         backend shares block storage between compute nodes.
7528 
7529         :param instance: nova.objects.instance.Instance object
7530         :param dest_check_data: dict with boolean fields image_type,
7531                                 is_shared_instance_path, and is_volume_backed
7532         """
7533         if (dest_check_data.obj_attr_is_set('image_type') and
7534                 CONF.libvirt.images_type == dest_check_data.image_type and
7535                 self.image_backend.backend().is_shared_block_storage()):
7536             # NOTE(dgenin): currently true only for RBD image backend
7537             return True
7538 
7539         if (dest_check_data.is_shared_instance_path and
7540                 self.image_backend.backend().is_file_in_instance_path()):
7541             # NOTE(angdraug): file based image backends (Flat, Qcow2)
7542             # place block device files under the instance path
7543             return True
7544 
7545         if (dest_check_data.is_volume_backed and
7546                 not bool(self._get_instance_disk_info(instance,
7547                                                       block_device_info))):
7548             return True
7549 
7550         return False
7551 
7552     def _assert_dest_node_has_enough_disk(self, context, instance,
7553                                              available_mb, disk_over_commit,
7554                                              block_device_info):
7555         """Checks if destination has enough disk for block migration."""
7556         # Libvirt supports qcow2 disk format,which is usually compressed
7557         # on compute nodes.
7558         # Real disk image (compressed) may enlarged to "virtual disk size",
7559         # that is specified as the maximum disk size.
7560         # (See qemu-img -f path-to-disk)
7561         # Scheduler recognizes destination host still has enough disk space
7562         # if real disk size < available disk size
7563         # if disk_over_commit is True,
7564         #  otherwise virtual disk size < available disk size.
7565 
7566         available = 0
7567         if available_mb:
7568             available = available_mb * units.Mi
7569 
7570         disk_infos = self._get_instance_disk_info(instance, block_device_info)
7571 
7572         necessary = 0
7573         if disk_over_commit:
7574             for info in disk_infos:
7575                 necessary += int(info['disk_size'])
7576         else:
7577             for info in disk_infos:
7578                 necessary += int(info['virt_disk_size'])
7579 
7580         # Check that available disk > necessary disk
7581         if (available - necessary) < 0:
7582             reason = (_('Unable to migrate %(instance_uuid)s: '
7583                         'Disk of instance is too large(available'
7584                         ' on destination host:%(available)s '
7585                         '< need:%(necessary)s)') %
7586                       {'instance_uuid': instance.uuid,
7587                        'available': available,
7588                        'necessary': necessary})
7589             raise exception.MigrationPreCheckError(reason=reason)
7590 
7591     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
7592         """Check the host is compatible with the requested CPU
7593 
7594         :param guest_cpu: nova.objects.VirtCPUModel or None
7595         :param host_cpu_str: JSON from _get_cpu_info() method
7596 
7597         If the 'guest_cpu' parameter is not None, this will be
7598         validated for migration compatibility with the host.
7599         Otherwise the 'host_cpu_str' JSON string will be used for
7600         validation.
7601 
7602         :returns:
7603             None. if given cpu info is not compatible to this server,
7604             raise exception.
7605         """
7606 
7607         # NOTE(kchamart): Comparing host to guest CPU model for emulated
7608         # guests (<domain type='qemu'>) should not matter -- in this
7609         # mode (QEMU "TCG") the CPU is fully emulated in software and no
7610         # hardware acceleration, like KVM, is involved. So, skip the CPU
7611         # compatibility check for the QEMU domain type, and retain it for
7612         # KVM guests.
7613         if CONF.libvirt.virt_type not in ['kvm']:
7614             return
7615 
7616         if guest_cpu is None:
7617             info = jsonutils.loads(host_cpu_str)
7618             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
7619             cpu = vconfig.LibvirtConfigCPU()
7620             cpu.arch = info['arch']
7621             cpu.model = info['model']
7622             cpu.vendor = info['vendor']
7623             cpu.sockets = info['topology']['sockets']
7624             cpu.cores = info['topology']['cores']
7625             cpu.threads = info['topology']['threads']
7626             for f in info['features']:
7627                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
7628         else:
7629             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
7630 
7631         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
7632              "virCPUCompareResult")
7633         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
7634         # unknown character exists in xml, then libvirt complains
7635         try:
7636             cpu_xml = cpu.to_xml()
7637             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
7638             ret = self._host.compare_cpu(cpu_xml)
7639         except libvirt.libvirtError as e:
7640             error_code = e.get_error_code()
7641             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
7642                 LOG.debug("URI %(uri)s does not support cpu comparison. "
7643                           "It will be proceeded though. Error: %(error)s",
7644                           {'uri': self._uri(), 'error': e})
7645                 return
7646             else:
7647                 LOG.error(m, {'ret': e, 'u': u})
7648                 raise exception.MigrationPreCheckError(
7649                     reason=m % {'ret': e, 'u': u})
7650 
7651         if ret <= 0:
7652             LOG.error(m, {'ret': ret, 'u': u})
7653             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
7654 
7655     def _create_shared_storage_test_file(self, instance):
7656         """Makes tmpfile under CONF.instances_path."""
7657         dirpath = CONF.instances_path
7658         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
7659         LOG.debug("Creating tmpfile %s to notify to other "
7660                   "compute nodes that they should mount "
7661                   "the same storage.", tmp_file, instance=instance)
7662         os.close(fd)
7663         return os.path.basename(tmp_file)
7664 
7665     def _check_shared_storage_test_file(self, filename, instance):
7666         """Confirms existence of the tmpfile under CONF.instances_path.
7667 
7668         Cannot confirm tmpfile return False.
7669         """
7670         # NOTE(tpatzig): if instances_path is a shared volume that is
7671         # under heavy IO (many instances on many compute nodes),
7672         # then checking the existence of the testfile fails,
7673         # just because it takes longer until the client refreshes and new
7674         # content gets visible.
7675         # os.utime (like touch) on the directory forces the client to refresh.
7676         os.utime(CONF.instances_path, None)
7677 
7678         tmp_file = os.path.join(CONF.instances_path, filename)
7679         if not os.path.exists(tmp_file):
7680             exists = False
7681         else:
7682             exists = True
7683         LOG.debug('Check if temp file %s exists to indicate shared storage '
7684                   'is being used for migration. Exists? %s', tmp_file, exists,
7685                   instance=instance)
7686         return exists
7687 
7688     def _cleanup_shared_storage_test_file(self, filename):
7689         """Removes existence of the tmpfile under CONF.instances_path."""
7690         tmp_file = os.path.join(CONF.instances_path, filename)
7691         os.remove(tmp_file)
7692 
7693     def ensure_filtering_rules_for_instance(self, instance, network_info):
7694         """Ensure that an instance's filtering rules are enabled.
7695 
7696         When migrating an instance, we need the filtering rules to
7697         be configured on the destination host before starting the
7698         migration.
7699 
7700         Also, when restarting the compute service, we need to ensure
7701         that filtering rules exist for all running services.
7702         """
7703 
7704         self.firewall_driver.setup_basic_filtering(instance, network_info)
7705         self.firewall_driver.prepare_instance_filter(instance,
7706                 network_info)
7707 
7708         # nwfilters may be defined in a separate thread in the case
7709         # of libvirt non-blocking mode, so we wait for completion
7710         timeout_count = list(range(CONF.live_migration_retry_count))
7711         while timeout_count:
7712             if self.firewall_driver.instance_filter_exists(instance,
7713                                                            network_info):
7714                 break
7715             timeout_count.pop()
7716             if len(timeout_count) == 0:
7717                 msg = _('The firewall filter for %s does not exist')
7718                 raise exception.InternalError(msg % instance.name)
7719             greenthread.sleep(1)
7720 
7721     def filter_defer_apply_on(self):
7722         self.firewall_driver.filter_defer_apply_on()
7723 
7724     def filter_defer_apply_off(self):
7725         self.firewall_driver.filter_defer_apply_off()
7726 
7727     def live_migration(self, context, instance, dest,
7728                        post_method, recover_method, block_migration=False,
7729                        migrate_data=None):
7730         """Spawning live_migration operation for distributing high-load.
7731 
7732         :param context: security context
7733         :param instance:
7734             nova.db.sqlalchemy.models.Instance object
7735             instance object that is migrated.
7736         :param dest: destination host
7737         :param post_method:
7738             post operation method.
7739             expected nova.compute.manager._post_live_migration.
7740         :param recover_method:
7741             recovery method when any exception occurs.
7742             expected nova.compute.manager._rollback_live_migration.
7743         :param block_migration: if true, do block migration.
7744         :param migrate_data: a LibvirtLiveMigrateData object
7745 
7746         """
7747 
7748         # 'dest' will be substituted into 'migration_uri' so ensure
7749         # it does't contain any characters that could be used to
7750         # exploit the URI accepted by libvirt
7751         if not libvirt_utils.is_valid_hostname(dest):
7752             raise exception.InvalidHostname(hostname=dest)
7753 
7754         self._live_migration(context, instance, dest,
7755                              post_method, recover_method, block_migration,
7756                              migrate_data)
7757 
7758     def live_migration_abort(self, instance):
7759         """Aborting a running live-migration.
7760 
7761         :param instance: instance object that is in migration
7762 
7763         """
7764 
7765         guest = self._host.get_guest(instance)
7766         dom = guest._domain
7767 
7768         try:
7769             dom.abortJob()
7770         except libvirt.libvirtError as e:
7771             LOG.error("Failed to cancel migration %s",
7772                     encodeutils.exception_to_unicode(e), instance=instance)
7773             raise
7774 
7775     def _verify_serial_console_is_disabled(self):
7776         if CONF.serial_console.enabled:
7777 
7778             msg = _('Your destination node does not support'
7779                     ' retrieving listen addresses. In order'
7780                     ' for live migration to work properly you'
7781                     ' must disable serial console.')
7782             raise exception.MigrationError(reason=msg)
7783 
7784     def _detach_direct_passthrough_vifs(self, context,
7785                                         migrate_data, instance):
7786         """detaches passthrough vif to enable live migration
7787 
7788         :param context: security context
7789         :param migrate_data: a LibvirtLiveMigrateData object
7790         :param instance: instance object that is migrated.
7791         """
7792         # NOTE(sean-k-mooney): if we have vif data available we
7793         # loop over each vif and detach all direct passthrough
7794         # vifs to allow sriov live migration.
7795         direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
7796         vifs = [vif.source_vif for vif in migrate_data.vifs
7797                 if "source_vif" in vif and vif.source_vif]
7798         for vif in vifs:
7799             if vif['vnic_type'] in direct_vnics:
7800                 LOG.info("Detaching vif %s from instnace "
7801                          "%s for live migration", vif['id'], instance.id)
7802                 self.detach_interface(context, instance, vif)
7803 
7804     def _live_migration_operation(self, context, instance, dest,
7805                                   block_migration, migrate_data, guest,
7806                                   device_names):
7807         """Invoke the live migration operation
7808 
7809         :param context: security context
7810         :param instance:
7811             nova.db.sqlalchemy.models.Instance object
7812             instance object that is migrated.
7813         :param dest: destination host
7814         :param block_migration: if true, do block migration.
7815         :param migrate_data: a LibvirtLiveMigrateData object
7816         :param guest: the guest domain object
7817         :param device_names: list of device names that are being migrated with
7818             instance
7819 
7820         This method is intended to be run in a background thread and will
7821         block that thread until the migration is finished or failed.
7822         """
7823         try:
7824             if migrate_data.block_migration:
7825                 migration_flags = self._block_migration_flags
7826             else:
7827                 migration_flags = self._live_migration_flags
7828 
7829             serial_listen_addr = libvirt_migrate.serial_listen_addr(
7830                 migrate_data)
7831             if not serial_listen_addr:
7832                 # In this context we want to ensure that serial console is
7833                 # disabled on source node. This is because nova couldn't
7834                 # retrieve serial listen address from destination node, so we
7835                 # consider that destination node might have serial console
7836                 # disabled as well.
7837                 self._verify_serial_console_is_disabled()
7838 
7839             # NOTE(aplanas) migrate_uri will have a value only in the
7840             # case that `live_migration_inbound_addr` parameter is
7841             # set, and we propose a non tunneled migration.
7842             migrate_uri = None
7843             if ('target_connect_addr' in migrate_data and
7844                     migrate_data.target_connect_addr is not None):
7845                 dest = migrate_data.target_connect_addr
7846                 if (migration_flags &
7847                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
7848                     migrate_uri = self._migrate_uri(dest)
7849 
7850             new_xml_str = None
7851             if CONF.libvirt.virt_type != "parallels":
7852                 # If the migrate_data has port binding information for the
7853                 # destination host, we need to prepare the guest vif config
7854                 # for the destination before we start migrating the guest.
7855                 get_vif_config = None
7856                 if 'vifs' in migrate_data and migrate_data.vifs:
7857                     # NOTE(mriedem): The vif kwarg must be built on the fly
7858                     # within get_updated_guest_xml based on migrate_data.vifs.
7859                     # We could stash the virt_type from the destination host
7860                     # into LibvirtLiveMigrateData but the host kwarg is a
7861                     # nova.virt.libvirt.host.Host object and is used to check
7862                     # information like libvirt version on the destination.
7863                     # If this becomes a problem, what we could do is get the
7864                     # VIF configs while on the destination host during
7865                     # pre_live_migration() and store those in the
7866                     # LibvirtLiveMigrateData object. For now we just use the
7867                     # source host information for virt_type and
7868                     # host (version) since the conductor live_migrate method
7869                     # _check_compatible_with_source_hypervisor() ensures that
7870                     # the hypervisor types and versions are compatible.
7871                     get_vif_config = functools.partial(
7872                         self.vif_driver.get_config,
7873                         instance=instance,
7874                         image_meta=instance.image_meta,
7875                         inst_type=instance.flavor,
7876                         virt_type=CONF.libvirt.virt_type,
7877                         host=self._host)
7878                     self._detach_direct_passthrough_vifs(context,
7879                         migrate_data, instance)
7880                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
7881                     # TODO(sahid): It's not a really good idea to pass
7882                     # the method _get_volume_config and we should to find
7883                     # a way to avoid this in future.
7884                     guest, migrate_data, self._get_volume_config,
7885                     get_vif_config=get_vif_config)
7886 
7887             # NOTE(pkoniszewski): Because of precheck which blocks
7888             # tunnelled block live migration with mapped volumes we
7889             # can safely remove migrate_disks when tunnelling is on.
7890             # Otherwise we will block all tunnelled block migrations,
7891             # even when an instance does not have volumes mapped.
7892             # This is because selective disk migration is not
7893             # supported in tunnelled block live migration. Also we
7894             # cannot fallback to migrateToURI2 in this case because of
7895             # bug #1398999
7896             #
7897             # TODO(kchamart) Move the following bit to guest.migrate()
7898             if (migration_flags & libvirt.VIR_MIGRATE_TUNNELLED != 0):
7899                 device_names = []
7900 
7901             # TODO(sahid): This should be in
7902             # post_live_migration_at_source but no way to retrieve
7903             # ports acquired on the host for the guest at this
7904             # step. Since the domain is going to be removed from
7905             # libvird on source host after migration, we backup the
7906             # serial ports to release them if all went well.
7907             serial_ports = []
7908             if CONF.serial_console.enabled:
7909                 serial_ports = list(self._get_serial_ports_from_guest(guest))
7910 
7911             LOG.debug("About to invoke the migrate API", instance=instance)
7912             guest.migrate(self._live_migration_uri(dest),
7913                           migrate_uri=migrate_uri,
7914                           flags=migration_flags,
7915                           migrate_disks=device_names,
7916                           destination_xml=new_xml_str,
7917                           bandwidth=CONF.libvirt.live_migration_bandwidth)
7918             LOG.debug("Migrate API has completed", instance=instance)
7919 
7920             for hostname, port in serial_ports:
7921                 serial_console.release_port(host=hostname, port=port)
7922         except Exception as e:
7923             with excutils.save_and_reraise_exception():
7924                 LOG.error("Live Migration failure: %s", e, instance=instance)
7925 
7926         # If 'migrateToURI' fails we don't know what state the
7927         # VM instances on each host are in. Possibilities include
7928         #
7929         #  1. src==running, dst==none
7930         #
7931         #     Migration failed & rolled back, or never started
7932         #
7933         #  2. src==running, dst==paused
7934         #
7935         #     Migration started but is still ongoing
7936         #
7937         #  3. src==paused,  dst==paused
7938         #
7939         #     Migration data transfer completed, but switchover
7940         #     is still ongoing, or failed
7941         #
7942         #  4. src==paused,  dst==running
7943         #
7944         #     Migration data transfer completed, switchover
7945         #     happened but cleanup on source failed
7946         #
7947         #  5. src==none,    dst==running
7948         #
7949         #     Migration fully succeeded.
7950         #
7951         # Libvirt will aim to complete any migration operation
7952         # or roll it back. So even if the migrateToURI call has
7953         # returned an error, if the migration was not finished
7954         # libvirt should clean up.
7955         #
7956         # So we take the error raise here with a pinch of salt
7957         # and rely on the domain job info status to figure out
7958         # what really happened to the VM, which is a much more
7959         # reliable indicator.
7960         #
7961         # In particular we need to try very hard to ensure that
7962         # Nova does not "forget" about the guest. ie leaving it
7963         # running on a different host to the one recorded in
7964         # the database, as that would be a serious resource leak
7965 
7966         LOG.debug("Migration operation thread has finished",
7967                   instance=instance)
7968 
7969     def _live_migration_copy_disk_paths(self, context, instance, guest):
7970         '''Get list of disks to copy during migration
7971 
7972         :param context: security context
7973         :param instance: the instance being migrated
7974         :param guest: the Guest instance being migrated
7975 
7976         Get the list of disks to copy during migration.
7977 
7978         :returns: a list of local source paths and a list of device names to
7979             copy
7980         '''
7981 
7982         disk_paths = []
7983         device_names = []
7984         block_devices = []
7985 
7986         if (self._block_migration_flags &
7987                 libvirt.VIR_MIGRATE_TUNNELLED == 0):
7988             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
7989                 context, instance.uuid)
7990             block_device_info = driver.get_block_device_info(instance,
7991                                                              bdm_list)
7992 
7993             block_device_mappings = driver.block_device_info_get_mapping(
7994                 block_device_info)
7995             for bdm in block_device_mappings:
7996                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
7997                 block_devices.append(device_name)
7998 
7999         for dev in guest.get_all_disks():
8000             if dev.readonly or dev.shareable:
8001                 continue
8002             if dev.source_type not in ["file", "block"]:
8003                 continue
8004             if dev.target_dev in block_devices:
8005                 continue
8006             disk_paths.append(dev.source_path)
8007             device_names.append(dev.target_dev)
8008         return (disk_paths, device_names)
8009 
8010     def _live_migration_data_gb(self, instance, disk_paths):
8011         '''Calculate total amount of data to be transferred
8012 
8013         :param instance: the nova.objects.Instance being migrated
8014         :param disk_paths: list of disk paths that are being migrated
8015         with instance
8016 
8017         Calculates the total amount of data that needs to be
8018         transferred during the live migration. The actual
8019         amount copied will be larger than this, due to the
8020         guest OS continuing to dirty RAM while the migration
8021         is taking place. So this value represents the minimal
8022         data size possible.
8023 
8024         :returns: data size to be copied in GB
8025         '''
8026 
8027         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
8028         if ram_gb < 2:
8029             ram_gb = 2
8030 
8031         disk_gb = 0
8032         for path in disk_paths:
8033             try:
8034                 size = os.stat(path).st_size
8035                 size_gb = (size / units.Gi)
8036                 if size_gb < 2:
8037                     size_gb = 2
8038                 disk_gb += size_gb
8039             except OSError as e:
8040                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
8041                             {'disk': path, 'ex': e})
8042                 # Ignore error since we don't want to break
8043                 # the migration monitoring thread operation
8044 
8045         return ram_gb + disk_gb
8046 
8047     def _get_migration_flags(self, is_block_migration):
8048         if is_block_migration:
8049             return self._block_migration_flags
8050         return self._live_migration_flags
8051 
8052     def _live_migration_monitor(self, context, instance, guest,
8053                                 dest, post_method,
8054                                 recover_method, block_migration,
8055                                 migrate_data, finish_event,
8056                                 disk_paths):
8057         on_migration_failure = deque()
8058         data_gb = self._live_migration_data_gb(instance, disk_paths)
8059         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
8060         migration = migrate_data.migration
8061         curdowntime = None
8062 
8063         migration_flags = self._get_migration_flags(
8064                                   migrate_data.block_migration)
8065 
8066         n = 0
8067         start = time.time()
8068         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
8069         while True:
8070             info = guest.get_job_info()
8071 
8072             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
8073                 # Either still running, or failed or completed,
8074                 # lets untangle the mess
8075                 if not finish_event.ready():
8076                     LOG.debug("Operation thread is still running",
8077                               instance=instance)
8078                 else:
8079                     info.type = libvirt_migrate.find_job_type(guest, instance)
8080                     LOG.debug("Fixed incorrect job type to be %d",
8081                               info.type, instance=instance)
8082 
8083             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
8084                 # Migration is not yet started
8085                 LOG.debug("Migration not running yet",
8086                           instance=instance)
8087             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
8088                 # Migration is still running
8089                 #
8090                 # This is where we wire up calls to change live
8091                 # migration status. eg change max downtime, cancel
8092                 # the operation, change max bandwidth
8093                 libvirt_migrate.run_tasks(guest, instance,
8094                                           self.active_migrations,
8095                                           on_migration_failure,
8096                                           migration,
8097                                           is_post_copy_enabled)
8098 
8099                 now = time.time()
8100                 elapsed = now - start
8101 
8102                 completion_timeout = int(
8103                     CONF.libvirt.live_migration_completion_timeout * data_gb)
8104                 # NOTE(yikun): Check the completion timeout to determine
8105                 # should trigger the timeout action, and there are two choices
8106                 # ``abort`` (default) or ``force_complete``. If the action is
8107                 # set to ``force_complete``, the post-copy will be triggered
8108                 # if available else the VM will be suspended, otherwise the
8109                 # live migrate operation will be aborted.
8110                 if libvirt_migrate.should_trigger_timeout_action(
8111                         instance, elapsed, completion_timeout,
8112                         migration.status):
8113                     timeout_act = CONF.libvirt.live_migration_timeout_action
8114                     if timeout_act == 'force_complete':
8115                         self.live_migration_force_complete(instance)
8116                     else:
8117                         # timeout action is 'abort'
8118                         try:
8119                             guest.abort_job()
8120                         except libvirt.libvirtError as e:
8121                             LOG.warning("Failed to abort migration %s",
8122                                     encodeutils.exception_to_unicode(e),
8123                                     instance=instance)
8124                             self._clear_empty_migration(instance)
8125                             raise
8126 
8127                 curdowntime = libvirt_migrate.update_downtime(
8128                     guest, instance, curdowntime,
8129                     downtime_steps, elapsed)
8130 
8131                 # We loop every 500ms, so don't log on every
8132                 # iteration to avoid spamming logs for long
8133                 # running migrations. Just once every 5 secs
8134                 # is sufficient for developers to debug problems.
8135                 # We log once every 30 seconds at info to help
8136                 # admins see slow running migration operations
8137                 # when debug logs are off.
8138                 if (n % 10) == 0:
8139                     # Ignoring memory_processed, as due to repeated
8140                     # dirtying of data, this can be way larger than
8141                     # memory_total. Best to just look at what's
8142                     # remaining to copy and ignore what's done already
8143                     #
8144                     # TODO(berrange) perhaps we could include disk
8145                     # transfer stats in the progress too, but it
8146                     # might make memory info more obscure as large
8147                     # disk sizes might dwarf memory size
8148                     remaining = 100
8149                     if info.memory_total != 0:
8150                         remaining = round(info.memory_remaining *
8151                                           100 / info.memory_total)
8152 
8153                     libvirt_migrate.save_stats(instance, migration,
8154                                                info, remaining)
8155 
8156                     # NOTE(fanzhang): do not include disk transfer stats in
8157                     # the progress percentage calculation but log them.
8158                     disk_remaining = 100
8159                     if info.disk_total != 0:
8160                         disk_remaining = round(info.disk_remaining *
8161                                                100 / info.disk_total)
8162 
8163                     lg = LOG.debug
8164                     if (n % 60) == 0:
8165                         lg = LOG.info
8166 
8167                     lg("Migration running for %(secs)d secs, "
8168                        "memory %(remaining)d%% remaining "
8169                        "(bytes processed=%(processed_memory)d, "
8170                        "remaining=%(remaining_memory)d, "
8171                        "total=%(total_memory)d); "
8172                        "disk %(disk_remaining)d%% remaining "
8173                        "(bytes processed=%(processed_disk)d, "
8174                        "remaining=%(remaining_disk)d, "
8175                        "total=%(total_disk)d).",
8176                        {"secs": n / 2, "remaining": remaining,
8177                         "processed_memory": info.memory_processed,
8178                         "remaining_memory": info.memory_remaining,
8179                         "total_memory": info.memory_total,
8180                         "disk_remaining": disk_remaining,
8181                         "processed_disk": info.disk_processed,
8182                         "remaining_disk": info.disk_remaining,
8183                         "total_disk": info.disk_total}, instance=instance)
8184 
8185                 n = n + 1
8186             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
8187                 # Migration is all done
8188                 LOG.info("Migration operation has completed",
8189                          instance=instance)
8190                 post_method(context, instance, dest, block_migration,
8191                             migrate_data)
8192                 break
8193             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
8194                 # Migration did not succeed
8195                 LOG.error("Migration operation has aborted", instance=instance)
8196                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
8197                                                   on_migration_failure)
8198                 recover_method(context, instance, dest, migrate_data)
8199                 break
8200             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
8201                 # Migration was stopped by admin
8202                 LOG.warning("Migration operation was cancelled",
8203                             instance=instance)
8204                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
8205                                                   on_migration_failure)
8206                 recover_method(context, instance, dest, migrate_data,
8207                                migration_status='cancelled')
8208                 break
8209             else:
8210                 LOG.warning("Unexpected migration job type: %d",
8211                             info.type, instance=instance)
8212 
8213             time.sleep(0.5)
8214         self._clear_empty_migration(instance)
8215 
8216     def _clear_empty_migration(self, instance):
8217         try:
8218             del self.active_migrations[instance.uuid]
8219         except KeyError:
8220             LOG.warning("There are no records in active migrations "
8221                         "for instance", instance=instance)
8222 
8223     def _live_migration(self, context, instance, dest, post_method,
8224                         recover_method, block_migration,
8225                         migrate_data):
8226         """Do live migration.
8227 
8228         :param context: security context
8229         :param instance:
8230             nova.db.sqlalchemy.models.Instance object
8231             instance object that is migrated.
8232         :param dest: destination host
8233         :param post_method:
8234             post operation method.
8235             expected nova.compute.manager._post_live_migration.
8236         :param recover_method:
8237             recovery method when any exception occurs.
8238             expected nova.compute.manager._rollback_live_migration.
8239         :param block_migration: if true, do block migration.
8240         :param migrate_data: a LibvirtLiveMigrateData object
8241 
8242         This fires off a new thread to run the blocking migration
8243         operation, and then this thread monitors the progress of
8244         migration and controls its operation
8245         """
8246 
8247         guest = self._host.get_guest(instance)
8248 
8249         disk_paths = []
8250         device_names = []
8251         if (migrate_data.block_migration and
8252                 CONF.libvirt.virt_type != "parallels"):
8253             disk_paths, device_names = self._live_migration_copy_disk_paths(
8254                 context, instance, guest)
8255 
8256         opthread = utils.spawn(self._live_migration_operation,
8257                                      context, instance, dest,
8258                                      block_migration,
8259                                      migrate_data, guest,
8260                                      device_names)
8261 
8262         finish_event = eventlet.event.Event()
8263         self.active_migrations[instance.uuid] = deque()
8264 
8265         def thread_finished(thread, event):
8266             LOG.debug("Migration operation thread notification",
8267                       instance=instance)
8268             event.send()
8269         opthread.link(thread_finished, finish_event)
8270 
8271         # Let eventlet schedule the new thread right away
8272         time.sleep(0)
8273 
8274         try:
8275             LOG.debug("Starting monitoring of live migration",
8276                       instance=instance)
8277             self._live_migration_monitor(context, instance, guest, dest,
8278                                          post_method, recover_method,
8279                                          block_migration, migrate_data,
8280                                          finish_event, disk_paths)
8281         except Exception as ex:
8282             LOG.warning("Error monitoring migration: %(ex)s",
8283                         {"ex": ex}, instance=instance, exc_info=True)
8284             raise
8285         finally:
8286             LOG.debug("Live migration monitoring is all done",
8287                       instance=instance)
8288 
8289     def _is_post_copy_enabled(self, migration_flags):
8290         return (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0
8291 
8292     def live_migration_force_complete(self, instance):
8293         try:
8294             self.active_migrations[instance.uuid].append('force-complete')
8295         except KeyError:
8296             raise exception.NoActiveMigrationForInstance(
8297                 instance_id=instance.uuid)
8298 
8299     def _try_fetch_image(self, context, path, image_id, instance,
8300                          fallback_from_host=None):
8301         try:
8302             libvirt_utils.fetch_image(context, path, image_id,
8303                                       instance.trusted_certs)
8304         except exception.ImageNotFound:
8305             if not fallback_from_host:
8306                 raise
8307             LOG.debug("Image %(image_id)s doesn't exist anymore on "
8308                       "image service, attempting to copy image "
8309                       "from %(host)s",
8310                       {'image_id': image_id, 'host': fallback_from_host})
8311             libvirt_utils.copy_image(src=path, dest=path,
8312                                      host=fallback_from_host,
8313                                      receive=True)
8314 
8315     def _fetch_instance_kernel_ramdisk(self, context, instance,
8316                                        fallback_from_host=None):
8317         """Download kernel and ramdisk for instance in instance directory."""
8318         instance_dir = libvirt_utils.get_instance_path(instance)
8319         if instance.kernel_id:
8320             kernel_path = os.path.join(instance_dir, 'kernel')
8321             # NOTE(dsanders): only fetch image if it's not available at
8322             # kernel_path. This also avoids ImageNotFound exception if
8323             # the image has been deleted from glance
8324             if not os.path.exists(kernel_path):
8325                 self._try_fetch_image(context,
8326                                       kernel_path,
8327                                       instance.kernel_id,
8328                                       instance, fallback_from_host)
8329             if instance.ramdisk_id:
8330                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
8331                 # NOTE(dsanders): only fetch image if it's not available at
8332                 # ramdisk_path. This also avoids ImageNotFound exception if
8333                 # the image has been deleted from glance
8334                 if not os.path.exists(ramdisk_path):
8335                     self._try_fetch_image(context,
8336                                           ramdisk_path,
8337                                           instance.ramdisk_id,
8338                                           instance, fallback_from_host)
8339 
8340     def _reattach_instance_vifs(self, context, instance, network_info):
8341         guest = self._host.get_guest(instance)
8342         # validate that the guest has the expected number of interfaces
8343         # attached.
8344         guest_interfaces = guest.get_interfaces()
8345         # NOTE(sean-k-mooney): In general len(guest_interfaces) will
8346         # be equal to len(network_info) as interfaces will not be hot unplugged
8347         # unless they are SR-IOV direct mode interfaces. As such we do not
8348         # need an else block here as it would be a noop.
8349         if len(guest_interfaces) < len(network_info):
8350             # NOTE(sean-k-mooney): we are doing a post live migration
8351             # for a guest with sriov vif that were detached as part of
8352             # the migration. loop over the vifs and attach the missing
8353             # vif as part of the post live migration phase.
8354             direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
8355             for vif in network_info:
8356                 if vif['vnic_type'] in direct_vnics:
8357                     LOG.info("Attaching vif %s to instance %s",
8358                              vif['id'], instance.id)
8359                     self.attach_interface(context, instance,
8360                                           instance.image_meta, vif)
8361 
8362     def rollback_live_migration_at_source(self, context, instance,
8363                                           migrate_data):
8364         """reconnect sriov interfaces after failed live migration
8365         :param context: security context
8366         :param instance:  the instance being migrated
8367         :param migrate_date: a LibvirtLiveMigrateData object
8368         """
8369         network_info = network_model.NetworkInfo(
8370             [vif.source_vif for vif in migrate_data.vifs
8371                             if "source_vif" in vif and vif.source_vif])
8372         self._reattach_instance_vifs(context, instance, network_info)
8373 
8374     def rollback_live_migration_at_destination(self, context, instance,
8375                                                network_info,
8376                                                block_device_info,
8377                                                destroy_disks=True,
8378                                                migrate_data=None):
8379         """Clean up destination node after a failed live migration."""
8380         try:
8381             self.destroy(context, instance, network_info, block_device_info,
8382                          destroy_disks)
8383         finally:
8384             # NOTE(gcb): Failed block live migration may leave instance
8385             # directory at destination node, ensure it is always deleted.
8386             is_shared_instance_path = True
8387             if migrate_data:
8388                 is_shared_instance_path = migrate_data.is_shared_instance_path
8389                 if (migrate_data.obj_attr_is_set("serial_listen_ports") and
8390                         migrate_data.serial_listen_ports):
8391                     # Releases serial ports reserved.
8392                     for port in migrate_data.serial_listen_ports:
8393                         serial_console.release_port(
8394                             host=migrate_data.serial_listen_addr, port=port)
8395 
8396             if not is_shared_instance_path:
8397                 instance_dir = libvirt_utils.get_instance_path_at_destination(
8398                     instance, migrate_data)
8399                 if os.path.exists(instance_dir):
8400                     shutil.rmtree(instance_dir)
8401 
8402     def _pre_live_migration_plug_vifs(self, instance, network_info,
8403                                       migrate_data):
8404         if 'vifs' in migrate_data and migrate_data.vifs:
8405             LOG.debug('Plugging VIFs using destination host port bindings '
8406                       'before live migration.', instance=instance)
8407             vif_plug_nw_info = network_model.NetworkInfo([])
8408             for migrate_vif in migrate_data.vifs:
8409                 vif_plug_nw_info.append(migrate_vif.get_dest_vif())
8410         else:
8411             LOG.debug('Plugging VIFs before live migration.',
8412                       instance=instance)
8413             vif_plug_nw_info = network_info
8414         # Retry operation is necessary because continuous live migration
8415         # requests to the same host cause concurrent requests to iptables,
8416         # then it complains.
8417         max_retry = CONF.live_migration_retry_count
8418         for cnt in range(max_retry):
8419             try:
8420                 self.plug_vifs(instance, vif_plug_nw_info)
8421                 break
8422             except processutils.ProcessExecutionError:
8423                 if cnt == max_retry - 1:
8424                     raise
8425                 else:
8426                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
8427                                 '%(max_retry)d.',
8428                                 {'cnt': cnt, 'max_retry': max_retry},
8429                                 instance=instance)
8430                     greenthread.sleep(1)
8431 
8432     def pre_live_migration(self, context, instance, block_device_info,
8433                            network_info, disk_info, migrate_data):
8434         """Preparation live migration."""
8435         if disk_info is not None:
8436             disk_info = jsonutils.loads(disk_info)
8437 
8438         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
8439                   instance=instance)
8440         is_shared_block_storage = migrate_data.is_shared_block_storage
8441         is_shared_instance_path = migrate_data.is_shared_instance_path
8442         is_block_migration = migrate_data.block_migration
8443 
8444         if not is_shared_instance_path:
8445             instance_dir = libvirt_utils.get_instance_path_at_destination(
8446                             instance, migrate_data)
8447 
8448             if os.path.exists(instance_dir):
8449                 raise exception.DestinationDiskExists(path=instance_dir)
8450 
8451             LOG.debug('Creating instance directory: %s', instance_dir,
8452                       instance=instance)
8453             os.mkdir(instance_dir)
8454 
8455             # Recreate the disk.info file and in doing so stop the
8456             # imagebackend from recreating it incorrectly by inspecting the
8457             # contents of each file when using the Raw backend.
8458             if disk_info:
8459                 image_disk_info = {}
8460                 for info in disk_info:
8461                     image_file = os.path.basename(info['path'])
8462                     image_path = os.path.join(instance_dir, image_file)
8463                     image_disk_info[image_path] = info['type']
8464 
8465                 LOG.debug('Creating disk.info with the contents: %s',
8466                           image_disk_info, instance=instance)
8467 
8468                 image_disk_info_path = os.path.join(instance_dir,
8469                                                     'disk.info')
8470                 libvirt_utils.write_to_file(image_disk_info_path,
8471                                             jsonutils.dumps(image_disk_info))
8472 
8473             if not is_shared_block_storage:
8474                 # Ensure images and backing files are present.
8475                 LOG.debug('Checking to make sure images and backing files are '
8476                           'present before live migration.', instance=instance)
8477                 self._create_images_and_backing(
8478                     context, instance, instance_dir, disk_info,
8479                     fallback_from_host=instance.host)
8480                 if (configdrive.required_by(instance) and
8481                         CONF.config_drive_format == 'iso9660'):
8482                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
8483                     # drive needs to be copied to destination prior to
8484                     # migration when instance path is not shared and block
8485                     # storage is not shared. Files that are already present
8486                     # on destination are excluded from a list of files that
8487                     # need to be copied to destination. If we don't do that
8488                     # live migration will fail on copying iso config drive to
8489                     # destination and writing to read-only device.
8490                     # Please see bug/1246201 for more details.
8491                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
8492                     self._remotefs.copy_file(src, instance_dir)
8493 
8494             if not is_block_migration:
8495                 # NOTE(angdraug): when block storage is shared between source
8496                 # and destination and instance path isn't (e.g. volume backed
8497                 # or rbd backed instance), instance path on destination has to
8498                 # be prepared
8499 
8500                 # Required by Quobyte CI
8501                 self._ensure_console_log_for_instance(instance)
8502 
8503                 # if image has kernel and ramdisk, just download
8504                 # following normal way.
8505                 self._fetch_instance_kernel_ramdisk(context, instance)
8506 
8507         # Establishing connection to volume server.
8508         block_device_mapping = driver.block_device_info_get_mapping(
8509             block_device_info)
8510 
8511         if len(block_device_mapping):
8512             LOG.debug('Connecting volumes before live migration.',
8513                       instance=instance)
8514 
8515         for bdm in block_device_mapping:
8516             connection_info = bdm['connection_info']
8517             # NOTE(lyarwood): Handle the P to Q LM during upgrade use case
8518             # where an instance has encrypted volumes attached using the
8519             # os-brick encryptors. Do not attempt to attach the encrypted
8520             # volume using native LUKS decryption on the destionation.
8521             src_native_luks = False
8522             if migrate_data.obj_attr_is_set('src_supports_native_luks'):
8523                 src_native_luks = migrate_data.src_supports_native_luks
8524             dest_native_luks = self._is_native_luks_available()
8525             allow_native_luks = src_native_luks and dest_native_luks
8526             self._connect_volume(context, connection_info, instance,
8527                                  allow_native_luks=allow_native_luks)
8528 
8529         self._pre_live_migration_plug_vifs(
8530             instance, network_info, migrate_data)
8531 
8532         # Store server_listen and latest disk device info
8533         if not migrate_data:
8534             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
8535         else:
8536             migrate_data.bdms = []
8537         # Store live_migration_inbound_addr
8538         migrate_data.target_connect_addr = \
8539             CONF.libvirt.live_migration_inbound_addr
8540         migrate_data.supported_perf_events = self._supported_perf_events
8541 
8542         migrate_data.serial_listen_ports = []
8543         if CONF.serial_console.enabled:
8544             num_ports = hardware.get_number_of_serial_ports(
8545                 instance.flavor, instance.image_meta)
8546             for port in six.moves.range(num_ports):
8547                 migrate_data.serial_listen_ports.append(
8548                     serial_console.acquire_port(
8549                         migrate_data.serial_listen_addr))
8550 
8551         for vol in block_device_mapping:
8552             connection_info = vol['connection_info']
8553             if connection_info.get('serial'):
8554                 disk_info = blockinfo.get_info_from_bdm(
8555                     instance, CONF.libvirt.virt_type,
8556                     instance.image_meta, vol)
8557 
8558                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
8559                 bdmi.serial = connection_info['serial']
8560                 bdmi.connection_info = connection_info
8561                 bdmi.bus = disk_info['bus']
8562                 bdmi.dev = disk_info['dev']
8563                 bdmi.type = disk_info['type']
8564                 bdmi.format = disk_info.get('format')
8565                 bdmi.boot_index = disk_info.get('boot_index')
8566                 volume_secret = self._host.find_secret('volume', vol.volume_id)
8567                 if volume_secret:
8568                     bdmi.encryption_secret_uuid = volume_secret.UUIDString()
8569 
8570                 migrate_data.bdms.append(bdmi)
8571 
8572         return migrate_data
8573 
8574     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
8575                                image_id, instance, size,
8576                                fallback_from_host=None):
8577         try:
8578             image.cache(fetch_func=fetch_func,
8579                         context=context,
8580                         filename=filename,
8581                         image_id=image_id,
8582                         size=size,
8583                         trusted_certs=instance.trusted_certs)
8584         except exception.ImageNotFound:
8585             if not fallback_from_host:
8586                 raise
8587             LOG.debug("Image %(image_id)s doesn't exist anymore "
8588                       "on image service, attempting to copy "
8589                       "image from %(host)s",
8590                       {'image_id': image_id, 'host': fallback_from_host},
8591                       instance=instance)
8592 
8593             def copy_from_host(target):
8594                 libvirt_utils.copy_image(src=target,
8595                                          dest=target,
8596                                          host=fallback_from_host,
8597                                          receive=True)
8598             image.cache(fetch_func=copy_from_host, size=size,
8599                         filename=filename)
8600 
8601         # NOTE(lyarwood): If the instance vm_state is shelved offloaded then we
8602         # must be unshelving for _try_fetch_image_cache to be called.
8603         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
8604             # NOTE(lyarwood): When using the rbd imagebackend the call to cache
8605             # above will attempt to clone from the shelved snapshot in Glance
8606             # if available from this compute. We then need to flatten the
8607             # resulting image to avoid it still referencing and ultimately
8608             # blocking the removal of the shelved snapshot at the end of the
8609             # unshelve. This is a no-op for all but the rbd imagebackend.
8610             try:
8611                 image.flatten()
8612                 LOG.debug('Image %s flattened successfully while unshelving '
8613                           'instance.', image.path, instance=instance)
8614             except NotImplementedError:
8615                 # NOTE(lyarwood): There's an argument to be made for logging
8616                 # our inability to call flatten here, however given this isn't
8617                 # implemented for most of the backends it may do more harm than
8618                 # good, concerning operators etc so for now just pass.
8619                 pass
8620 
8621     def _create_images_and_backing(self, context, instance, instance_dir,
8622                                    disk_info, fallback_from_host=None):
8623         """:param context: security context
8624            :param instance:
8625                nova.db.sqlalchemy.models.Instance object
8626                instance object that is migrated.
8627            :param instance_dir:
8628                instance path to use, calculated externally to handle block
8629                migrating an instance with an old style instance path
8630            :param disk_info:
8631                disk info specified in _get_instance_disk_info_from_config
8632                (list of dicts)
8633            :param fallback_from_host:
8634                host where we can retrieve images if the glance images are
8635                not available.
8636 
8637         """
8638 
8639         # Virtuozzo containers don't use backing file
8640         if (CONF.libvirt.virt_type == "parallels" and
8641                 instance.vm_mode == fields.VMMode.EXE):
8642             return
8643 
8644         if not disk_info:
8645             disk_info = []
8646 
8647         for info in disk_info:
8648             base = os.path.basename(info['path'])
8649             # Get image type and create empty disk image, and
8650             # create backing file in case of qcow2.
8651             instance_disk = os.path.join(instance_dir, base)
8652             if not info['backing_file'] and not os.path.exists(instance_disk):
8653                 libvirt_utils.create_image(info['type'], instance_disk,
8654                                            info['virt_disk_size'])
8655             elif info['backing_file']:
8656                 # Creating backing file follows same way as spawning instances.
8657                 cache_name = os.path.basename(info['backing_file'])
8658 
8659                 disk = self.image_backend.by_name(instance, instance_disk,
8660                                                   CONF.libvirt.images_type)
8661                 if cache_name.startswith('ephemeral'):
8662                     # The argument 'size' is used by image.cache to
8663                     # validate disk size retrieved from cache against
8664                     # the instance disk size (should always return OK)
8665                     # and ephemeral_size is used by _create_ephemeral
8666                     # to build the image if the disk is not already
8667                     # cached.
8668                     disk.cache(
8669                         fetch_func=self._create_ephemeral,
8670                         fs_label=cache_name,
8671                         os_type=instance.os_type,
8672                         filename=cache_name,
8673                         size=info['virt_disk_size'],
8674                         ephemeral_size=info['virt_disk_size'] / units.Gi)
8675                 elif cache_name.startswith('swap'):
8676                     inst_type = instance.get_flavor()
8677                     swap_mb = inst_type.swap
8678                     disk.cache(fetch_func=self._create_swap,
8679                                 filename="swap_%s" % swap_mb,
8680                                 size=swap_mb * units.Mi,
8681                                 swap_mb=swap_mb)
8682                 else:
8683                     self._try_fetch_image_cache(disk,
8684                                                 libvirt_utils.fetch_image,
8685                                                 context, cache_name,
8686                                                 instance.image_ref,
8687                                                 instance,
8688                                                 info['virt_disk_size'],
8689                                                 fallback_from_host)
8690 
8691         # if disk has kernel and ramdisk, just download
8692         # following normal way.
8693         self._fetch_instance_kernel_ramdisk(
8694             context, instance, fallback_from_host=fallback_from_host)
8695 
8696     def post_live_migration(self, context, instance, block_device_info,
8697                             migrate_data=None):
8698         # Disconnect from volume server
8699         block_device_mapping = driver.block_device_info_get_mapping(
8700                 block_device_info)
8701         for vol in block_device_mapping:
8702             # NOTE(mdbooth): The block_device_info we were passed was
8703             # initialized with BDMs from the source host before they were
8704             # updated to point to the destination. We can safely use this to
8705             # disconnect the source without re-fetching.
8706             self._disconnect_volume(context, vol['connection_info'], instance)
8707 
8708     def post_live_migration_at_source(self, context, instance, network_info):
8709         """Unplug VIFs from networks at source.
8710 
8711         :param context: security context
8712         :param instance: instance object reference
8713         :param network_info: instance network information
8714         """
8715         self.unplug_vifs(instance, network_info)
8716 
8717     def post_live_migration_at_destination(self, context,
8718                                            instance,
8719                                            network_info,
8720                                            block_migration=False,
8721                                            block_device_info=None):
8722         """Post operation of live migration at destination host.
8723 
8724         :param context: security context
8725         :param instance:
8726             nova.db.sqlalchemy.models.Instance object
8727             instance object that is migrated.
8728         :param network_info: instance network information
8729         :param block_migration: if true, post operation of block_migration.
8730         """
8731         self._reattach_instance_vifs(context, instance, network_info)
8732 
8733     def _get_instance_disk_info_from_config(self, guest_config,
8734                                             block_device_info):
8735         """Get the non-volume disk information from the domain xml
8736 
8737         :param LibvirtConfigGuest guest_config: the libvirt domain config
8738                                                 for the instance
8739         :param dict block_device_info: block device info for BDMs
8740         :returns disk_info: list of dicts with keys:
8741 
8742           * 'type': the disk type (str)
8743           * 'path': the disk path (str)
8744           * 'virt_disk_size': the virtual disk size (int)
8745           * 'backing_file': backing file of a disk image (str)
8746           * 'disk_size': physical disk size (int)
8747           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
8748         """
8749         block_device_mapping = driver.block_device_info_get_mapping(
8750             block_device_info)
8751 
8752         volume_devices = set()
8753         for vol in block_device_mapping:
8754             disk_dev = vol['mount_device'].rpartition("/")[2]
8755             volume_devices.add(disk_dev)
8756 
8757         disk_info = []
8758 
8759         if (guest_config.virt_type == 'parallels' and
8760                 guest_config.os_type == fields.VMMode.EXE):
8761             node_type = 'filesystem'
8762         else:
8763             node_type = 'disk'
8764 
8765         for device in guest_config.devices:
8766             if device.root_name != node_type:
8767                 continue
8768             disk_type = device.source_type
8769             if device.root_name == 'filesystem':
8770                 target = device.target_dir
8771                 if device.source_type == 'file':
8772                     path = device.source_file
8773                 elif device.source_type == 'block':
8774                     path = device.source_dev
8775                 else:
8776                     path = None
8777             else:
8778                 target = device.target_dev
8779                 path = device.source_path
8780 
8781             if not path:
8782                 LOG.debug('skipping disk for %s as it does not have a path',
8783                           guest_config.name)
8784                 continue
8785 
8786             if disk_type not in ['file', 'block']:
8787                 LOG.debug('skipping disk because it looks like a volume', path)
8788                 continue
8789 
8790             if target in volume_devices:
8791                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
8792                           'volume', {'path': path, 'target': target})
8793                 continue
8794 
8795             if device.root_name == 'filesystem':
8796                 driver_type = device.driver_type
8797             else:
8798                 driver_type = device.driver_format
8799             # get the real disk size or
8800             # raise a localized error if image is unavailable
8801             if disk_type == 'file' and driver_type == 'ploop':
8802                 dk_size = 0
8803                 for dirpath, dirnames, filenames in os.walk(path):
8804                     for f in filenames:
8805                         fp = os.path.join(dirpath, f)
8806                         dk_size += os.path.getsize(fp)
8807                 qemu_img_info = disk_api.get_disk_info(path)
8808                 virt_size = qemu_img_info.virtual_size
8809                 backing_file = libvirt_utils.get_disk_backing_file(path)
8810                 over_commit_size = int(virt_size) - dk_size
8811 
8812             elif disk_type == 'file' and driver_type == 'qcow2':
8813                 qemu_img_info = disk_api.get_disk_info(path)
8814                 dk_size = qemu_img_info.disk_size
8815                 virt_size = qemu_img_info.virtual_size
8816                 backing_file = libvirt_utils.get_disk_backing_file(path)
8817                 over_commit_size = int(virt_size) - dk_size
8818 
8819             elif disk_type == 'file':
8820                 dk_size = os.stat(path).st_blocks * 512
8821                 virt_size = os.path.getsize(path)
8822                 backing_file = ""
8823                 over_commit_size = 0
8824 
8825             elif disk_type == 'block' and block_device_info:
8826                 dk_size = lvm.get_volume_size(path)
8827                 virt_size = dk_size
8828                 backing_file = ""
8829                 over_commit_size = 0
8830 
8831             else:
8832                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
8833                           'determine if volume',
8834                           {'path': path, 'target': target})
8835                 continue
8836 
8837             disk_info.append({'type': driver_type,
8838                               'path': path,
8839                               'virt_disk_size': virt_size,
8840                               'backing_file': backing_file,
8841                               'disk_size': dk_size,
8842                               'over_committed_disk_size': over_commit_size})
8843         return disk_info
8844 
8845     def _get_instance_disk_info(self, instance, block_device_info):
8846         try:
8847             guest = self._host.get_guest(instance)
8848             config = guest.get_config()
8849         except libvirt.libvirtError as ex:
8850             error_code = ex.get_error_code()
8851             LOG.warning('Error from libvirt while getting description of '
8852                         '%(instance_name)s: [Error Code %(error_code)s] '
8853                         '%(ex)s',
8854                         {'instance_name': instance.name,
8855                          'error_code': error_code,
8856                          'ex': encodeutils.exception_to_unicode(ex)},
8857                         instance=instance)
8858             raise exception.InstanceNotFound(instance_id=instance.uuid)
8859 
8860         return self._get_instance_disk_info_from_config(config,
8861                                                         block_device_info)
8862 
8863     def get_instance_disk_info(self, instance,
8864                                block_device_info=None):
8865         return jsonutils.dumps(
8866             self._get_instance_disk_info(instance, block_device_info))
8867 
8868     def _get_disk_over_committed_size_total(self):
8869         """Return total over committed disk size for all instances."""
8870         # Disk size that all instance uses : virtual_size - disk_size
8871         disk_over_committed_size = 0
8872         instance_domains = self._host.list_instance_domains(only_running=False)
8873         if not instance_domains:
8874             return disk_over_committed_size
8875 
8876         # Get all instance uuids
8877         instance_uuids = [dom.UUIDString() for dom in instance_domains]
8878         ctx = nova_context.get_admin_context()
8879         # Get instance object list by uuid filter
8880         filters = {'uuid': instance_uuids}
8881         # NOTE(ankit): objects.InstanceList.get_by_filters method is
8882         # getting called twice one is here and another in the
8883         # _update_available_resource method of resource_tracker. Since
8884         # _update_available_resource method is synchronized, there is a
8885         # possibility the instances list retrieved here to calculate
8886         # disk_over_committed_size would differ to the list you would get
8887         # in _update_available_resource method for calculating usages based
8888         # on instance utilization.
8889         local_instance_list = objects.InstanceList.get_by_filters(
8890             ctx, filters, use_slave=True)
8891         # Convert instance list to dictionary with instance uuid as key.
8892         local_instances = {inst.uuid: inst for inst in local_instance_list}
8893 
8894         # Get bdms by instance uuids
8895         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
8896             ctx, instance_uuids)
8897 
8898         for dom in instance_domains:
8899             try:
8900                 guest = libvirt_guest.Guest(dom)
8901                 config = guest.get_config()
8902 
8903                 block_device_info = None
8904                 if guest.uuid in local_instances \
8905                         and (bdms and guest.uuid in bdms):
8906                     # Get block device info for instance
8907                     block_device_info = driver.get_block_device_info(
8908                         local_instances[guest.uuid], bdms[guest.uuid])
8909 
8910                 disk_infos = self._get_instance_disk_info_from_config(
8911                     config, block_device_info)
8912                 if not disk_infos:
8913                     continue
8914 
8915                 for info in disk_infos:
8916                     disk_over_committed_size += int(
8917                         info['over_committed_disk_size'])
8918             except libvirt.libvirtError as ex:
8919                 error_code = ex.get_error_code()
8920                 LOG.warning(
8921                     'Error from libvirt while getting description of '
8922                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
8923                     {'instance_name': guest.name,
8924                      'error_code': error_code,
8925                      'ex': encodeutils.exception_to_unicode(ex)})
8926             except OSError as e:
8927                 if e.errno in (errno.ENOENT, errno.ESTALE):
8928                     LOG.warning('Periodic task is updating the host stat, '
8929                                 'it is trying to get disk %(i_name)s, '
8930                                 'but disk file was removed by concurrent '
8931                                 'operations such as resize.',
8932                                 {'i_name': guest.name})
8933                 elif e.errno == errno.EACCES:
8934                     LOG.warning('Periodic task is updating the host stat, '
8935                                 'it is trying to get disk %(i_name)s, '
8936                                 'but access is denied. It is most likely '
8937                                 'due to a VM that exists on the compute '
8938                                 'node but is not managed by Nova.',
8939                                 {'i_name': guest.name})
8940                 else:
8941                     raise
8942             except exception.VolumeBDMPathNotFound as e:
8943                 LOG.warning('Periodic task is updating the host stats, '
8944                             'it is trying to get disk info for %(i_name)s, '
8945                             'but the backing volume block device was removed '
8946                             'by concurrent operations such as resize. '
8947                             'Error: %(error)s',
8948                             {'i_name': guest.name, 'error': e})
8949             except exception.DiskNotFound:
8950                 with excutils.save_and_reraise_exception() as err_ctxt:
8951                     # If the instance is undergoing a task state transition,
8952                     # like moving to another host or is being deleted, we
8953                     # should ignore this instance and move on.
8954                     if guest.uuid in local_instances:
8955                         inst = local_instances[guest.uuid]
8956                         # bug 1774249 indicated when instance is in RESIZED
8957                         # state it might also can't find back disk
8958                         if (inst.task_state is not None or
8959                             inst.vm_state == vm_states.RESIZED):
8960                             LOG.info('Periodic task is updating the host '
8961                                      'stats; it is trying to get disk info '
8962                                      'for %(i_name)s, but the backing disk '
8963                                      'was removed by a concurrent operation '
8964                                      '(task_state=%(task_state)s) and '
8965                                      '(vm_state=%(vm_state)s)',
8966                                      {'i_name': guest.name,
8967                                       'task_state': inst.task_state,
8968                                       'vm_state': inst.vm_state},
8969                                      instance=inst)
8970                             err_ctxt.reraise = False
8971 
8972             # NOTE(gtt116): give other tasks a chance.
8973             greenthread.sleep(0)
8974         return disk_over_committed_size
8975 
8976     def unfilter_instance(self, instance, network_info):
8977         """See comments of same method in firewall_driver."""
8978         self.firewall_driver.unfilter_instance(instance,
8979                                                network_info=network_info)
8980 
8981     def get_available_nodes(self, refresh=False):
8982         return [self._host.get_hostname()]
8983 
8984     def get_host_cpu_stats(self):
8985         """Return the current CPU state of the host."""
8986         return self._host.get_cpu_stats()
8987 
8988     def get_host_uptime(self):
8989         """Returns the result of calling "uptime"."""
8990         out, err = processutils.execute('env', 'LANG=C', 'uptime')
8991         return out
8992 
8993     def manage_image_cache(self, context, all_instances):
8994         """Manage the local cache of images."""
8995         self.image_cache_manager.update(context, all_instances)
8996 
8997     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
8998                                   shared_storage=False):
8999         """Used only for cleanup in case migrate_disk_and_power_off fails."""
9000         try:
9001             if os.path.exists(inst_base_resize):
9002                 shutil.rmtree(inst_base, ignore_errors=True)
9003                 os.rename(inst_base_resize, inst_base)
9004                 if not shared_storage:
9005                     self._remotefs.remove_dir(dest, inst_base)
9006         except Exception:
9007             pass
9008 
9009     def _is_storage_shared_with(self, dest, inst_base):
9010         # NOTE (rmk): There are two methods of determining whether we are
9011         #             on the same filesystem: the source and dest IP are the
9012         #             same, or we create a file on the dest system via SSH
9013         #             and check whether the source system can also see it.
9014         # NOTE (drwahl): Actually, there is a 3rd way: if images_type is rbd,
9015         #                it will always be shared storage
9016         if CONF.libvirt.images_type == 'rbd':
9017             return True
9018         shared_storage = (dest == self.get_host_ip_addr())
9019         if not shared_storage:
9020             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
9021             tmp_path = os.path.join(inst_base, tmp_file)
9022 
9023             try:
9024                 self._remotefs.create_file(dest, tmp_path)
9025                 if os.path.exists(tmp_path):
9026                     shared_storage = True
9027                     os.unlink(tmp_path)
9028                 else:
9029                     self._remotefs.remove_file(dest, tmp_path)
9030             except Exception:
9031                 pass
9032         return shared_storage
9033 
9034     def migrate_disk_and_power_off(self, context, instance, dest,
9035                                    flavor, network_info,
9036                                    block_device_info=None,
9037                                    timeout=0, retry_interval=0):
9038         LOG.debug("Starting migrate_disk_and_power_off",
9039                    instance=instance)
9040 
9041         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
9042 
9043         # get_bdm_ephemeral_disk_size() will return 0 if the new
9044         # instance's requested block device mapping contain no
9045         # ephemeral devices. However, we still want to check if
9046         # the original instance's ephemeral_gb property was set and
9047         # ensure that the new requested flavor ephemeral size is greater
9048         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
9049                     instance.flavor.ephemeral_gb)
9050 
9051         # Checks if the migration needs a disk resize down.
9052         root_down = flavor.root_gb < instance.flavor.root_gb
9053         ephemeral_down = flavor.ephemeral_gb < eph_size
9054         booted_from_volume = self._is_booted_from_volume(block_device_info)
9055 
9056         if (root_down and not booted_from_volume) or ephemeral_down:
9057             reason = _("Unable to resize disk down.")
9058             raise exception.InstanceFaultRollback(
9059                 exception.ResizeError(reason=reason))
9060 
9061         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
9062         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
9063             reason = _("Migration is not supported for LVM backed instances")
9064             raise exception.InstanceFaultRollback(
9065                 exception.MigrationPreCheckError(reason=reason))
9066 
9067         # copy disks to destination
9068         # rename instance dir to +_resize at first for using
9069         # shared storage for instance dir (eg. NFS).
9070         inst_base = libvirt_utils.get_instance_path(instance)
9071         inst_base_resize = inst_base + "_resize"
9072         shared_storage = self._is_storage_shared_with(dest, inst_base)
9073 
9074         # try to create the directory on the remote compute node
9075         # if this fails we pass the exception up the stack so we can catch
9076         # failures here earlier
9077         if not shared_storage:
9078             try:
9079                 self._remotefs.create_dir(dest, inst_base)
9080             except processutils.ProcessExecutionError as e:
9081                 reason = _("not able to execute ssh command: %s") % e
9082                 raise exception.InstanceFaultRollback(
9083                     exception.ResizeError(reason=reason))
9084 
9085         self.power_off(instance, timeout, retry_interval)
9086 
9087         block_device_mapping = driver.block_device_info_get_mapping(
9088             block_device_info)
9089         for vol in block_device_mapping:
9090             connection_info = vol['connection_info']
9091             self._disconnect_volume(context, connection_info, instance)
9092 
9093         disk_info = self._get_instance_disk_info(instance, block_device_info)
9094 
9095         try:
9096             os.rename(inst_base, inst_base_resize)
9097             # if we are migrating the instance with shared storage then
9098             # create the directory.  If it is a remote node the directory
9099             # has already been created
9100             if shared_storage:
9101                 dest = None
9102                 fileutils.ensure_tree(inst_base)
9103 
9104             on_execute = lambda process: \
9105                 self.job_tracker.add_job(instance, process.pid)
9106             on_completion = lambda process: \
9107                 self.job_tracker.remove_job(instance, process.pid)
9108 
9109             for info in disk_info:
9110                 # assume inst_base == dirname(info['path'])
9111                 img_path = info['path']
9112                 fname = os.path.basename(img_path)
9113                 from_path = os.path.join(inst_base_resize, fname)
9114 
9115                 # We will not copy over the swap disk here, and rely on
9116                 # finish_migration to re-create it for us. This is ok because
9117                 # the OS is shut down, and as recreating a swap disk is very
9118                 # cheap it is more efficient than copying either locally or
9119                 # over the network. This also means we don't have to resize it.
9120                 if fname == 'disk.swap':
9121                     continue
9122 
9123                 compression = info['type'] not in NO_COMPRESSION_TYPES
9124                 libvirt_utils.copy_image(from_path, img_path, host=dest,
9125                                          on_execute=on_execute,
9126                                          on_completion=on_completion,
9127                                          compression=compression)
9128 
9129             # Ensure disk.info is written to the new path to avoid disks being
9130             # reinspected and potentially changing format.
9131             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
9132             if os.path.exists(src_disk_info_path):
9133                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
9134                 libvirt_utils.copy_image(src_disk_info_path,
9135                                          dst_disk_info_path,
9136                                          host=dest, on_execute=on_execute,
9137                                          on_completion=on_completion)
9138         except Exception:
9139             with excutils.save_and_reraise_exception():
9140                 self._cleanup_remote_migration(dest, inst_base,
9141                                                inst_base_resize,
9142                                                shared_storage)
9143 
9144         return jsonutils.dumps(disk_info)
9145 
9146     def _wait_for_running(self, instance):
9147         state = self.get_info(instance).state
9148 
9149         if state == power_state.RUNNING:
9150             LOG.info("Instance running successfully.", instance=instance)
9151             raise loopingcall.LoopingCallDone()
9152 
9153     @staticmethod
9154     def _disk_raw_to_qcow2(path):
9155         """Converts a raw disk to qcow2."""
9156         path_qcow = path + '_qcow'
9157         images.convert_image(path, path_qcow, 'raw', 'qcow2')
9158         os.rename(path_qcow, path)
9159 
9160     def finish_migration(self, context, migration, instance, disk_info,
9161                          network_info, image_meta, resize_instance,
9162                          block_device_info=None, power_on=True):
9163         LOG.debug("Starting finish_migration", instance=instance)
9164 
9165         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
9166                                                   instance,
9167                                                   image_meta,
9168                                                   block_device_info)
9169         # assume _create_image does nothing if a target file exists.
9170         # NOTE: This has the intended side-effect of fetching a missing
9171         # backing file.
9172         self._create_image(context, instance, block_disk_info['mapping'],
9173                            block_device_info=block_device_info,
9174                            ignore_bdi_for_swap=True,
9175                            fallback_from_host=migration.source_compute)
9176 
9177         # Required by Quobyte CI
9178         self._ensure_console_log_for_instance(instance)
9179 
9180         gen_confdrive = functools.partial(
9181             self._create_configdrive, context, instance,
9182             InjectionInfo(admin_pass=None, network_info=network_info,
9183                           files=None))
9184 
9185         # Convert raw disks to qcow2 if migrating to host which uses
9186         # qcow2 from host which uses raw.
9187         disk_info = jsonutils.loads(disk_info)
9188         for info in disk_info:
9189             path = info['path']
9190             disk_name = os.path.basename(path)
9191 
9192             # NOTE(mdbooth): The code below looks wrong, but is actually
9193             # required to prevent a security hole when migrating from a host
9194             # with use_cow_images=False to one with use_cow_images=True.
9195             # Imagebackend uses use_cow_images to select between the
9196             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
9197             # writes to disk.info, but does not read it as it assumes qcow2.
9198             # Therefore if we don't convert raw to qcow2 here, a raw disk will
9199             # be incorrectly assumed to be qcow2, which is a severe security
9200             # flaw. The reverse is not true, because the atrociously-named-Raw
9201             # backend supports both qcow2 and raw disks, and will choose
9202             # appropriately between them as long as disk.info exists and is
9203             # correctly populated, which it is because Qcow2 writes to
9204             # disk.info.
9205             #
9206             # In general, we do not yet support format conversion during
9207             # migration. For example:
9208             #   * Converting from use_cow_images=True to use_cow_images=False
9209             #     isn't handled. This isn't a security bug, but is almost
9210             #     certainly buggy in other cases, as the 'Raw' backend doesn't
9211             #     expect a backing file.
9212             #   * Converting to/from lvm and rbd backends is not supported.
9213             #
9214             # This behaviour is inconsistent, and therefore undesirable for
9215             # users. It is tightly-coupled to implementation quirks of 2
9216             # out of 5 backends in imagebackend and defends against a severe
9217             # security flaw which is not at all obvious without deep analysis,
9218             # and is therefore undesirable to developers. We should aim to
9219             # remove it. This will not be possible, though, until we can
9220             # represent the storage layout of a specific instance
9221             # independent of the default configuration of the local compute
9222             # host.
9223 
9224             # Config disks are hard-coded to be raw even when
9225             # use_cow_images=True (see _get_disk_config_image_type),so don't
9226             # need to be converted.
9227             if (disk_name != 'disk.config' and
9228                         info['type'] == 'raw' and CONF.use_cow_images):
9229                 self._disk_raw_to_qcow2(info['path'])
9230 
9231         xml = self._get_guest_xml(context, instance, network_info,
9232                                   block_disk_info, image_meta,
9233                                   block_device_info=block_device_info)
9234         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
9235         # or not we've migrated to another host, because we unplug VIFs locally
9236         # and the status change in the port might go undetected by the neutron
9237         # L2 agent (or neutron server) so neutron may not know that the VIF was
9238         # unplugged in the first place and never send an event.
9239         guest = self._create_domain_and_network(context, xml, instance,
9240                                         network_info,
9241                                         block_device_info=block_device_info,
9242                                         power_on=power_on,
9243                                         vifs_already_plugged=True,
9244                                         post_xml_callback=gen_confdrive)
9245         if power_on:
9246             timer = loopingcall.FixedIntervalLoopingCall(
9247                                                     self._wait_for_running,
9248                                                     instance)
9249             timer.start(interval=0.5).wait()
9250 
9251             # Sync guest time after migration.
9252             guest.sync_guest_time()
9253 
9254         LOG.debug("finish_migration finished successfully.", instance=instance)
9255 
9256     def _cleanup_failed_migration(self, inst_base):
9257         """Make sure that a failed migrate doesn't prevent us from rolling
9258         back in a revert.
9259         """
9260         try:
9261             shutil.rmtree(inst_base)
9262         except OSError as e:
9263             if e.errno != errno.ENOENT:
9264                 raise
9265 
9266     def finish_revert_migration(self, context, instance, network_info,
9267                                 block_device_info=None, power_on=True):
9268         LOG.debug("Starting finish_revert_migration",
9269                   instance=instance)
9270 
9271         inst_base = libvirt_utils.get_instance_path(instance)
9272         inst_base_resize = inst_base + "_resize"
9273 
9274         # NOTE(danms): if we're recovering from a failed migration,
9275         # make sure we don't have a left-over same-host base directory
9276         # that would conflict. Also, don't fail on the rename if the
9277         # failure happened early.
9278         if os.path.exists(inst_base_resize):
9279             self._cleanup_failed_migration(inst_base)
9280             os.rename(inst_base_resize, inst_base)
9281 
9282         root_disk = self.image_backend.by_name(instance, 'disk')
9283         # Once we rollback, the snapshot is no longer needed, so remove it
9284         if root_disk.exists():
9285             root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
9286             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
9287 
9288         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
9289                                             instance,
9290                                             instance.image_meta,
9291                                             block_device_info)
9292         xml = self._get_guest_xml(context, instance, network_info, disk_info,
9293                                   instance.image_meta,
9294                                   block_device_info=block_device_info)
9295         self._create_domain_and_network(context, xml, instance, network_info,
9296                                         block_device_info=block_device_info,
9297                                         power_on=power_on)
9298 
9299         if power_on:
9300             timer = loopingcall.FixedIntervalLoopingCall(
9301                                                     self._wait_for_running,
9302                                                     instance)
9303             timer.start(interval=0.5).wait()
9304 
9305         LOG.debug("finish_revert_migration finished successfully.",
9306                   instance=instance)
9307 
9308     def confirm_migration(self, context, migration, instance, network_info):
9309         """Confirms a resize, destroying the source VM."""
9310         self._cleanup_resize(context, instance, network_info)
9311 
9312     @staticmethod
9313     def _get_io_devices(xml_doc):
9314         """get the list of io devices from the xml document."""
9315         result = {"volumes": [], "ifaces": []}
9316         try:
9317             doc = etree.fromstring(xml_doc)
9318         except Exception:
9319             return result
9320         blocks = [('./devices/disk', 'volumes'),
9321             ('./devices/interface', 'ifaces')]
9322         for block, key in blocks:
9323             section = doc.findall(block)
9324             for node in section:
9325                 for child in node.getchildren():
9326                     if child.tag == 'target' and child.get('dev'):
9327                         result[key].append(child.get('dev'))
9328         return result
9329 
9330     def get_diagnostics(self, instance):
9331         guest = self._host.get_guest(instance)
9332 
9333         # TODO(sahid): We are converting all calls from a
9334         # virDomain object to use nova.virt.libvirt.Guest.
9335         # We should be able to remove domain at the end.
9336         domain = guest._domain
9337         output = {}
9338         # get cpu time, might launch an exception if the method
9339         # is not supported by the underlying hypervisor being
9340         # used by libvirt
9341         try:
9342             for vcpu in guest.get_vcpus_info():
9343                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
9344         except libvirt.libvirtError:
9345             pass
9346         # get io status
9347         xml = guest.get_xml_desc()
9348         dom_io = LibvirtDriver._get_io_devices(xml)
9349         for guest_disk in dom_io["volumes"]:
9350             try:
9351                 # blockStats might launch an exception if the method
9352                 # is not supported by the underlying hypervisor being
9353                 # used by libvirt
9354                 stats = domain.blockStats(guest_disk)
9355                 output[guest_disk + "_read_req"] = stats[0]
9356                 output[guest_disk + "_read"] = stats[1]
9357                 output[guest_disk + "_write_req"] = stats[2]
9358                 output[guest_disk + "_write"] = stats[3]
9359                 output[guest_disk + "_errors"] = stats[4]
9360             except libvirt.libvirtError:
9361                 pass
9362         for interface in dom_io["ifaces"]:
9363             try:
9364                 # interfaceStats might launch an exception if the method
9365                 # is not supported by the underlying hypervisor being
9366                 # used by libvirt
9367                 stats = domain.interfaceStats(interface)
9368                 output[interface + "_rx"] = stats[0]
9369                 output[interface + "_rx_packets"] = stats[1]
9370                 output[interface + "_rx_errors"] = stats[2]
9371                 output[interface + "_rx_drop"] = stats[3]
9372                 output[interface + "_tx"] = stats[4]
9373                 output[interface + "_tx_packets"] = stats[5]
9374                 output[interface + "_tx_errors"] = stats[6]
9375                 output[interface + "_tx_drop"] = stats[7]
9376             except libvirt.libvirtError:
9377                 pass
9378         output["memory"] = domain.maxMemory()
9379         # memoryStats might launch an exception if the method
9380         # is not supported by the underlying hypervisor being
9381         # used by libvirt
9382         try:
9383             mem = domain.memoryStats()
9384             for key in mem.keys():
9385                 output["memory-" + key] = mem[key]
9386         except (libvirt.libvirtError, AttributeError):
9387             pass
9388         return output
9389 
9390     def get_instance_diagnostics(self, instance):
9391         guest = self._host.get_guest(instance)
9392 
9393         # TODO(sahid): We are converting all calls from a
9394         # virDomain object to use nova.virt.libvirt.Guest.
9395         # We should be able to remove domain at the end.
9396         domain = guest._domain
9397 
9398         xml = guest.get_xml_desc()
9399         xml_doc = etree.fromstring(xml)
9400 
9401         # TODO(sahid): Needs to use get_info but more changes have to
9402         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
9403         # needed.
9404         (state, max_mem, mem, num_cpu, cpu_time) = \
9405             guest._get_domain_info(self._host)
9406         config_drive = configdrive.required_by(instance)
9407         launched_at = timeutils.normalize_time(instance.launched_at)
9408         uptime = timeutils.delta_seconds(launched_at,
9409                                          timeutils.utcnow())
9410         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
9411                                         driver='libvirt',
9412                                         config_drive=config_drive,
9413                                         hypervisor=CONF.libvirt.virt_type,
9414                                         hypervisor_os='linux',
9415                                         uptime=uptime)
9416         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
9417             maximum=max_mem / units.Mi,
9418             used=mem / units.Mi)
9419 
9420         # get cpu time, might launch an exception if the method
9421         # is not supported by the underlying hypervisor being
9422         # used by libvirt
9423         try:
9424             for vcpu in guest.get_vcpus_info():
9425                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
9426         except libvirt.libvirtError:
9427             pass
9428         # get io status
9429         dom_io = LibvirtDriver._get_io_devices(xml)
9430         for guest_disk in dom_io["volumes"]:
9431             try:
9432                 # blockStats might launch an exception if the method
9433                 # is not supported by the underlying hypervisor being
9434                 # used by libvirt
9435                 stats = domain.blockStats(guest_disk)
9436                 diags.add_disk(read_bytes=stats[1],
9437                                read_requests=stats[0],
9438                                write_bytes=stats[3],
9439                                write_requests=stats[2],
9440                                errors_count=stats[4])
9441             except libvirt.libvirtError:
9442                 pass
9443 
9444         for interface in xml_doc.findall('./devices/interface'):
9445             mac_address = interface.find('mac').get('address')
9446             target = interface.find('./target')
9447 
9448             # add nic that has no target (therefore no stats)
9449             if target is None:
9450                 diags.add_nic(mac_address=mac_address)
9451                 continue
9452 
9453             # add nic with stats
9454             dev = target.get('dev')
9455             try:
9456                 if dev:
9457                     # interfaceStats might launch an exception if the
9458                     # method is not supported by the underlying hypervisor
9459                     # being used by libvirt
9460                     stats = domain.interfaceStats(dev)
9461                     diags.add_nic(mac_address=mac_address,
9462                                   rx_octets=stats[0],
9463                                   rx_errors=stats[2],
9464                                   rx_drop=stats[3],
9465                                   rx_packets=stats[1],
9466                                   tx_octets=stats[4],
9467                                   tx_errors=stats[6],
9468                                   tx_drop=stats[7],
9469                                   tx_packets=stats[5])
9470 
9471             except libvirt.libvirtError:
9472                 pass
9473 
9474         return diags
9475 
9476     @staticmethod
9477     def _prepare_device_bus(dev):
9478         """Determines the device bus and its hypervisor assigned address
9479         """
9480         bus = None
9481         address = (dev.device_addr.format_address() if
9482                    dev.device_addr else None)
9483         if isinstance(dev.device_addr,
9484                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
9485             bus = objects.PCIDeviceBus()
9486         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
9487             if dev.target_bus == 'scsi':
9488                 bus = objects.SCSIDeviceBus()
9489             elif dev.target_bus == 'ide':
9490                 bus = objects.IDEDeviceBus()
9491             elif dev.target_bus == 'usb':
9492                 bus = objects.USBDeviceBus()
9493         if address is not None and bus is not None:
9494             bus.address = address
9495         return bus
9496 
9497     def _build_interface_metadata(self, dev, vifs_to_expose, vlans_by_mac,
9498                                   trusted_by_mac):
9499         """Builds a metadata object for a network interface
9500 
9501         :param dev: The LibvirtConfigGuestInterface to build metadata for.
9502         :param vifs_to_expose: The list of tagged and/or vlan'ed
9503                                VirtualInterface objects.
9504         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
9505         :param trusted_by_mac: A dictionary of mac address -> vf_trusted
9506                                associations.
9507         :return: A NetworkInterfaceMetadata object, or None.
9508         """
9509         vif = vifs_to_expose.get(dev.mac_addr)
9510         if not vif:
9511             LOG.debug('No VIF found with MAC %s, not building metadata',
9512                       dev.mac_addr)
9513             return None
9514         bus = self._prepare_device_bus(dev)
9515         device = objects.NetworkInterfaceMetadata(mac=vif.address)
9516         if 'tag' in vif and vif.tag:
9517             device.tags = [vif.tag]
9518         if bus:
9519             device.bus = bus
9520         vlan = vlans_by_mac.get(vif.address)
9521         if vlan:
9522             device.vlan = int(vlan)
9523         device.vf_trusted = trusted_by_mac.get(vif.address, False)
9524         return device
9525 
9526     def _build_disk_metadata(self, dev, tagged_bdms):
9527         """Builds a metadata object for a disk
9528 
9529         :param dev: The vconfig.LibvirtConfigGuestDisk to build metadata for.
9530         :param tagged_bdms: The list of tagged BlockDeviceMapping objects.
9531         :return: A DiskMetadata object, or None.
9532         """
9533         bdm = tagged_bdms.get(dev.target_dev)
9534         if not bdm:
9535             LOG.debug('No BDM found with device name %s, not building '
9536                       'metadata.', dev.target_dev)
9537             return None
9538         bus = self._prepare_device_bus(dev)
9539         device = objects.DiskMetadata(tags=[bdm.tag])
9540         # NOTE(artom) Setting the serial (which corresponds to
9541         # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
9542         # find the disks's BlockDeviceMapping object when we detach the
9543         # volume and want to clean up its metadata.
9544         device.serial = bdm.volume_id
9545         if bus:
9546             device.bus = bus
9547         return device
9548 
9549     def _build_hostdev_metadata(self, dev, vifs_to_expose, vlans_by_mac):
9550         """Builds a metadata object for a hostdev. This can only be a PF, so we
9551         don't need trusted_by_mac like in _build_interface_metadata because
9552         only VFs can be trusted.
9553 
9554         :param dev: The LibvirtConfigGuestHostdevPCI to build metadata for.
9555         :param vifs_to_expose: The list of tagged and/or vlan'ed
9556                                VirtualInterface objects.
9557         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
9558         :return: A NetworkInterfaceMetadata object, or None.
9559         """
9560         # Strip out the leading '0x'
9561         pci_address = pci_utils.get_pci_address(
9562             *[x[2:] for x in (dev.domain, dev.bus, dev.slot, dev.function)])
9563         try:
9564             mac = pci_utils.get_mac_by_pci_address(pci_address,
9565                                                    pf_interface=True)
9566         except exception.PciDeviceNotFoundById:
9567             LOG.debug('Not exposing metadata for not found PCI device %s',
9568                       pci_address)
9569             return None
9570 
9571         vif = vifs_to_expose.get(mac)
9572         if not vif:
9573             LOG.debug('No VIF found with MAC %s, not building metadata', mac)
9574             return None
9575 
9576         device = objects.NetworkInterfaceMetadata(mac=mac)
9577         device.bus = objects.PCIDeviceBus(address=pci_address)
9578         if 'tag' in vif and vif.tag:
9579             device.tags = [vif.tag]
9580         vlan = vlans_by_mac.get(mac)
9581         if vlan:
9582             device.vlan = int(vlan)
9583         return device
9584 
9585     def _build_device_metadata(self, context, instance):
9586         """Builds a metadata object for instance devices, that maps the user
9587            provided tag to the hypervisor assigned device address.
9588         """
9589         def _get_device_name(bdm):
9590             return block_device.strip_dev(bdm.device_name)
9591 
9592         network_info = instance.info_cache.network_info
9593         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
9594         trusted_by_mac = netutils.get_cached_vifs_with_trusted(network_info)
9595         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
9596                                                                  instance.uuid)
9597         vifs_to_expose = {vif.address: vif for vif in vifs
9598                           if ('tag' in vif and vif.tag) or
9599                              vlans_by_mac.get(vif.address)}
9600         # TODO(mriedem): We should be able to avoid the DB query here by using
9601         # block_device_info['block_device_mapping'] which is passed into most
9602         # methods that call this function.
9603         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9604             context, instance.uuid)
9605         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
9606 
9607         devices = []
9608         guest = self._host.get_guest(instance)
9609         xml = guest.get_xml_desc()
9610         xml_dom = etree.fromstring(xml)
9611         guest_config = vconfig.LibvirtConfigGuest()
9612         guest_config.parse_dom(xml_dom)
9613 
9614         for dev in guest_config.devices:
9615             device = None
9616             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
9617                 device = self._build_interface_metadata(dev, vifs_to_expose,
9618                                                         vlans_by_mac,
9619                                                         trusted_by_mac)
9620             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
9621                 device = self._build_disk_metadata(dev, tagged_bdms)
9622             if isinstance(dev, vconfig.LibvirtConfigGuestHostdevPCI):
9623                 device = self._build_hostdev_metadata(dev, vifs_to_expose,
9624                                                       vlans_by_mac)
9625             if device:
9626                 devices.append(device)
9627         if devices:
9628             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
9629             return dev_meta
9630 
9631     def instance_on_disk(self, instance):
9632         # ensure directories exist and are writable
9633         instance_path = libvirt_utils.get_instance_path(instance)
9634         LOG.debug('Checking instance files accessibility %s', instance_path,
9635                   instance=instance)
9636         shared_instance_path = os.access(instance_path, os.W_OK)
9637         # NOTE(flwang): For shared block storage scenario, the file system is
9638         # not really shared by the two hosts, but the volume of evacuated
9639         # instance is reachable.
9640         shared_block_storage = (self.image_backend.backend().
9641                                 is_shared_block_storage())
9642         return shared_instance_path or shared_block_storage
9643 
9644     def inject_network_info(self, instance, nw_info):
9645         self.firewall_driver.setup_basic_filtering(instance, nw_info)
9646 
9647     def delete_instance_files(self, instance):
9648         target = libvirt_utils.get_instance_path(instance)
9649         # A resize may be in progress
9650         target_resize = target + '_resize'
9651         # Other threads may attempt to rename the path, so renaming the path
9652         # to target + '_del' (because it is atomic) and iterating through
9653         # twice in the unlikely event that a concurrent rename occurs between
9654         # the two rename attempts in this method. In general this method
9655         # should be fairly thread-safe without these additional checks, since
9656         # other operations involving renames are not permitted when the task
9657         # state is not None and the task state should be set to something
9658         # other than None by the time this method is invoked.
9659         target_del = target + '_del'
9660         for i in range(2):
9661             try:
9662                 os.rename(target, target_del)
9663                 break
9664             except Exception:
9665                 pass
9666             try:
9667                 os.rename(target_resize, target_del)
9668                 break
9669             except Exception:
9670                 pass
9671         # Either the target or target_resize path may still exist if all
9672         # rename attempts failed.
9673         remaining_path = None
9674         for p in (target, target_resize):
9675             if os.path.exists(p):
9676                 remaining_path = p
9677                 break
9678 
9679         # A previous delete attempt may have been interrupted, so target_del
9680         # may exist even if all rename attempts during the present method
9681         # invocation failed due to the absence of both target and
9682         # target_resize.
9683         if not remaining_path and os.path.exists(target_del):
9684             self.job_tracker.terminate_jobs(instance)
9685 
9686             LOG.info('Deleting instance files %s', target_del,
9687                      instance=instance)
9688             remaining_path = target_del
9689             try:
9690                 shutil.rmtree(target_del)
9691             except OSError as e:
9692                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
9693                           {'target': target_del, 'e': e}, instance=instance)
9694 
9695         # It is possible that the delete failed, if so don't mark the instance
9696         # as cleaned.
9697         if remaining_path and os.path.exists(remaining_path):
9698             LOG.info('Deletion of %s failed', remaining_path,
9699                      instance=instance)
9700             return False
9701 
9702         LOG.info('Deletion of %s complete', target_del, instance=instance)
9703         return True
9704 
9705     @property
9706     def need_legacy_block_device_info(self):
9707         return False
9708 
9709     def default_root_device_name(self, instance, image_meta, root_bdm):
9710         disk_bus = blockinfo.get_disk_bus_for_device_type(
9711             instance, CONF.libvirt.virt_type, image_meta, "disk")
9712         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
9713             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
9714         root_info = blockinfo.get_root_info(
9715             instance, CONF.libvirt.virt_type, image_meta,
9716             root_bdm, disk_bus, cdrom_bus)
9717         return block_device.prepend_dev(root_info['dev'])
9718 
9719     def default_device_names_for_instance(self, instance, root_device_name,
9720                                           *block_device_lists):
9721         block_device_mapping = list(itertools.chain(*block_device_lists))
9722         # NOTE(ndipanov): Null out the device names so that blockinfo code
9723         #                 will assign them
9724         for bdm in block_device_mapping:
9725             if bdm.device_name is not None:
9726                 LOG.info(
9727                     "Ignoring supplied device name: %(device_name)s. "
9728                     "Libvirt can't honour user-supplied dev names",
9729                     {'device_name': bdm.device_name}, instance=instance)
9730                 bdm.device_name = None
9731         block_device_info = driver.get_block_device_info(instance,
9732                                                          block_device_mapping)
9733 
9734         blockinfo.default_device_names(CONF.libvirt.virt_type,
9735                                        nova_context.get_admin_context(),
9736                                        instance,
9737                                        block_device_info,
9738                                        instance.image_meta)
9739 
9740     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
9741         block_device_info = driver.get_block_device_info(instance, bdms)
9742         instance_info = blockinfo.get_disk_info(
9743                 CONF.libvirt.virt_type, instance,
9744                 instance.image_meta, block_device_info=block_device_info)
9745 
9746         suggested_dev_name = block_device_obj.device_name
9747         if suggested_dev_name is not None:
9748             LOG.info(
9749                 'Ignoring supplied device name: %(suggested_dev)s',
9750                 {'suggested_dev': suggested_dev_name}, instance=instance)
9751 
9752         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
9753         #                 only when it's actually not set on the bd object
9754         block_device_obj.device_name = None
9755         disk_info = blockinfo.get_info_from_bdm(
9756             instance, CONF.libvirt.virt_type, instance.image_meta,
9757             block_device_obj, mapping=instance_info['mapping'])
9758         return block_device.prepend_dev(disk_info['dev'])
9759 
9760     def is_supported_fs_format(self, fs_type):
9761         return fs_type in [nova.privsep.fs.FS_FORMAT_EXT2,
9762                            nova.privsep.fs.FS_FORMAT_EXT3,
9763                            nova.privsep.fs.FS_FORMAT_EXT4,
9764                            nova.privsep.fs.FS_FORMAT_XFS]
9765 
9766     def _get_cpu_traits(self):
9767         """Get CPU traits of VMs based on guest CPU model config:
9768         1. if mode is 'host-model' or 'host-passthrough', use host's
9769         CPU features.
9770         2. if mode is None, choose a default CPU model based on CPU
9771         architecture.
9772         3. if mode is 'custom', use cpu_model to generate CPU features.
9773         The code also accounts for cpu_model_extra_flags configuration when
9774         cpu_mode is 'host-model', 'host-passthrough' or 'custom', this
9775         ensures user specified CPU feature flags to be included.
9776         :return: A dict of trait names mapped to boolean values or None.
9777         """
9778         cpu = self._get_guest_cpu_model_config()
9779         if not cpu:
9780             LOG.info('The current libvirt hypervisor %(virt_type)s '
9781                      'does not support reporting CPU traits.',
9782                      {'virt_type': CONF.libvirt.virt_type})
9783             return
9784 
9785         caps = deepcopy(self._host.get_capabilities())
9786         if cpu.mode in ('host-model', 'host-passthrough'):
9787             # Account for features in cpu_model_extra_flags conf
9788             host_features = [f.name for f in
9789                              caps.host.cpu.features | cpu.features]
9790             return libvirt_utils.cpu_features_to_traits(host_features)
9791 
9792         # Choose a default CPU model when cpu_mode is not specified
9793         if cpu.mode is None:
9794             caps.host.cpu.model = libvirt_utils.get_cpu_model_from_arch(
9795                 caps.host.cpu.arch)
9796             caps.host.cpu.features = set()
9797         else:
9798             # For custom mode, set model to guest CPU model
9799             caps.host.cpu.model = cpu.model
9800             caps.host.cpu.features = set()
9801             # Account for features in cpu_model_extra_flags conf
9802             for f in cpu.features:
9803                 caps.host.cpu.add_feature(
9804                     vconfig.LibvirtConfigCPUFeature(name=f.name))
9805 
9806         xml_str = caps.host.cpu.to_xml()
9807         features_xml = self._get_guest_baseline_cpu_features(xml_str)
9808         feature_names = []
9809         if features_xml:
9810             cpu.parse_str(features_xml)
9811             feature_names = [f.name for f in cpu.features]
9812         return libvirt_utils.cpu_features_to_traits(feature_names)
9813 
9814     def _get_guest_baseline_cpu_features(self, xml_str):
9815         """Calls libvirt's baselineCPU API to compute the biggest set of
9816         CPU features which is compatible with the given host CPU.
9817 
9818         :param xml_str: XML description of host CPU
9819         :return: An XML string of the computed CPU, or None on error
9820         """
9821         LOG.debug("Libvirt baseline CPU %s", xml_str)
9822         # TODO(lei-zh): baselineCPU is not supported on all platforms.
9823         # There is some work going on in the libvirt community to replace the
9824         # baseline call. Consider using the new apis when they are ready. See
9825         # https://www.redhat.com/archives/libvir-list/2018-May/msg01204.html.
9826         try:
9827             if hasattr(libvirt, 'VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES'):
9828                 return self._host.get_connection().baselineCPU(
9829                     [xml_str],
9830                     libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
9831             else:
9832                 return self._host.get_connection().baselineCPU([xml_str])
9833         except libvirt.libvirtError as ex:
9834             with excutils.save_and_reraise_exception() as ctxt:
9835                 error_code = ex.get_error_code()
9836                 if error_code == libvirt.VIR_ERR_NO_SUPPORT:
9837                     ctxt.reraise = False
9838                     LOG.info('URI %(uri)s does not support full set'
9839                              ' of host capabilities: %(error)s',
9840                              {'uri': self._host._uri, 'error': ex})
9841                     return None
